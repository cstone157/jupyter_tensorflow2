{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9379725b-e940-4dba-9e15-4da7f0c9f18b",
   "metadata": {},
   "source": [
    "## Image classification with Vision Transformer\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example implements the <a href=\"https://arxiv.org/abs/2010.11929\">Vision Transformer (ViT)</a> model by Alexey Dosovitskiy et al. for image classification, and demonstrates it on the CIFAR-100 dataset. The ViT model applies the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fc7205-c862-44db-b844-c3386a388897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
      "Requirement already satisfied: jaxlib<=0.5.3,>=0.5.3 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.3)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax) (2.0.2)\n",
      "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.15.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983b1a24-fff7-4ac1-8c59-3e2d8c1152c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 03:30:27.502071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744083027.556829  647113 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744083027.568543  647113 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744083027.627202  647113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744083027.627226  647113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744083027.627229  647113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744083027.627231  647113 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-08 03:30:27.658596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af59148-cd93-4cfb-98e5-72b14ece4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeebf3a-00df-403a-b8c9-6135c4ef1d60",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2086195c-cef4-4d4b-bf16-91e8b2f645d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n",
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcfc888-ae10-4b73-98e3-941d123707fb",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a79e51-615e-4865-a2a1-c8fdd0835c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47f965-d3ca-4496-9160-27462556c945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
