{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eae2ab4-dc1c-4e35-8639-78ab13900433",
   "metadata": {},
   "source": [
    "# Image classification with EANet\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example implements the EANet model for image classification, and demonstrates it on the CIFAR-100 dataset. EANet introduces a novel attention mechanism named <b>external attention</import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "b>, based on two external, small, learnable, and shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers. It conveniently replaces self-attention as used in existing architectures. External attention has linear complexity, as it only implicitly considers the correlations between all samples.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4746f9ff-449a-4801-8414-1a7ae7127465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 03:17:49.326616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745291869.371854  153565 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745291869.386645  153565 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745291869.422868  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745291869.422910  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745291869.422913  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745291869.422915  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-22 03:17:49.431462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc5ca-6b9c-4694-8d06-1e4d53227316",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07e44c8-beb3-401e-99c0-752e4a2809d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 100)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122b1f5-4fa4-4228-b6e8-f3710c0e9ee2",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938d9829-dfc7-42c0-a364-30f93e17af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size: 2 X 2 = 4 \n",
      "Patches per image: 256\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "learning_rate = 0.001\n",
    "label_smoothing = 0.1\n",
    "validation_split = 0.2\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "patch_size = 2  # Size of the patches to be extracted from the input images.\n",
    "num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n",
    "embedding_dim = 64  # Number of hidden units.\n",
    "mlp_dim = 64\n",
    "dim_coefficient = 4\n",
    "num_heads = 4\n",
    "attention_dropout = 0.2\n",
    "projection_dropout = 0.2\n",
    "num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n",
    "\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4ac4a-9272-4dfb-bae2-a6b2a3df5ba3",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb08c04-95f2-46b2-905a-3d76756fd15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 03:18:58.415288: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.1),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13639db-96ae-4bdd-a7ae-3c82d6869d74",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the patch extraction and encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2782e697-f6ea-4901-97d4-301f64697bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, x):\n",
    "        B, C = ops.shape(x)[0], ops.shape(x)[-1]\n",
    "        x = ops.image.extract_patches(x, self.patch_size)\n",
    "        x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = ops.arange(start=0, stop=self.num_patch, step=1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c93af-1a40-4a0e-99df-b665b373ac14",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the external attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575b07bb-b706-44fe-9d69-2270e02130a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_attention(\n",
    "    x,\n",
    "    dim,\n",
    "    num_heads,\n",
    "    dim_coefficient=4,\n",
    "    attention_dropout=0,\n",
    "    projection_dropout=0,\n",
    "):\n",
    "    _, num_patch, channel = x.shape\n",
    "    assert dim % num_heads == 0\n",
    "    num_heads = num_heads * dim_coefficient\n",
    "\n",
    "    x = layers.Dense(dim * dim_coefficient)(x)\n",
    "    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n",
    "    x = ops.reshape(x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads))\n",
    "    x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
    "    # a linear layer M_k\n",
    "    attn = layers.Dense(dim // dim_coefficient)(x)\n",
    "    # normalize attention map\n",
    "    attn = layers.Softmax(axis=2)(attn)\n",
    "    # dobule-normalization\n",
    "    attn = layers.Lambda(\n",
    "        lambda attn: ops.divide(\n",
    "            attn,\n",
    "            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n",
    "        )\n",
    "    )(attn)\n",
    "    attn = layers.Dropout(attention_dropout)(attn)\n",
    "    # a linear layer M_v\n",
    "    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n",
    "    x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
    "    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n",
    "    # a linear layer to project original dim\n",
    "    x = layers.Dense(dim)(x)\n",
    "    x = layers.Dropout(projection_dropout)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54ef11-55d9-46b2-8639-07ef91281271",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the MLP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16693b30-7c0c-4f22-a833-02e5ebbd82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n",
    "    x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "    x = layers.Dense(embedding_dim)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ad8c9-7f69-4e2e-bc8f-c435bbcf3628",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe4e8dd-9655-4219-a405-e5dc8b57b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(\n",
    "    x,\n",
    "    embedding_dim,\n",
    "    mlp_dim,\n",
    "    num_heads,\n",
    "    dim_coefficient,\n",
    "    attention_dropout,\n",
    "    projection_dropout,\n",
    "    attention_type=\"external_attention\",\n",
    "):\n",
    "    residual_1 = x\n",
    "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "    if attention_type == \"external_attention\":\n",
    "        x = external_attention(\n",
    "            x,\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            dim_coefficient,\n",
    "            attention_dropout,\n",
    "            projection_dropout,\n",
    "        )\n",
    "    elif attention_type == \"self_attention\":\n",
    "        x = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=attention_dropout,\n",
    "        )(x, x)\n",
    "    x = layers.add([x, residual_1])\n",
    "    residual_2 = x\n",
    "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "    x = mlp(x, embedding_dim, mlp_dim)\n",
    "    x = layers.add([x, residual_2])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbcd79-5273-4ce9-8ba4-5c342161b903",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the EANet model\n",
    "\n",
    "The EANet model leverages external attention. The computational complexity of traditional self attention is O(d * N ** 2), where d is the embedding size, and N is the number of patch. the authors find that most pixels are closely related to just a few other pixels, and an N-to-N attention matrix may be redundant. So, they propose as an alternative an external attention module where the computational complexity of external attention is O(d * S * N). As d and S are hyper-parameters, the proposed algorithm is linear in the number of pixels. In fact, this is equivalent to a drop patch operation, because a lot of information contained in a patch in an image is redundant and unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63697de-cd35-47e8-8364-1981a9d6b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(attention_type=\"external_attention\"):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Image augment\n",
    "    x = data_augmentation(inputs)\n",
    "    # Extract patches.\n",
    "    x = PatchExtract(patch_size)(x)\n",
    "    # Create patch embedding.\n",
    "    x = PatchEmbedding(num_patches, embedding_dim)(x)\n",
    "    # Create Transformer block.\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(\n",
    "            x,\n",
    "            embedding_dim,\n",
    "            mlp_dim,\n",
    "            num_heads,\n",
    "            dim_coefficient,\n",
    "            attention_dropout,\n",
    "            projection_dropout,\n",
    "            attention_type,\n",
    "        )\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e751bc-bc25-4984-9c29-c79ffbe3ad21",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Train on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b81648-2d61-48f9-815f-50437bf0e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 11/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:39\u001b[0m 3s/step - accuracy: 0.0132 - loss: 4.9923 - top-5-accuracy: 0.0502"
     ]
    }
   ],
   "source": [
    "model = get_model(attention_type=\"external_attention\")\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=validation_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5daa0-8406-4043-8543-b4c9c2b97ce9",
   "metadata": {},
   "source": [
    "#### Let's visualize the training progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64bebc6-78e7-4ef1-b7c4-b78e2452c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d423d-e61d-40f5-8d92-b2faba3c6293",
   "metadata": {},
   "source": [
    "#### Let's display the final results of the test on CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20df5b-24c6-42f5-b6b1-6771d8ae9903",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {round(loss, 2)}\")\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f616d-108a-472f-b4fc-1a81f24e98f6",
   "metadata": {},
   "source": [
    "EANet just replaces self attention in Vit with external attention. The traditional Vit achieved a ~73% test top-5 accuracy and ~41 top-1 accuracy after training 50 epochs, but with 0.6M parameters. Under the same experimental environment and the same hyperparameters, The EANet model we just trained has just 0.3M parameters, and it gets us to ~73% test top-5 accuracy and ~43% top-1 accuracy. This fully demonstrates the effectiveness of external attention.\n",
    "\n",
    "We only show the training process of EANet, you can train Vit under the same experimental conditions and observe the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a262388-4772-4197-aa0b-893984900696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
