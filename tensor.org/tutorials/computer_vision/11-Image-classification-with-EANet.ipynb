{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eae2ab4-dc1c-4e35-8639-78ab13900433",
   "metadata": {},
   "source": [
    "# Image classification with EANet\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example implements the EANet model for image classification, and demonstrates it on the CIFAR-100 dataset. EANet introduces a novel attention mechanism named <b>external attention</import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "b>, based on two external, small, learnable, and shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers. It conveniently replaces self-attention as used in existing architectures. External attention has linear complexity, as it only implicitly considers the correlations between all samples.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4746f9ff-449a-4801-8414-1a7ae7127465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 03:17:49.326616: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745291869.371854  153565 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745291869.386645  153565 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745291869.422868  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745291869.422910  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745291869.422913  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745291869.422915  153565 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-22 03:17:49.431462: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc5ca-6b9c-4694-8d06-1e4d53227316",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07e44c8-beb3-401e-99c0-752e4a2809d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 100)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122b1f5-4fa4-4228-b6e8-f3710c0e9ee2",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938d9829-dfc7-42c0-a364-30f93e17af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size: 2 X 2 = 4 \n",
      "Patches per image: 256\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.0001\n",
    "learning_rate = 0.001\n",
    "label_smoothing = 0.1\n",
    "validation_split = 0.2\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "patch_size = 2  # Size of the patches to be extracted from the input images.\n",
    "num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n",
    "embedding_dim = 64  # Number of hidden units.\n",
    "mlp_dim = 64\n",
    "dim_coefficient = 4\n",
    "num_heads = 4\n",
    "attention_dropout = 0.2\n",
    "projection_dropout = 0.2\n",
    "num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n",
    "\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4ac4a-9272-4dfb-bae2-a6b2a3df5ba3",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb08c04-95f2-46b2-905a-3d76756fd15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 03:18:58.415288: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.1),\n",
    "        layers.RandomContrast(factor=0.1),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13639db-96ae-4bdd-a7ae-3c82d6869d74",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the patch extraction and encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2782e697-f6ea-4901-97d4-301f64697bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, x):\n",
    "        B, C = ops.shape(x)[0], ops.shape(x)[-1]\n",
    "        x = ops.image.extract_patches(x, self.patch_size)\n",
    "        x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = ops.arange(start=0, stop=self.num_patch, step=1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c93af-1a40-4a0e-99df-b665b373ac14",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the external attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575b07bb-b706-44fe-9d69-2270e02130a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_attention(\n",
    "    x,\n",
    "    dim,\n",
    "    num_heads,\n",
    "    dim_coefficient=4,\n",
    "    attention_dropout=0,\n",
    "    projection_dropout=0,\n",
    "):\n",
    "    _, num_patch, channel = x.shape\n",
    "    assert dim % num_heads == 0\n",
    "    num_heads = num_heads * dim_coefficient\n",
    "\n",
    "    x = layers.Dense(dim * dim_coefficient)(x)\n",
    "    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n",
    "    x = ops.reshape(x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads))\n",
    "    x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
    "    # a linear layer M_k\n",
    "    attn = layers.Dense(dim // dim_coefficient)(x)\n",
    "    # normalize attention map\n",
    "    attn = layers.Softmax(axis=2)(attn)\n",
    "    # dobule-normalization\n",
    "    attn = layers.Lambda(\n",
    "        lambda attn: ops.divide(\n",
    "            attn,\n",
    "            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n",
    "        )\n",
    "    )(attn)\n",
    "    attn = layers.Dropout(attention_dropout)(attn)\n",
    "    # a linear layer M_v\n",
    "    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n",
    "    x = ops.transpose(x, axes=[0, 2, 1, 3])\n",
    "    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n",
    "    # a linear layer to project original dim\n",
    "    x = layers.Dense(dim)(x)\n",
    "    x = layers.Dropout(projection_dropout)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54ef11-55d9-46b2-8639-07ef91281271",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the MLP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16693b30-7c0c-4f22-a833-02e5ebbd82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n",
    "    x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "    x = layers.Dense(embedding_dim)(x)\n",
    "    x = layers.Dropout(drop_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ad8c9-7f69-4e2e-bc8f-c435bbcf3628",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe4e8dd-9655-4219-a405-e5dc8b57b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(\n",
    "    x,\n",
    "    embedding_dim,\n",
    "    mlp_dim,\n",
    "    num_heads,\n",
    "    dim_coefficient,\n",
    "    attention_dropout,\n",
    "    projection_dropout,\n",
    "    attention_type=\"external_attention\",\n",
    "):\n",
    "    residual_1 = x\n",
    "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "    if attention_type == \"external_attention\":\n",
    "        x = external_attention(\n",
    "            x,\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            dim_coefficient,\n",
    "            attention_dropout,\n",
    "            projection_dropout,\n",
    "        )\n",
    "    elif attention_type == \"self_attention\":\n",
    "        x = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dim,\n",
    "            dropout=attention_dropout,\n",
    "        )(x, x)\n",
    "    x = layers.add([x, residual_1])\n",
    "    residual_2 = x\n",
    "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "    x = mlp(x, embedding_dim, mlp_dim)\n",
    "    x = layers.add([x, residual_2])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de261e-41da-407c-93ef-2be74a5e5ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
