{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cdeff64-87f1-4d5d-bf88-665ebe63d7e0",
   "metadata": {},
   "source": [
    "# Pneumonia Classification on TPU\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction + Set-up\n",
    "\n",
    "This tutorial will explain how to build an X-ray image classification model to predict whether an X-ray scan shows presence of pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c3959f4-573b-4bea-95ab-4494c3b9b2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 03:34:17.388157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744688057.424996   54131 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744688057.435778   54131 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744688057.456194   54131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744688057.456227   54131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744688057.456230   54131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744688057.456232   54131 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 03:34:17.461625: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    print(\"Device:\", tpu.master())\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a2b14-cf2c-46f0-a80a-6d7961d7a1af",
   "metadata": {},
   "source": [
    "We need a Google Cloud link to our data to load the data using a TPU. Below, we define key configuration parameters we'll use in this example. To run on TPU, this example must be on Colab with the TPU runtime selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8c61b7-abfb-449f-8ece-fc60d4a9ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 25 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [180, 180]\n",
    "CLASS_NAMES = [\"NORMAL\", \"PNEUMONIA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11b3d0b-434b-435e-9930-a37f766ae54d",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Load the data\n",
    "\n",
    "The Chest X-ray data we are using from <a href=\"https://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\">Cell</a> divides the data into training and test files. Let's first load in the training TFRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350eccdb-1525-44ea-a260-f88ddcee2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 03:35:18.832118: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "train_images = tf.data.TFRecordDataset(\n",
    "    \"gs://download.tensorflow.org/data/ChestXRay2017/train/images.tfrec\"\n",
    ")\n",
    "train_paths = tf.data.TFRecordDataset(\n",
    "    \"gs://download.tensorflow.org/data/ChestXRay2017/train/paths.tfrec\"\n",
    ")\n",
    "\n",
    "ds = tf.data.Dataset.zip((train_images, train_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dded92d-adf4-4bf8-9946-632e0264b973",
   "metadata": {},
   "source": [
    "Let's count how many healthy/normal chest X-rays we have and how many pneumonia chest X-rays we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f1076d-fd92-44b4-8118-61bd12e634c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 03:35:38.649363: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:381] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2025-04-15 03:35:38.743234: W external/local_xla/xla/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Could not resolve hostname', error details: Could not resolve host: metadata.google.internal\".\n",
      "2025-04-15 03:35:40.086889: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal images count in training set: 1349\n",
      "Pneumonia images count in training set: 3883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 03:35:41.487640: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "COUNT_NORMAL = len(\n",
    "    [\n",
    "        filename\n",
    "        for filename in train_paths\n",
    "        if \"NORMAL\" in filename.numpy().decode(\"utf-8\")\n",
    "    ]\n",
    ")\n",
    "print(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n",
    "\n",
    "COUNT_PNEUMONIA = len(\n",
    "    [\n",
    "        filename\n",
    "        for filename in train_paths\n",
    "        if \"PNEUMONIA\" in filename.numpy().decode(\"utf-8\")\n",
    "    ]\n",
    ")\n",
    "print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb6932-d184-4eeb-a55e-e83068da56fe",
   "metadata": {},
   "source": [
    "Notice that there are way more images that are classified as pneumonia than normal. This shows that we have an imbalance in our data. We will correct for this imbalance later on in our notebook.\n",
    "\n",
    "We want to map each filename to the corresponding (image, label) pair. The following methods will help us do that.\n",
    "\n",
    "As we only have two labels, we will encode the label so that 1 or True indicates pneumonia and 0 or False indicates normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921008cc-345c-4226-9d15-4b34a8b1d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, \"/\")\n",
    "    # The second to last is the class-directory\n",
    "    if parts[-2] == \"PNEUMONIA\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, IMAGE_SIZE)\n",
    "\n",
    "\n",
    "def process_path(image, path):\n",
    "    label = get_label(path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = decode_img(image)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "ds = ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee10234-1daa-4e45-9cdf-06c7c9ef5b1e",
   "metadata": {},
   "source": [
    "Let's split the data into a training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc73d31d-bdb0-45c1-9d1a-a000fe638b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(10000)\n",
    "train_ds = ds.take(4200)\n",
    "val_ds = ds.skip(4200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d497ed60-b1a2-4598-8e72-5d92e583f5d8",
   "metadata": {},
   "source": [
    "Let's visualize the shape of an (image, label) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538531ab-d556-4a54-9c8c-459b0ba0aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 03:16:38.112435: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 236 of 10000\n",
      "2025-04-18 03:16:57.022679: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 776 of 10000\n",
      "2025-04-18 03:17:15.934902: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 1202 of 10000\n",
      "2025-04-18 03:17:38.954474: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 4141 of 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (180, 180, 3)\n",
      "Label:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 03:17:41.116366: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:482] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "    print(\"Image shape: \", image.numpy().shape)\n",
    "    print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f463439-db9c-4aff-9cd6-43e5f4306fba",
   "metadata": {},
   "source": [
    "Load and format the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0aad24-37f5-4df1-beec-e6d066616b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.TFRecordDataset(\n",
    "    \"gs://download.tensorflow.org/data/ChestXRay2017/test/images.tfrec\"\n",
    ")\n",
    "test_paths = tf.data.TFRecordDataset(\n",
    "    \"gs://download.tensorflow.org/data/ChestXRay2017/test/paths.tfrec\"\n",
    ")\n",
    "test_ds = tf.data.Dataset.zip((test_images, test_paths))\n",
    "\n",
    "test_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c4828-bfff-48ec-bab6-dd3e5fd13edb",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Visualize the dataset\n",
    "\n",
    "First, let's use buffered prefetching so we can yield data from disk without having I/O become blocking.\n",
    "\n",
    "Please note that large image datasets should not be cached in memory. We do it here because the dataset is not very large and we want to train on TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57e39e86-20eb-4084-866f-215e6ab2f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a70861-126f-48b9-b185-fcc20340efee",
   "metadata": {},
   "source": [
    "Call the next batch iteration of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba272665-e8b0-4b9a-b5ac-f65c29987266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 03:17:53.272472: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 236 of 10000\n",
      "2025-04-18 03:18:12.520025: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 776 of 10000\n",
      "2025-04-18 03:18:23.172356: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] ShuffleDatasetV3:9: Filling up shuffle buffer (this may take a while): 990 of 10000\n"
     ]
    }
   ],
   "source": [
    "train_ds = prepare_for_training(train_ds)\n",
    "val_ds = prepare_for_training(val_ds)\n",
    "\n",
    "image_batch, label_batch = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d324d7-b121-4fa4-86c6-169fab06707c",
   "metadata": {},
   "source": [
    "Define the method to show the images in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce29914f-281e-460f-831f-0985469b544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5, 5, n + 1)\n",
    "        plt.imshow(image_batch[n] / 255)\n",
    "        if label_batch[n]:\n",
    "            plt.title(\"PNEUMONIA\")\n",
    "        else:\n",
    "            plt.title(\"NORMAL\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb32ea-9087-497c-934a-01a6ffcc09a8",
   "metadata": {},
   "source": [
    "As the method takes in NumPy arrays as its parameters, call the numpy function on the batches to return the tensor in NumPy array form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964ab55-a62d-40de-a7f4-3ca5c95b9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a20b6-8ca6-42fe-a489-58b1b69c551a",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Build the CNN\n",
    "\n",
    "To make our model more modular and easier to understand, let's define some blocks. As we're building a convolution neural network, we'll create a convolution block and a dense layer block.\n",
    "\n",
    "The architecture for this CNN has been inspired by this <a href=\"https://towardsdatascience.com/deep-learning-for-detecting-pneumonia-from-x-ray-images-fc9a3d9fdba8\">article</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8320a-6b34-408f-b0e4-9db0539572e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "def conv_block(filters, inputs):\n",
    "    x = layers.SeparableConv2D(filters, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.SeparableConv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.MaxPool2D()(x)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def dense_block(units, dropout_rate, inputs):\n",
    "    x = layers.Dense(units, activation=\"relu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f08d1-bb15-46a4-8e07-83d4ea545598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
