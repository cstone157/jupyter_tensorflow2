{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c88038c-2032-4f5a-bbd9-67f91f9f7af2",
   "metadata": {},
   "source": [
    "# Image classification with Perceiver\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This example implements the <a href=\"https://arxiv.org/abs/2103.03206\">Perceiver: General Perception with Iterative Attention</a> model by Andrew Jaegle et al. for image classification, and demonstrates it on the CIFAR-100 dataset.\n",
    "\n",
    "The Perceiver model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs.\n",
    "\n",
    "In other words: let's assume that your input data array (e.g. image) has M elements (i.e. patches), where M is large. In a standard Transformer model, a self-attention operation is performed for the M elements. The complexity of this operation is O(M^2). However, the Perceiver model creates a latent array of size N elements, where N << M, and performs two operations iteratively:\n",
    "\n",
    "<ul>\n",
    "    <li>Cross-attention Transformer between the latent array and the data array - The complexity of this operation is O(M.N).</li>\n",
    "    <li>Self-attention Transformer on the latent array - The complexity of this operation is O(N^2).</li>\n",
    "</ul>\n",
    "\n",
    "This example requires Keras 3.0 or higher.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f151ca2-625d-4e75-bb4d-40eb0fb6fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 03:23:13.861151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745464993.877507  212247 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745464993.882911  212247 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745464993.893922  212247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745464993.893936  212247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745464993.893938  212247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745464993.893939  212247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 03:23:13.897221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers, activations, ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77526c1a-a431-43a6-915a-8d10fb164a23",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a360c91-7698-4a52-a4c9-c975b7ac77c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150f20c-5923-4217-bec7-fb9e50482aad",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524edceb-f388-46f7-bbb0-bd5b918633fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 64 X 64 = 4096\n",
      "Patch size: 2 X 2 = 4 \n",
      "Patches per image: 1024\n",
      "Elements per patch (3 channels): 12\n",
      "Latent array shape: 256 X 256\n",
      "Data array shape: 1024 X 256\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 64\n",
    "num_epochs = 2  # You should actually use 50 epochs!\n",
    "dropout_rate = 0.2\n",
    "image_size = 64  # We'll resize input images to this size.\n",
    "patch_size = 2  # Size of the patches to be extract from the input images.\n",
    "num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n",
    "latent_dim = 256  # Size of the latent array.\n",
    "projection_dim = 256  # Embedding size of each element in the data and latent arrays.\n",
    "num_heads = 8  # Number of Transformer heads.\n",
    "ffn_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]  # Size of the Transformer Feedforward network.\n",
    "num_transformer_blocks = 4\n",
    "num_iterations = 2  # Repetitions of the cross-attention and Transformer modules.\n",
    "classifier_units = [\n",
    "    projection_dim,\n",
    "    num_classes,\n",
    "]  # Size of the Feedforward network of the final classifier.\n",
    "\n",
    "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n",
    "print(f\"Patches per image: {num_patches}\")\n",
    "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
    "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
    "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9bc42-fdc8-4418-9a34-1c2f9f35fe73",
   "metadata": {},
   "source": [
    "Note that, in order to use each pixel as an individual input in the data array, set patch_size to 1.\n",
    "\n",
    "Use data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
