{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f76574b3-2bf5-43e6-a04f-b6712a050e06",
   "metadata": {},
   "source": [
    "# Semi-supervised\n",
    "\n",
    "<hr />\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Semi-supervised learning\n",
    "\n",
    "Semi-supervised learning is a machine learning paradigm that deals with <b>partially labeled datasets</b>. When applying deep learning in the real world, one usually has to gather a large dataset to make it work well. However, while the cost of labeling scales linearly with the dataset size (labeling each example takes a constant time), model performance only scales <a href=\"https://arxiv.org/abs/2001.08361\">sublinearly</a> with it. This means that labeling more and more samples becomes less and less cost-efficient, while gathering unlabeled data is generally cheap, as it is usually readily available in large quantities.\n",
    "\n",
    "Semi-supervised learning offers to solve this problem by only requiring a partially labeled dataset, and by being label-efficient by utilizing the unlabeled examples for learning as well.\n",
    "\n",
    "In this example, we will pretrain an encoder with contrastive learning on the <a href=\"https://ai.stanford.edu/~acoates/stl10/\">STL-10</a> semi-supervised dataset using no labels at all, and then fine-tune it using only its labeled subset.\n",
    "Contrastive learning\n",
    "\n",
    "On the highest level, the main idea behind contrastive learning is to <b>learn representations that are invariant to image augmentations</b> in a self-supervised manner. One problem with this objective is that it has a trivial degenerate solution: the case where the representations are constant, and do not depend at all on the input images.\n",
    "\n",
    "Contrastive learning avoids this trap by modifying the objective in the following way: it pulls representations of augmented versions/views of the same image closer to each other (contracting positives), while simultaneously pushing different images away from each other (contrasting negatives) in representation space.\n",
    "\n",
    "One such contrastive approach is <a href=\"https://arxiv.org/abs/2002.05709\">SimCLR</a>, which essentially identifies the core components needed to optimize this objective, and can achieve high performance by scaling this simple approach.\n",
    "\n",
    "Another approach is <a href=\"https://arxiv.org/abs/2011.10566\">SimSiam</a> (<a href=\"https://keras.io/examples/vision/simsiam/\">Keras example</a>), whose main difference from SimCLR is that the former does not use any negatives in its loss. Therefore, it does not explicitly prevent the trivial solution, and, instead, avoids it implicitly by architecture design (asymmetric encoding paths using a predictor network and batch normalization (BatchNorm) are applied in the final layers).\n",
    "\n",
    "For further reading about SimCLR, check out the <a href=\"https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html\">official Google AI blog post</a>, and for an overview of self-supervised learning across both vision and language check out <a href=\"https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/\">this blog post</a>.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf5dc76-ecd7-480b-9668-c4f5b41bdc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 03:35:28.232388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745811328.249944  529479 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745811328.255363  529479 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745811328.267018  529479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745811328.267033  529479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745811328.267034  529479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745811328.267035  529479 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-28 03:35:28.270860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "\n",
    "# Make sure we are able to handle large datasets\n",
    "import resource\n",
    "\n",
    "low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e57c9-7f0e-4c94-ad63-7344862a22c5",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Hyperparameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d313519-296f-41d4-a57a-bdc0286a416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset hyperparameters\n",
    "unlabeled_dataset_size = 100000\n",
    "labeled_dataset_size = 5000\n",
    "image_channels = 3\n",
    "\n",
    "# Algorithm hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 525  # Corresponds to 200 steps per epoch\n",
    "width = 128\n",
    "temperature = 0.1\n",
    "# Stronger augmentations for contrastive, weaker ones for supervised training\n",
    "contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n",
    "classification_augmentation = {\n",
    "    \"min_area\": 0.75,\n",
    "    \"brightness\": 0.3,\n",
    "    \"jitter\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c1c9-7c56-4db8-9d77-295426b98fb2",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Dataset\n",
    "\n",
    "During training we will simultaneously load a large batch of unlabeled images along with a smaller batch of labeled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9e088-f388-48ff-80fa-af81578c0354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size is 500 (unlabeled) + 25 (labeled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /root/tensorflow_datasets/stl10/1.0.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/stl10/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cec9458547423dad18aeb3995fbfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8250ea5150b4ba493a070796baab080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e71eaa3a444248ae7610f33c29c240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e09ed53b13b4573ab5d8155e1c36ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 03:38:58.342105: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/stl10/incomplete.SIYPF1_1.0.0/stl10-train.tfrecord*...:   0%|          | 0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcb5e7146344d6fa5af0d14aafacfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset():\n",
    "    # Labeled and unlabeled samples are loaded synchronously\n",
    "    # with batch sizes selected accordingly\n",
    "    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n",
    "    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n",
    "    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n",
    "    print(\n",
    "        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n",
    "    )\n",
    "\n",
    "    # Turning off shuffle to lower resource usage\n",
    "    unlabeled_train_dataset = (\n",
    "        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False)\n",
    "        .shuffle(buffer_size=10 * unlabeled_batch_size)\n",
    "        .batch(unlabeled_batch_size)\n",
    "    )\n",
    "    labeled_train_dataset = (\n",
    "        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=False)\n",
    "        .shuffle(buffer_size=10 * labeled_batch_size)\n",
    "        .batch(labeled_batch_size)\n",
    "    )\n",
    "    test_dataset = (\n",
    "        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    # Labeled and unlabeled datasets are zipped together\n",
    "    train_dataset = tf.data.Dataset.zip(\n",
    "        (unlabeled_train_dataset, labeled_train_dataset)\n",
    "    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, labeled_train_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Load STL10 dataset\n",
    "train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2d988-8d3c-4b68-bfef-eb6728ad5f27",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Image augmentations\n",
    "\n",
    "The two most important image augmentations for contrastive learning are the following:\n",
    "\n",
    "<ul>\n",
    "    <li>Cropping: forces the model to encode different parts of the same image similarly, we implement it with the <a href=\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_translation/\">RandomTranslation</a> and <a href=\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/random_zoom/\">RandomZoom</a> layers</li>\n",
    "    <li>Color jitter: prevents a trivial color histogram-based solution to the task by distorting color histograms. A principled way to implement that is by affine transformations in color space.</li>\n",
    "</ul>\n",
    "\n",
    "In this example we use random horizontal flips as well. Stronger augmentations are applied for contrastive learning, along with weaker ones for supervised classification to avoid overfitting on the few labeled examples.\n",
    "\n",
    "We implement random color jitter as a custom preprocessing layer. Using preprocessing layers for data augmentation has the following two advantages:\n",
    "\n",
    "<ul>\n",
    "    <li>The data augmentation will run on GPU in batches, so the training will not be bottlenecked by the data pipeline in environments with constrained CPU resources (such as a Colab Notebook, or a personal machine)</li>\n",
    "    <li>Deployment is easier as the data preprocessing pipeline is encapsulated in the model, and does not have to be reimplemented when deploying it</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318a32c-a6a3-4aa4-b83b-5ffea4a09310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distorts the color distibutions of images\n",
    "class RandomColorAffine(layers.Layer):\n",
    "    def __init__(self, brightness=0, jitter=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "        self.brightness = brightness\n",
    "        self.jitter = jitter\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})\n",
    "        return config\n",
    "\n",
    "    def call(self, images, training=True):\n",
    "        if training:\n",
    "            batch_size = ops.shape(images)[0]\n",
    "\n",
    "            # Same for all colors\n",
    "            brightness_scales = 1 + keras.random.uniform(\n",
    "                (batch_size, 1, 1, 1),\n",
    "                minval=-self.brightness,\n",
    "                maxval=self.brightness,\n",
    "                seed=self.seed_generator,\n",
    "            )\n",
    "            # Different for all colors\n",
    "            jitter_matrices = keras.random.uniform(\n",
    "                (batch_size, 1, 3, 3), \n",
    "                minval=-self.jitter, \n",
    "                maxval=self.jitter,\n",
    "                seed=self.seed_generator,\n",
    "            )\n",
    "\n",
    "            color_transforms = (\n",
    "                ops.tile(ops.expand_dims(ops.eye(3), axis=0), (batch_size, 1, 1, 1))\n",
    "                * brightness_scales\n",
    "                + jitter_matrices\n",
    "            )\n",
    "            images = ops.clip(ops.matmul(images, color_transforms), 0, 1)\n",
    "        return images\n",
    "\n",
    "\n",
    "# Image augmentation module\n",
    "def get_augmenter(min_area, brightness, jitter):\n",
    "    zoom_factor = 1.0 - math.sqrt(min_area)\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Rescaling(1 / 255),\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n",
    "            layers.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n",
    "            RandomColorAffine(brightness, jitter),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_augmentations(num_images):\n",
    "    # Sample a batch from a dataset\n",
    "    images = next(iter(train_dataset))[0][0][:num_images]\n",
    "\n",
    "    # Apply augmentations\n",
    "    augmented_images = zip(\n",
    "        images,\n",
    "        get_augmenter(**classification_augmentation)(images),\n",
    "        get_augmenter(**contrastive_augmentation)(images),\n",
    "        get_augmenter(**contrastive_augmentation)(images),\n",
    "    )\n",
    "    row_titles = [\n",
    "        \"Original:\",\n",
    "        \"Weakly augmented:\",\n",
    "        \"Strongly augmented:\",\n",
    "        \"Strongly augmented:\",\n",
    "    ]\n",
    "    plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)\n",
    "    for column, image_row in enumerate(augmented_images):\n",
    "        for row, image in enumerate(image_row):\n",
    "            plt.subplot(4, num_images, row * num_images + column + 1)\n",
    "            plt.imshow(image)\n",
    "            if column == 0:\n",
    "                plt.title(row_titles[row], loc=\"left\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "visualize_augmentations(num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a769c-1bc3-4582-8838-f27a9113760b",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Encoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa0e018-782a-4fec-95bc-c4cdffb65879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder architecture\n",
    "def get_encoder():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(width, activation=\"relu\"),\n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a99a99-1fab-4750-b35d-512494d9a9ca",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Supervised baseline model\n",
    "\n",
    "A baseline supervised model is trained using random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805de73-b06c-49f1-b8db-b8db7946f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline supervised training with random initialization\n",
    "baseline_model = keras.Sequential(\n",
    "    [\n",
    "        get_augmenter(**classification_augmentation),\n",
    "        get_encoder(),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"baseline_model\",\n",
    ")\n",
    "baseline_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "baseline_history = baseline_model.fit(\n",
    "    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Maximal validation accuracy: {:.2f}%\".format(\n",
    "        max(baseline_history.history[\"val_acc\"]) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb73bc-ade9-4269-9a5f-ff8a12cec0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
