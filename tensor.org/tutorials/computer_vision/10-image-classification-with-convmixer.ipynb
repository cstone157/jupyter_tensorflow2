{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d81d36-7372-4513-af8f-accca05d725a",
   "metadata": {},
   "source": [
    "# Image classification with ConvMixer\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Vision Transformers (ViT; <a href=\"https://arxiv.org/abs/1612.00593\">Dosovitskiy et al.</a>) extract small patches from the input images, linearly project them, and then apply the Transformer (<a href=\"https://arxiv.org/abs/1706.03762\">Vaswani et al.</a>) blocks. The application of ViTs to image recognition tasks is quickly becoming a promising area of research, because ViTs eliminate the need to have strong inductive biases (such as convolutions) for modeling locality. This presents them as a general computation primititive capable of learning just from the training data with as minimal inductive priors as possible. ViTs yield great downstream performance when trained with proper regularization, data augmentation, and relatively large datasets.\n",
    "\n",
    "In the <a href=\"https://openreview.net/pdf?id=TVHS5Y4dNvM\">Patches Are All You Need</a> paper (note: at the time of writing, it is a submission to the ICLR 2022 conference), the authors extend the idea of using patches to train an all-convolutional network and demonstrate competitive results. Their architecture namely <b>ConvMixer</b> uses recipes from the recent isotrophic architectures like ViT, MLP-Mixer (<a href=\"https://arxiv.org/abs/2105.01601\">Tolstikhin et al.</a>), such as using the same depth and resolution across different layers in the network, residual connections, and so on.\n",
    "\n",
    "In this example, we will implement the ConvMixer model and demonstrate its performance on the CIFAR-10 dataset.\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c413de07-0ff4-4869-860d-c80a534b475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 04:51:03.900037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745211063.915648  105436 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745211063.920650  105436 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745211063.931585  105436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745211063.931600  105436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745211063.931602  105436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745211063.931603  105436 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-21 04:51:03.935083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7706c-06f5-4a2e-a716-639fdbce1228",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "To keep run time short, we will train the model for only 10 epochs. To focus on the core ideas of ConvMixer, we will not use other training-specific elements like RandAugment (<a href=\"https://arxiv.org/abs/1909.13719\">Cubuk et al.</a>). If you are interested in learning more about those details, please refer to the <a href=\"https://openreview.net/pdf?id=TVHS5Y4dNvM\">original paper</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c47379c9-521a-4cef-93eb-b14cfe2f17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 128\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6407d92-7f15-4532-89d8-59b60caef1e5",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Load the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2dd41d7-608c-4a34-9b02-8566fd8d14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data samples: 45000\n",
      "Validation data samples: 5000\n",
      "Test data samples: 10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "val_split = 0.1\n",
    "\n",
    "val_indices = int(len(x_train) * val_split)\n",
    "new_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]\n",
    "x_val, y_val = x_train[:val_indices], y_train[:val_indices]\n",
    "\n",
    "print(f\"Training data samples: {len(new_x_train)}\")\n",
    "print(f\"Validation data samples: {len(x_val)}\")\n",
    "print(f\"Test data samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e093d51-5304-4ff1-a91a-2aed66bced81",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset\">tf.data.Dataset</a> objects\n",
    "\n",
    "Our data augmentation pipeline is different from what the authors used for the CIFAR-10 dataset, which is fine for the purpose of the example. Note that, it's ok to use <b>TF APIs for data I/O and preprocessing</b> with other backends (jax, torch) as it is feature-complete framework when it comes to data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c183627f-984d-4bc7-855f-144c05d022fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 04:53:37.736305: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "augmentation_layers = [\n",
    "    keras.layers.RandomCrop(image_size, image_size),\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "]\n",
    "\n",
    "\n",
    "def augment_images(images):\n",
    "    for layer in augmentation_layers:\n",
    "        images = layer(images, training=True)\n",
    "    return images\n",
    "\n",
    "\n",
    "def make_datasets(images, labels, is_train=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size * 10)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if is_train:\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (augment_images(x), y), num_parallel_calls=auto\n",
    "        )\n",
    "    return dataset.prefetch(auto)\n",
    "\n",
    "\n",
    "train_dataset = make_datasets(new_x_train, new_y_train, is_train=True)\n",
    "val_dataset = make_datasets(x_val, y_val)\n",
    "test_dataset = make_datasets(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e102a8-163d-4579-92da-485d82a5b4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
