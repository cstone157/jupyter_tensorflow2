{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1004c53-a405-45db-9674-4cc32d01b78d",
   "metadata": {},
   "source": [
    "##  Overfit and underfit (https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)\n",
    "\n",
    "As always, the code in this example will use the <code><a href=\"\">tf.keras API, which you can learn more about in the TensorFlow <a href=\"\">Keras guide.\n",
    "\n",
    "In both of the previous examples—<a href=\"\">classifying text and <a href=\"\">predicting fuel efficiency</a>—the accuracy of models on the validation data would peak after training for a number of epochs and then stagnate or start decreasing.\n",
    "\n",
    "In other words, your model would overfit to the training data. Learning how to deal with overfitting is important. Although it's often possible to achieve high accuracy on the training set, what you really want is to develop models that generalize well to a testing set (or data they haven't seen before).\n",
    "\n",
    "The opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the train data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data.\n",
    "\n",
    "If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. You need to strike a balance. Understanding how to train for an appropriate number of epochs as you'll explore below is a useful skill.\n",
    "\n",
    "To prevent overfitting, the best solution is to use more complete training data. The dataset should cover the full range of inputs that the model is expected to handle. Additional data may only be useful if it covers new and interesting cases.\n",
    "\n",
    "A model trained on more complete data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization. These place constraints on the quantity and type of information your model can store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
    "\n",
    "In this notebook, you'll explore several common regularization techniques, and use them to improve on a classification model.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Before getting started, import the necessary packages:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
