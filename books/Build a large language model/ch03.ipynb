{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### -----\n",
        "#### Build a Large Language Model\n",
        "#### Sebastian Raschka\n",
        "#### -----\n",
        "\n",
        "# Coding attention mechanisms\n",
        "\n",
        "  - This chapter will focus on coding the remaining parts of the LLMs surrounding the self-attention mechanism to see it in action and to create a model to generate text.\n",
        "  - Different types of attention mechanisms\n",
        "    - Simplified self-attention:\n",
        "    - Self-attention: Simplified self-attention with trainable weights\n",
        "    - Casual attention: Type of self-attention used in LLMs that allows a model to consider on previous and current inputs in a sequence\n",
        "    - Multi-head attention:\n",
        "\n",
        "## 3.1 - The problem with modeling long sequences\n",
        "\n",
        "  - You can't simply translate a text word into another language due to the grammatical structures in source and target language.\n",
        "  - Deep neural network with two submodules, encoder and decoder:\n",
        "    - encode : first read in and process the entire text\n",
        "    - decoder : then produces the translated text.\n",
        "  - Before transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.\n",
        "\n",
        "## 3.2 - Capturing data dependencies with attention mechanisms\n",
        "\n",
        "  - Bahdanau attention mechanism : modifies the encoder-decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step.\n",
        "  - Self-attention is a mechanism that allows each position in the input sequence when computing the representation of a sequence.\n",
        "\n",
        "## 3.3 - Attending to different parts of the input with self-attention\n",
        "\n",
        "  - In self-attention, the \"self\" refers to the mechanism's ability to compute attention weights by relating different positions within a single input sequence.\n",
        "\n",
        "### 3.3.1 - A simple self-attention mechanism without trainable weights\n",
        "\n",
        "  - In self-attention, our goal is to calculate context vectors for each element x in the input sequence.  A context vector can be interpreted as an enrighted embedding vector.\n",
        "  - Context vectors play a crucial role in self-attention.  Their purpose is to create enriched representations of each element in an input sequence by incorporating information from all other elsements in the sequence.\n",
        "  - The first step of implementing self-attention is to compute the intermediate values w, referred to as attention scores."
      ],
      "metadata": {
        "id": "_dZv9slceG97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ApPghy3neA0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a44fda5-e4d2-4876-8896-1c8b485d3216"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# small embedding dimension\n",
        "import torch\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # Your       (x^1)\n",
        "     [0.55, 0.87, 0.66],  # journey    (x^2)\n",
        "     [0.57, 0.85, 0.64],  # starts     (x^3)\n",
        "     [0.22, 0.58, 0.33],  # with       (x^4)\n",
        "     [0.77, 0.25, 0.10],  # one        (x^5)\n",
        "     [0.05, 0.80, 0.55]]  # step       (x^6)\n",
        ")\n",
        "\n",
        "query = inputs[1]\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n",
        "attn_scores_2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step is to normalize each of the attention scores we computed previously\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "print(\"Attention weights: \", attn_weights_2_tmp)\n",
        "print(\"Sum: \", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oukr-kHrQVfd",
        "outputId": "a8332568-2d5d-46a2-f2ed-f43a70a98301"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   More common to use the softmax function for normalization.\n",
        "#   Additionally, softmax function ensures that the attention weights are always\n",
        "# positive.\n",
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "print(\"Attention weights: \", attn_weights_2_naive)\n",
        "print(\"Sum: \", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOMzXwmkQx2q",
        "outputId": "a5a69c1a-5acf-4287-80a7-59c8146d2fde"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   It's advisable to use PyTorch of softmax to prevent overflow and underflow\n",
        "att_weights_2 = torch.softmax(attn_scores_2, dim = 0)\n",
        "print(\"Attention weights: \", att_weights_2)\n",
        "print(\"Sum: \", att_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O7ECE_bXHUx",
        "outputId": "e5160cf6-663a-4817-c17e-59ee68e7d424"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   After we computed the normalized attention weights, we are ready to\n",
        "# calculate the context vector z, by multiplying the embedded input tokens, x\n",
        "query = inputs[1]\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2_naive[i] * x_i\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6CW8QlRS5K",
        "outputId": "71ec6b11-60a4-429c-ecb6-52f3022a583b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 - Computing attention weights for all input tokens"
      ],
      "metadata": {
        "id": "AFgcRSTbSd8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Let's extend this computation to calculate attention weights and context\n",
        "# vectors for all inputs\n",
        "attn_scores = torch.empty(6, 6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_qu2uCBSI6L",
        "outputId": "067c07da-cc33-442d-b77a-2d32ff7a1775"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Each element in the tensor represents an attention score beween each pair\n",
        "# of inputs.  The values are normalized, which is why they differ from\n",
        "# unnormalized attention scores in the preceding tensor.\n",
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgzV0UQtWURe",
        "outputId": "480a7d05-7fe2-4b7d-9942-31eb9aa3dd68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim = 1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGbtusrTX19K",
        "outputId": "33885265-21bc-4c46-e911-10ac8c313acb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   By setting dim = -1, we are instructing the softmax function to apply\n",
        "# normalization along the last dimension of the attn_scores tensor.\n",
        "\n",
        "# Verify that the rows indeed sum to 1:\n",
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum: \", row_2_sum)\n",
        "print(\"All row sums: \", attn_weights.sum(dim = -1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fihSXc3sX7dO",
        "outputId": "ad67479a-926d-402b-a606-95460ce1ea05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum:  1.0\n",
            "All row sums:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   In the third and final step we use these attention weights to comput all\n",
        "# context vectors via matrix multiplication\n",
        "all_context_vectors = attn_weights @ inputs\n",
        "print(all_context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54vfoL25Ys4N",
        "outputId": "65c6dcc2-07b4-4830-efb5-1869a3c5be70"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can doubl-check that the code is correct\n",
        "print(\"Previous 2nd context vector: \", context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt5a2z09ZYO1",
        "outputId": "2c630a97-18a3-4cd6-a1d4-1e862cb14c03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector:  tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 - Implementing self-attention with trainable weights\n",
        "\n",
        "  -"
      ],
      "metadata": {
        "id": "Kw-ijIBUZj6C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6gPTljwZi1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}