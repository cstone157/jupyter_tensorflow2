{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### -----\n",
        "#### Build a Large Language Model\n",
        "#### Sebastian Raschka\n",
        "#### -----\n",
        "\n",
        "# Coding attention mechanisms\n",
        "\n",
        "  - This chapter will focus on coding the remaining parts of the LLMs surrounding the self-attention mechanism to see it in action and to create a model to generate text.\n",
        "  - Different types of attention mechanisms\n",
        "    - Simplified self-attention:\n",
        "    - Self-attention: Simplified self-attention with trainable weights\n",
        "    - Casual attention: Type of self-attention used in LLMs that allows a model to consider on previous and current inputs in a sequence\n",
        "    - Multi-head attention:\n",
        "\n",
        "## 3.1 - The problem with modeling long sequences\n",
        "\n",
        "  - You can't simply translate a text word into another language due to the grammatical structures in source and target language.\n",
        "  - Deep neural network with two submodules, encoder and decoder:\n",
        "    - encode : first read in and process the entire text\n",
        "    - decoder : then produces the translated text.\n",
        "  - Before transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.\n",
        "\n",
        "## 3.2 - Capturing data dependencies with attention mechanisms\n",
        "\n",
        "  - Bahdanau attention mechanism : modifies the encoder-decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step.\n",
        "  - Self-attention is a mechanism that allows each position in the input sequence when computing the representation of a sequence.\n",
        "\n",
        "## 3.3 - Attending to different parts of the input with self-attention\n",
        "\n",
        "  - In self-attention, the \"self\" refers to the mechanism's ability to compute attention weights by relating different positions within a single input sequence.\n",
        "\n",
        "### 3.3.1 - A simple self-attention mechanism without trainable weights\n",
        "\n",
        "  - In self-attention, our goal is to calculate context vectors for each element x in the input sequence.  A context vector can be interpreted as an enrighted embedding vector.\n",
        "  - Context vectors play a crucial role in self-attention.  Their purpose is to create enriched representations of each element in an input sequence by incorporating information from all other elsements in the sequence.\n",
        "  - The first step of implementing self-attention is to compute the intermediate values w, referred to as attention scores."
      ],
      "metadata": {
        "id": "_dZv9slceG97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ApPghy3neA0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfc96a0-c12a-4aa6-bf90-510f8741ab72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# small embedding dimension\n",
        "import torch\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89],  # Your       (x^1)\n",
        "     [0.55, 0.87, 0.66],  # journey    (x^2)\n",
        "     [0.57, 0.85, 0.64],  # starts     (x^3)\n",
        "     [0.22, 0.58, 0.33],  # with       (x^4)\n",
        "     [0.77, 0.25, 0.10],  # one        (x^5)\n",
        "     [0.05, 0.80, 0.55]]  # step       (x^6)\n",
        ")\n",
        "\n",
        "query = inputs[1]\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query)\n",
        "attn_scores_2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next step is to normalize each of the attention scores we computed previously\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "print(\"Attention weights: \", attn_weights_2_tmp)\n",
        "print(\"Sum: \", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oukr-kHrQVfd",
        "outputId": "150d343d-d87a-449e-c25e-65ee690b988d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   More common to use the softmax function for normalization.\n",
        "#   Additionally, softmax function ensures that the attention weights are always\n",
        "# positive.\n",
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "print(\"Attention weights: \", attn_weights_2_naive)\n",
        "print(\"Sum: \", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOMzXwmkQx2q",
        "outputId": "a638591b-0379-470e-e540-0b08a33d1966"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   It's advisable to use PyTorch of softmax to prevent overflow and underflow\n",
        "att_weights_2 = torch.softmax(attn_scores_2, dim = 0)\n",
        "print(\"Attention weights: \", att_weights_2)\n",
        "print(\"Sum: \", att_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3O7ECE_bXHUx",
        "outputId": "07450177-1022-4390-ea60-884751c0bfd1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   After we computed the normalized attention weights, we are ready to\n",
        "# calculate the context vector z, by multiplying the embedded input tokens, x\n",
        "query = inputs[1]\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2_naive[i] * x_i\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW6CW8QlRS5K",
        "outputId": "c015a308-1d9c-4f25-cb1d-e3584aa15d62"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 - Computing attention weights for all input tokens"
      ],
      "metadata": {
        "id": "AFgcRSTbSd8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Let's extend this computation to calculate attention weights and context\n",
        "# vectors for all inputs\n",
        "attn_scores = torch.empty(6, 6)\n",
        "for i, x_i in enumerate(inputs):\n",
        "  for j, x_j in enumerate(inputs):\n",
        "    attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_qu2uCBSI6L",
        "outputId": "79dc0347-2ea5-4444-f1c7-e19bea2756d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Each element in the tensor represents an attention score beween each pair\n",
        "# of inputs.  The values are normalized, which is why they differ from\n",
        "# unnormalized attention scores in the preceding tensor.\n",
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgzV0UQtWURe",
        "outputId": "6b725a55-112b-41b0-833e-49ec0010f121"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim = 1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGbtusrTX19K",
        "outputId": "991ac88d-050b-4423-f95f-14e2f34731c6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   By setting dim = -1, we are instructing the softmax function to apply\n",
        "# normalization along the last dimension of the attn_scores tensor.\n",
        "\n",
        "# Verify that the rows indeed sum to 1:\n",
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum: \", row_2_sum)\n",
        "print(\"All row sums: \", attn_weights.sum(dim = -1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fihSXc3sX7dO",
        "outputId": "04be81ad-db0c-42d8-e87a-8981fdeafb58"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum:  1.0\n",
            "All row sums:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   In the third and final step we use these attention weights to comput all\n",
        "# context vectors via matrix multiplication\n",
        "all_context_vectors = attn_weights @ inputs\n",
        "print(all_context_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54vfoL25Ys4N",
        "outputId": "3b225da3-83c7-482d-e3bf-ae9f5d3c6831"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can doubl-check that the code is correct\n",
        "print(\"Previous 2nd context vector: \", context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt5a2z09ZYO1",
        "outputId": "17adc782-e03c-4dea-8935-c33ee3bf2431"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector:  tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 - Implementing self-attention with trainable weights\n",
        "\n",
        "  - Next implement the self-attention mechanism called <i>scaled dot-product</i>.\n",
        "  - Self-attention mechanism with trainable weights builds on the previous concepts: we want to compute context vectors as weighted sums over the input vectors specific to a certain input elements.\n",
        "  - The most notable difference is the introduction of weight matrices that are updated during model training.  These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce \"good\" context vectors.\n",
        "\n",
        "### 3.4.1 - Computing the attention weights step by step\n",
        "\n",
        "  - Implement the self-attention mechanism step by step by introducing the tree trainable weight matrices wq, wk, and wv.  These 3 matrices are used to project the embedded input tokens x(i), into query, key and value vectors."
      ],
      "metadata": {
        "id": "Kw-ijIBUZj6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   We start by computing only one context vector, z(2)\n",
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2\n",
        "\n",
        "#   Next initialize the three weight matrices wq, wk, wv\n",
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "#   Set requires_grad = False, to reduce clutter in the outputs, but if we were\n",
        "# to use the weight matrices for model training, we would set requires_grad = True\n",
        "\n",
        "#   Compute the query, key, and value vectors\n",
        "query_2 = x_2 @ W_query\n",
        "key_2   = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6gPTljwZi1V",
        "outputId": "2474e504-f29e-45aa-f0b1-af72675ddb55"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  We can obtain all keys and values via matrix multiplication:\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"keys.shape: \", keys.shape)\n",
        "print(\"values.shape: \", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn323JFtfTrq",
        "outputId": "51e3775f-8e6d-4953-dafb-7a38995a0e0d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape:  torch.Size([6, 2])\n",
            "values.shape:  torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The next step is to comput the attention score w22:\n",
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-SPnX_4flve",
        "outputId": "5673a237-5fa2-4e23-abd2-1c7119edc9c3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   We can generalize this computation to all attention scores via\n",
        "# matrix multiplication\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzRMSh4Yf1jb",
        "outputId": "870996a8-5da9-4db7-ff0b-f253a2d82656"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   We can compute the attention weights by scaling the attention scores and\n",
        "# using the softmax function.\n",
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim = -1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TROWi_FugHsn",
        "outputId": "5c502c80-3df2-4a3e-cf1f-72d419c7fecd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   The final step is to comput the context vectors\n",
        "#   We compute the context vector as weighted sum over the input vectors\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjVcdMYagf3O",
        "outputId": "9b971d06-a19a-4c0b-ca49-70b21256b5c4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.2 - Implementing a compact self-attention Python class"
      ],
      "metadata": {
        "id": "Hg_YDFkgg7IO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A compact self-attention class\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys        = x @ self.W_key\n",
        "    queries     = x @ self.W_query\n",
        "    values      = x @ self.W_value\n",
        "    attn_scores = queries @ keys.T #omega\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim = -1\n",
        "    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "RkbW-1rQg5BG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use this class as follows:\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2X-qeNiiZoW",
        "outputId": "fb9e64ab-703b-4782-981f-86440200cb9b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Self-attention involves the trainable weight matrices wq, wk, and wv.\n",
        "  - These matrices transform input data into queries, keys, and values, repectively, which are crucial components of the attention mechanism.\n",
        "  - We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias unites are disabled.  \n",
        "  - Additionall, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training."
      ],
      "metadata": {
        "id": "gw2u2N9OlRvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### TO-DO : ERROR STARTS HERE\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys        = self.W_key(x)\n",
        "    queries     = self.W_query(x)\n",
        "    values      = self.W_value(x)\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim = -1\n",
        "    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTv3YsS4lEjH",
        "outputId": "197d21da-fce8-4aff-8d0c-7dc544208e47"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5337, -0.1051],\n",
            "        [-0.5323, -0.1080],\n",
            "        [-0.5323, -0.1079],\n",
            "        [-0.5297, -0.1076],\n",
            "        [-0.5311, -0.1066],\n",
            "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - v1 and v2 give different outputs because they use different initial weights.\n",
        "\n",
        "## 3.5 - Hiding future words with causal attention\n",
        "\n",
        "  - For many LLM tasks, you will want the self-attention mechanism to consider only tokens that appear prior to the current position when predicting the next token in a sequence.\n",
        "  - Casual attention (masked attention) is specialized form of self-attention.\n",
        "    - It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores.\n",
        "\n",
        "### 3.5.1 - Applying a casual attention mask"
      ],
      "metadata": {
        "id": "hh9KzJSpovtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the attentoion weights using the softmax function as we have done before\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkCCK8isodhR",
        "outputId": "fa074668-c036-4b71-c665-6a3ae744b979"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
            "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
            "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
            "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
            "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use PyTorch's tril function to create a mask where the values above the diagonal are zero\n",
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6woEVJytmcZv",
        "outputId": "4248b18b-d690-4e9a-d773-d747e18e05d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now multiply this mask with the attention weights to zero-out the values above the diagonal:\n",
        "mask_simple = attn_weights * mask_simple\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReZTs0NAnBRP",
        "outputId": "b41b86a8-abee-4379-95f9-e12a57d93998"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The next step is to renormalize the attention weights to sum up to 1 again in each row.\n",
        "row_sums = mask_simple.sum(dim = -1, keepdim = True)\n",
        "masked_simple_norm = mask_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOs8XkxtnNjK",
        "outputId": "27453928-a320-448f-bd19-f6e5a1c35e8c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
            "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   To improve our casual attention, let's take a mathematical property of the\n",
        "# softmax function and implement the computation of the masked attention weights\n",
        "# more effivciently in fewer steps\n",
        "#   The softmax function converts into an probablility distribution.  When\n",
        "# negative infinity values are preseent in a row, it treats them as zer.\n",
        "#   Implement this by creating a mask with 1s above the diagonal the then\n",
        "# replacing these 1s with negative infinity\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrWeOPQuoiNR",
        "outputId": "d364fdfe-bfab-42d5-ad4f-e25c29810ae7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
            "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
            "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
            "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   No apply the softmax function\n",
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim = -1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rERwTJQ0px6j",
        "outputId": "1907f0a6-3cc7-4d3a-c33a-55ed43849c02"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
            "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
            "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.2 - Masking additional attention weights with dropout\n",
        "\n",
        "  -"
      ],
      "metadata": {
        "id": "UlInriPXrmCE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8LK5g1LWp9OM"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}