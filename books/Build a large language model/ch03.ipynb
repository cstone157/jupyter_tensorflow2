{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### -----\n",
        "#### Build a Large Language Model\n",
        "#### Sebastian Raschka\n",
        "#### -----\n",
        "\n",
        "# Coding attention mechanisms\n",
        "\n",
        "  - This chapter will focus on coding the remaining parts of the LLMs surrounding the self-attention mechanism to see it in action and to create a model to generate text.\n",
        "  - Different types of attention mechanisms\n",
        "    - Simplified self-attention:\n",
        "    - Self-attention: Simplified self-attention with trainable weights\n",
        "    - Casual attention: Type of self-attention used in LLMs that allows a model to consider on previous and current inputs in a sequence\n",
        "    - Multi-head attention:\n",
        "\n",
        "## 3.1 - The problem with modeling long sequences\n",
        "\n",
        "  - You can't simply translate a text word into another language due to the grammatical structures in source and target language.\n",
        "  - Deep neural network with two submodules, encoder and decoder:\n",
        "    - encode : first read in and process the entire text\n",
        "    - decoder : then produces the translated text.\n",
        "  - Before transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.\n",
        "\n",
        "## 3.2 - Capturing data dependencies with attention mechanisms\n",
        "\n",
        "  -"
      ],
      "metadata": {
        "id": "_dZv9slceG97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApPghy3neA0x"
      },
      "outputs": [],
      "source": []
    }
  ]
}