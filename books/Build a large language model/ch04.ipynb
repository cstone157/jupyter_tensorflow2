{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eQc16KdAYJNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c6eafc-0ab5-4c14-dc97-718b2814e87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "print(\"torch version:\", version(\"torch\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7oYbOT2QBij",
        "outputId": "3fd0e09b-0818-45ea-e244-a932ff850ba5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a GPT model from scrath to generate text\n",
        "\n",
        "## 4.1 - Coding an LLM architecture\n",
        "\n",
        "  - GPT (generative pretrained transformer), are large deep neural network architectures designed to generate new text one word at a time."
      ],
      "metadata": {
        "id": "YiD4RmnsQNvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Specify the configuration of the small GPT-2 model via the following\n",
        "# Python dictionary, which we will use in the code examples later:\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "FbWE8_elQEvC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The GPT_CONFIG_124M dictionary, we use concise variable names for clarity and to prevent long lines of code\n",
        "    - vocab_size : refers to a vocabulary of 50257 words, as used by the BPE tokenizer\n",
        "    - context_length : denotres the maximum number of input tokens the model can handle via the positional embeddings\n",
        "    - emb_dim : represents the embedding size, transforming each token into a 768-dimensional vector.\n",
        "    - n_heads : indicates the count of attention heads in the multi-head attention mechanism\n",
        "    - n_layer : specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.\n",
        "    - drop_rate : indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop_out of hidden units) to prevent overfitting\n",
        "    - qkv_bias determines whether to include a bias vector in the Linear layers of the multi-head attention for query, key, and value computations.  We will initially disable this, following the norms of modern LLMs"
      ],
      "metadata": {
        "id": "50qqvPfgTx7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A placeholder GPT model architecture class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(cfg)\n",
        "          for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(\n",
        "        torch.arange(seq_len, device=in_idx.device)\n",
        "    )\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "aToy2cY2TvB9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - This code defines a simplified version of a GPT-like model using PyTorch's neural network module.\n",
        "    - DummyGPTModel class consits of token and positional embeddings, droupout, a series of transformer blocks, a final layer normalization, and a linear output layer.\n",
        "    - The forward method describes the data flow through the model: it computes token and positional embeddings for the input indices, applies dropout, processes the data through the transformer blocks, applies normalization, and finally produces logits with linear output layer.\n",
        "  - Next prepare the input data and initialize a new GPT model to illustrate its usage."
      ],
      "metadata": {
        "id": "gHq6z1UbdieD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize a batch consisting of two text inputs for the GPT model using the tiktoken\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoEOcFxTdc1H",
        "outputId": "a46504fa-266d-414e-9880-9185f4613816"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Initialize a new 124-million-parameter DummyGPTModel instance and feed it\n",
        "# the tokenized batch:\n",
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Output shape: \", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd-bvM5efTb-",
        "outputId": "051144c4-7f0c-4c2b-f2c3-b1594549953c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape:  torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The ouput tensor has two rows corresponding to the two text samples.\n",
        "\n",
        "## 4.2 - Normalizing activations with layer normalization\n",
        "\n",
        "  - One of the problems with training deep neural networks with may layers can sometimes prove challenging due to problems like vanishing or exploding gradients.\n",
        "  - Layer normalization to improve the stability and efficiency of neural network training.\n",
        "    - layer normalization - adjust the activation (outputs) of a neural network layer to have a mean of 0 and a variance of 1"
      ],
      "metadata": {
        "id": "F1fHolpPgWX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an example\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_example = torch.randn(2, 5)\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2paF_8Qgf5jf",
        "outputId": "afbbe6e3-00eb-48d9-812a-f3177bc41cf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Before applying layer normalization, review mean and variance:\n",
        "\n",
        "#   The keepdim = True in operations like mean or variance calculation ensures\n",
        "# ensures ouput tensor retains the same number of dimensions as the input tensor\n",
        "\n",
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZC8usDniMhw",
        "outputId": "f44c9243-deea-4de4-923e-874900ef9964"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   dim = -1 for operations such as mean or variance calculations is the same as dim=1\n",
        "\n",
        "#   Apply layer normalization to the layer outputs\n",
        "\n",
        "out_norm = (out - mean) / torch.sqrt(var)\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "567Z3aCdiiEe",
        "outputId": "8edd32fd-6240-42e8-d8fe-160eb47a6ba3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Improve readability, we turn off scientific notation\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulHL_imzjiPN",
        "outputId": "c204a52b-be92-4ee3-859f-278588b594a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    0.0000],\n",
            "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Encapsulate this process in a PyTorch module that we can use in GPT model\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n"
      ],
      "metadata": {
        "id": "ndHkD4FyjvhK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The scale and shift are two trainable parameters (of the same dimension as the input) that the LLM automatically adjusts during training if it is determined that doing so would improve the model's performance on its training task."
      ],
      "metadata": {
        "id": "3rqqKGwOkZo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Let's now try the LayerNorm module in practice and apply it to the batch input:\n",
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbjX4KJ4kVlo",
        "outputId": "88fb18bd-971d-47b4-8019-57cfbbaa5c81"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    -0.0000],\n",
            "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOnL_VHWlIKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}