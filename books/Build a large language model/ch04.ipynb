{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eQc16KdAYJNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a6487a-d4c8-4c40-ae32-2e21bcab2b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "print(\"torch version:\", version(\"torch\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7oYbOT2QBij",
        "outputId": "0db27240-4174-4c68-f9a0-a123fd76a4f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing a GPT model from scrath to generate text\n",
        "\n",
        "## 4.1 - Coding an LLM architecture\n",
        "\n",
        "  - GPT (generative pretrained transformer), are large deep neural network architectures designed to generate new text one word at a time."
      ],
      "metadata": {
        "id": "YiD4RmnsQNvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Specify the configuration of the small GPT-2 model via the following\n",
        "# Python dictionary, which we will use in the code examples later:\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "FbWE8_elQEvC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The GPT_CONFIG_124M dictionary, we use concise variable names for clarity and to prevent long lines of code\n",
        "    - vocab_size : refers to a vocabulary of 50257 words, as used by the BPE tokenizer\n",
        "    - context_length : denotres the maximum number of input tokens the model can handle via the positional embeddings\n",
        "    - emb_dim : represents the embedding size, transforming each token into a 768-dimensional vector.\n",
        "    - n_heads : indicates the count of attention heads in the multi-head attention mechanism\n",
        "    - n_layer : specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.\n",
        "    - drop_rate : indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop_out of hidden units) to prevent overfitting\n",
        "    - qkv_bias determines whether to include a bias vector in the Linear layers of the multi-head attention for query, key, and value computations.  We will initially disable this, following the norms of modern LLMs"
      ],
      "metadata": {
        "id": "50qqvPfgTx7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A placeholder GPT model architecture class\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "aToy2cY2TvB9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}