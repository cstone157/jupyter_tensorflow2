{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOT7LUfl8df1xa5isTh3Rj3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["-----\n","Build a Large Language Model\n","Sebastian Raschka\n","-----\n","\n","# Working with text data\n","\n","  - During the pretraining stage, LLMs process text, one word at a time.\n","\n","## 2.1 - Understanding word embeddings\n","\n","  - Converting data into a vector format is referred to as embedding.  An embedding is a mapping from discrete objects, such as words, images, or even entire documents, to point in a continuous vector space - the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.\n","  - In addition to word embeddings, there are also embedding for sentences, paragraphs, or whole documents.\n","  - Retrieval-augmented generation combines generation (like producting text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.\n","\n","## 2.2 - Tokenizing text\n","\n","  - We will tokenize \"The Verdict,\" from https://en.wikisource.org/wiki/The_Verdict\n"],"metadata":{"id":"KYlZjvsSZr88"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhIOhqVnZrW8","executionInfo":{"status":"ok","timestamp":1754715941985,"user_tz":420,"elapsed":82,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"0c110335-5fbe-46e3-b0f5-959269788825"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}],"source":["# Download our text\n","import urllib.request\n","url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n","file_path = \"the-verdict.txt\"\n","urllib.request.urlretrieve(url, file_path)\n","\n","# Now load our text file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    raw_text = file.read()\n","print(\"Total number of characters:\", len(raw_text))\n","print(raw_text[:99])"]},{"cell_type":"markdown","source":["  - Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training."],"metadata":{"id":"noJXRYhfaFM2"}},{"cell_type":"code","source":["# Simple example text using re.split command with the following syntax\n","import re\n","text = \"Hello, world.  This, is a test.\"\n","result = re.split(r'(\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeWk1J7vKtcW","executionInfo":{"status":"ok","timestamp":1754715941987,"user_tz":420,"elapsed":10,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"910441f7-1670-4035-859a-cdd72c2704f3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello,', ' ', 'world.', ' ', '', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"]}]},{"cell_type":"markdown","source":["  - Modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.])\n","  - Capitalization helps LLMs distinguish between proper nouns and common nouns, so we will refrain from making all text lowercase."],"metadata":{"id":"ZOmG-ymmamEP"}},{"cell_type":"code","source":["result = re.split(f'([,.]|\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSmfgx0_ai56","executionInfo":{"status":"ok","timestamp":1754715941992,"user_tz":420,"elapsed":4,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"53c6f442-3cc9-4556-8191-6bea004116e7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"]}]},{"cell_type":"markdown","source":["  - Remove all the whitespace characters"],"metadata":{"id":"65Yg8SYbbSrC"}},{"cell_type":"code","source":["result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iM0DbembMT3","executionInfo":{"status":"ok","timestamp":1754715942104,"user_tz":420,"elapsed":7,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"aef1081c-bca6-40c9-c29d-bad03db3c0a6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"]}]},{"cell_type":"markdown","source":["  - Need to modify the exmple further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double dashes we have seen earlier in the first 100 characters"],"metadata":{"id":"TknIjlo8bnIE"}},{"cell_type":"code","source":["text = \"Hello, world.  Is this -- a test?\"\n","result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIc6NAiUbeVS","executionInfo":{"status":"ok","timestamp":1754715942111,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"cfe78195-b0b0-4d6e-fb3e-3c9462a607f2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"]}]},{"cell_type":"markdown","source":["- Apply the basic tokenizer to the story"],"metadata":{"id":"y_Ga5zYFcL5w"}},{"cell_type":"code","source":["preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(len(preprocessed))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sPs-uHNcGbn","executionInfo":{"status":"ok","timestamp":1754715942120,"user_tz":420,"elapsed":8,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"f39ccc74-58ad-4ba8-8327-b630897376c5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["4690\n"]}]},{"cell_type":"markdown","source":["  - Let's print the first 30 tokens for a quick visual check"],"metadata":{"id":"uez6G3vii6BI"}},{"cell_type":"code","source":["print(preprocessed[:30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7DdEef-i0oh","executionInfo":{"status":"ok","timestamp":1754715942278,"user_tz":420,"elapsed":156,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"e639b10b-a338-40e2-cc68-d4352363e39f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"]}]},{"cell_type":"markdown","source":["## 2.3 - Converting tokens into token IDs\n","\n","  - Next let's convert these tokens from a Python string to an integer representation to produce the token IDs."],"metadata":{"id":"o1HvaSd1jqRX"}},{"cell_type":"code","source":["all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","print(f\"Vocab size: {vocab_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffdVi2azjAS-","executionInfo":{"status":"ok","timestamp":1754715942285,"user_tz":420,"elapsed":4,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"01ef48a4-d566-437d-944a-2014284c17e3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 1130\n"]}]},{"cell_type":"markdown","source":["  - Create a vocabulary and print the first 51 entries."],"metadata":{"id":"xnmiz2I7kIU5"}},{"cell_type":"code","source":["vocab = {token:integer for integer, token in enumerate(all_words)}\n","for i, item in enumerate(vocab.items()):\n","  print(item)\n","  if i >= 50:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoduyISskD6V","executionInfo":{"status":"ok","timestamp":1754715942292,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"01cb9348-eb42-40d8-d963-3e50aa183d43"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"markdown","source":["  - Our next goal is to apply this vocabulary to convert new text into token IDs.\n","  - Let's implete a tokenizer class in Python with an encode method.\n","  - Also create a decode method, so we can reverse this process."],"metadata":{"id":"RpaWpINskgyz"}},{"cell_type":"code","source":["class SimpleTokenizerV1:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()}\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"aQvyGQ5akdja","executionInfo":{"status":"ok","timestamp":1754715942448,"user_tz":420,"elapsed":155,"user":{"displayName":"culver stone","userId":"14998972164611572123"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["  - Using the SimpleTokenizerV1, we can intantiate new tokenizer objects via an existing vocabulary"],"metadata":{"id":"rZYk0-Yslsx5"}},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV1(vocab)\n","text = \"\"\"\"It's the last he painted, you know,\"\n","        Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS9zO47Tlrlt","executionInfo":{"status":"ok","timestamp":1754715942450,"user_tz":420,"elapsed":155,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"b35706a2-008a-452e-df13-fa31bf7f81ed"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"]}]},{"cell_type":"markdown","source":["  - Next ensure we can turn the token IDs back into text"],"metadata":{"id":"N4jwqYCGmHRB"}},{"cell_type":"code","source":["print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUUuZDGhmEL7","executionInfo":{"status":"ok","timestamp":1754715942451,"user_tz":420,"elapsed":150,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"1717999b-e628-4ca8-d484-51df8e617958"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"]}]},{"cell_type":"markdown","source":["  - Let's now apply our tokenizer to a new text sample."],"metadata":{"id":"k6xX9-x-mZlP"}},{"cell_type":"code","source":["text = \"Hello, do you like tea?\"\n","\n","try:\n","  print(tokenizer.encode(text))\n","except Exception as e:\n","  print(f\"unknown token {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ktn-kmJVmNEv","executionInfo":{"status":"ok","timestamp":1754715942452,"user_tz":420,"elapsed":146,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"d2be8f24-1141-4fc4-f0e0-d037de0bf661"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["unknown token 'Hello'\n"]}]},{"cell_type":"markdown","source":["  - Throws and error since \"Hello\" was not in our short story, hence it's not in our vocabulary.\n","\n","## 2.4 - Adding special context tokens\n","\n","  - We need to modify the tokenizer to handle unknown words.\n","  - Modify the tokenizer to handle two special tokesn, <unk> and <|endoftext|>"],"metadata":{"id":"bViZH38Jmkmx"}},{"cell_type":"code","source":["all_tokens = sorted(list(set(preprocessed)))\n","all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n","vocab = {token:integer for integer, token in enumerate(all_tokens)}\n","\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7s8k4uvMmjzR","executionInfo":{"status":"ok","timestamp":1754715942458,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"e8495feb-1edc-40c2-a9af-5b19ceae7672"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["1132\n"]}]},{"cell_type":"markdown","source":["  - The new vocabulary size is 1132 (the previous vocabulary size was 1130)"],"metadata":{"id":"nE8CSu7euzXu"}},{"cell_type":"code","source":["for i, item in enumerate(list(vocab.items())[-5:]):\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zecndQ9RuzJz","executionInfo":{"status":"ok","timestamp":1754715942464,"user_tz":420,"elapsed":4,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"97c09558-b151-4235-b669-cf856c99fe87"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["('younger', 1127)\n","('your', 1128)\n","('yourself', 1129)\n","('<|endoftext|>', 1130)\n","('<|unk|>', 1131)\n"]}]},{"cell_type":"markdown","source":["  - Base on the ouput, we can confirm that the two new special tokens were indeed successfully incorporated into the vocab."],"metadata":{"id":"Dy9C1laWvbc0"}},{"cell_type":"code","source":["class SimpleTokenizerV2:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = { i:s for s,i in vocab.items() }\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    preprocessed = [item if item in self.str_to_int\n","                    else \"<|unk|>\" for item in preprocessed]\n","\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"U1cGNVYgvcur","executionInfo":{"status":"ok","timestamp":1754715942468,"user_tz":420,"elapsed":2,"user":{"displayName":"culver stone","userId":"14998972164611572123"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["  - Compared to v1, v2 replaces unknown words with <|unk|> tokens."],"metadata":{"id":"1zz_1YG0wIwi"}},{"cell_type":"code","source":["text1 = \"Hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","text = \" <|endoftext|> \".join((text1, text2))\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlsYev7YwH5y","executionInfo":{"status":"ok","timestamp":1754715942474,"user_tz":420,"elapsed":4,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"1debc700-7e54-421f-d27e-ffacca4e40d4"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]}]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV2(vocab)\n","print(tokenizer.encode(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMLJ3WOdwdH9","executionInfo":{"status":"ok","timestamp":1754715942481,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"509666a0-0ebb-4665-e12d-7df839b7a0c5"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"]}]},{"cell_type":"code","source":["strings = tokenizer.decode(tokenizer.encode(text))\n","print(strings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BslPebcZwidy","executionInfo":{"status":"ok","timestamp":1754715942490,"user_tz":420,"elapsed":7,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"ee62a1f3-3e63-4d88-f96b-30fb4f20d994"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"]}]},{"cell_type":"markdown","source":["  - Depending on the LLM, some researchers also consider additional special tokens such as:\n","    - [BOS] (beginning of sequence) - marks the start of text\n","    - [EOS] (end of sequence) - positionaed at the end of a text is especially useful when concatenating multiple unreleated text similar to <|endoftext|>.\n","    - [PAD] (padding) - when training LLMs the batch sizes can vary, to ensure all texts have the same length, the shorter texts are extended or \"padded\".\n","\n","## 2.5 Byte pair encoding\n","\n","  - Since BPE (byte pair encoding) is complicated we will use an existing python library called toktoken.  The code is based on tiktoken 0.7.0, check the version you currently have installed."],"metadata":{"id":"vlZeVKfPxGHA"}},{"cell_type":"code","source":["pip install tiktoken==0.7.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPwjLDRww0Mu","executionInfo":{"status":"ok","timestamp":1754715949238,"user_tz":420,"elapsed":6750,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"81549ed3-8cf8-4f2a-af40-6d5c19338598"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken==0.7.0\n","  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2025.8.3)\n","Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","  Attempting uninstall: tiktoken\n","    Found existing installation: tiktoken 0.10.0\n","    Uninstalling tiktoken-0.10.0:\n","      Successfully uninstalled tiktoken-0.10.0\n","Successfully installed tiktoken-0.7.0\n"]}]},{"cell_type":"code","source":["from importlib.metadata import version\n","import tiktoken\n","print(\"tiktoken version:\", version(\"tiktoken\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B95EH_eyyb7H","executionInfo":{"status":"ok","timestamp":1754715949265,"user_tz":420,"elapsed":26,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"5161bb4b-7de8-40da-c040-a1497e3a23d2"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tiktoken version: 0.7.0\n"]}]},{"cell_type":"code","source":["# Instantiate the BPE tokenizer\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","# Demonstrated usage of tokenizer\n","text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces \"\n","    \"of someunknownPlace.\"\n",")\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5wqkrhuypJg","executionInfo":{"status":"ok","timestamp":1754715951797,"user_tz":420,"elapsed":2531,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"2292c3da-e658-4517-b25d-fcba00e4288f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":["# Convert the token IDs back into text using the decode method\n","strings = tokenizer.decode(integers)\n","print(strings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwwpgvZozTvg","executionInfo":{"status":"ok","timestamp":1754715951797,"user_tz":420,"elapsed":19,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"62f995ac-307a-4425-ac39-02f34d9d3824"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"]}]},{"cell_type":"markdown","source":["  - The <|endoftext|> is assigned a relatively large token ID.  The total vocabulary is 50257\n","  - The BPE tokenizer encodes and decodes unknown words, such as someunknownPlace corrrectly.\n","  - BPE breaks down words that aren't in its predifined vocab into smaller subword units or even individual characters.\n","  - The BPE builds its vocab by iteratively merging frequent characters into sub-words and frequent subwords into words.\n","\n","## 2.6 - Data sampling with a sliding window\n","\n","  - The next step in creating the embeddings for LLM is to generate a input-target pairs required for training an LLM."],"metadata":{"id":"TrAsZFkUyfuU"}},{"cell_type":"code","source":["with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n","  text = file.read()\n","\n","enc_text = tokenizer.encode(raw_text)\n","print(len(enc_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3ezgmEcyTIb","executionInfo":{"status":"ok","timestamp":1754715951798,"user_tz":420,"elapsed":14,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"02db9947-d709-403f-d7a6-fcfa115f5b92"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["5145\n"]}]},{"cell_type":"code","source":["#   Remove the first 50 tokens from the dataset for demonstration purposes, as it\n","# results in a slightly more interesting text passage\n","enc_sample = enc_text[50:]\n","\n","#   Create two variable x and y, where x contains the input tokens and y contains\n","# the targets\n","context_size = 4\n","x = enc_sample[:context_size]\n","y = enc_sample[1:context_size+1]\n","\n","print(f\"x: {x}\")\n","print(f\"y:      {y}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-RTVQA9zqzW","executionInfo":{"status":"ok","timestamp":1754715951803,"user_tz":420,"elapsed":8,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"d7ffb4d5-e9ee-4462-cf16-7fa53eb2388e"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["x: [290, 4920, 2241, 287]\n","y:      [4920, 2241, 287, 257]\n"]}]},{"cell_type":"code","source":["#   By processing the inputs along with targets, we can create the next-word\n","# prediction tasks\n","for i in range(1, context_size+1):\n","  context = enc_sample[:i]\n","  desired = enc_sample[i]\n","  print(context, \"---->\", desired)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCgLSRxd0Iyf","executionInfo":{"status":"ok","timestamp":1754715951813,"user_tz":420,"elapsed":10,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"f831df67-f88b-406a-decf-9bfc035004c2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[290] ----> 4920\n","[290, 4920] ----> 2241\n","[290, 4920, 2241] ----> 287\n","[290, 4920, 2241, 287] ----> 257\n"]}]},{"cell_type":"code","source":["#   Everything left of the arrow refers to the input, and the token id on the right\n","# side represents the target token ID that the LLM is supposed to predict.\n","\n","for i in range(1, context_size+1):\n","  context = enc_sample[:i]\n","  desired = enc_sample[i]\n","  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxMKpVyi49dI","executionInfo":{"status":"ok","timestamp":1754715951819,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"439c32a9-5d85-4559-d30a-48a6d496281e"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":[" and ---->  established\n"," and established ---->  himself\n"," and established himself ---->  in\n"," and established himself in ---->  a\n"]}]},{"cell_type":"markdown","source":["  - The last task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors."],"metadata":{"id":"7e0TElAR51Ma"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class GPTDatasetV1(Dataset):\n","  def __init__(self, txt, tokenizer, max_length, stride):\n","    self.input_ids = []\n","    self.target_ids = []\n","\n","    token_ids = tokenizer.encode(txt)\n","    for i in range(0, len(token_ids) - max_length, stride):\n","      input_chunk = token_ids[i:i + max_length]\n","      target_chunk = token_ids[i + 1: i + max_length + 1]\n","      self.input_ids.append(torch.tensor(input_chunk))\n","      self.target_ids.append(torch.tensor(target_chunk))\n","\n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.target_ids[idx]"],"metadata":{"id":"Wy2mqw4Q5spN","executionInfo":{"status":"ok","timestamp":1754715956507,"user_tz":420,"elapsed":4678,"user":{"displayName":"culver stone","userId":"14998972164611572123"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["  - GPTDatasetV1 is based on PyTorch Dataset class and defines how individual rows are fetched."],"metadata":{"id":"WfAJCJ9ENB4K"}},{"cell_type":"code","source":["def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","  tokenizer = tiktoken.get_encoding(\"gpt2\")\n","  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","  dataloader = DataLoader(\n","      dataset,\n","      batch_size=batch_size,\n","      shuffle=shuffle,\n","      drop_last=drop_last,\n","      num_workers=num_workers\n","  )\n","\n","  return dataloader\n","\n","# Test dataloader with batch size of 1 for an LLM\n","with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","  raw_text = f.read()\n","\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",")\n","data_iter = iter(dataloader)\n","first_batch = next(data_iter)\n","print(first_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNshQQD76NNk","executionInfo":{"status":"ok","timestamp":1754715956712,"user_tz":420,"elapsed":194,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"3649152a-8ba6-46be-bec8-722cfeb9a2a8"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"]}]},{"cell_type":"code","source":["#   The first_batch contains two tensors: the first tensor stores the input token\n","# IDs, and the second tensor stores the target toke IDs.\n","second_batch = next(data_iter)\n","print(second_batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpPywcHTOz1l","executionInfo":{"status":"ok","timestamp":1754716202133,"user_tz":420,"elapsed":8,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"1686a72a-1453-47c2-b8dd-07f0988db03b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"]}]},{"cell_type":"code","source":["# Use the data loader to sample with a batch size greater than 1\n","dataloader = create_dataloader_v1(\n","    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",")\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\", inputs)\n","print(\"Targets:\\n\", targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOp9kVvfPFIH","executionInfo":{"status":"ok","timestamp":1754716337939,"user_tz":420,"elapsed":184,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"e1a58b68-8211-45ac-998b-af95a35d81a9"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Inputs:\n"," tensor([[   40,   367,  2885,  1464],\n","        [ 1807,  3619,   402,   271],\n","        [10899,  2138,   257,  7026],\n","        [15632,   438,  2016,   257],\n","        [  922,  5891,  1576,   438],\n","        [  568,   340,   373,   645],\n","        [ 1049,  5975,   284,   502],\n","        [  284,  3285,   326,    11]])\n","Targets:\n"," tensor([[  367,  2885,  1464,  1807],\n","        [ 3619,   402,   271, 10899],\n","        [ 2138,   257,  7026, 15632],\n","        [  438,  2016,   257,   922],\n","        [ 5891,  1576,   438,   568],\n","        [  340,   373,   645,  1049],\n","        [ 5975,   284,   502,   284],\n","        [ 3285,   326,    11,   287]])\n"]}]},{"cell_type":"markdown","source":["## 2.7 - Creating token embeddings\n","\n","  - The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors."],"metadata":{"id":"PgGfqzx5PoRL"}},{"cell_type":"code","source":[],"metadata":{"id":"WaMcm3ZfPj7I"},"execution_count":null,"outputs":[]}]}