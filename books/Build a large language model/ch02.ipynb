{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4Hdm/zwudz1afdz0OSDEQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["-----\n","Build a Large Language Model\n","Sebastian Raschka\n","-----\n","\n","# Working with text data\n","\n","  - During the pretraining stage, LLMs process text, one word at a time.\n","\n","## 2.1 - Understanding word embeddings\n","\n","  - Converting data into a vector format is referred to as embedding.  An embedding is a mapping from discrete objects, such as words, images, or even entire documents, to point in a continuous vector space - the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.\n","  - In addition to word embeddings, there are also embedding for sentences, paragraphs, or whole documents.\n","  - Retrieval-augmented generation combines generation (like producting text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.\n","\n","## 2.2 - Tokenizing text\n","\n","  - We will tokenize \"The Verdict,\" from https://en.wikisource.org/wiki/The_Verdict\n"],"metadata":{"id":"KYlZjvsSZr88"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhIOhqVnZrW8","executionInfo":{"status":"ok","timestamp":1754452941251,"user_tz":420,"elapsed":134,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"255f23d4-8da5-411d-8162-99ded35928df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}],"source":["# Download our text\n","import urllib.request\n","url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n","file_path = \"the-verdict.txt\"\n","urllib.request.urlretrieve(url, file_path)\n","\n","# Now load our text file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    raw_text = file.read()\n","print(\"Total number of characters:\", len(raw_text))\n","print(raw_text[:99])"]},{"cell_type":"markdown","source":["  - Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training."],"metadata":{"id":"noJXRYhfaFM2"}},{"cell_type":"code","source":["# Simple example text using re.split command with the following syntax\n","import re\n","text = \"Hello, world.  This, is a test.\"\n","result = re.split(r'(\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeWk1J7vKtcW","executionInfo":{"status":"ok","timestamp":1754452941253,"user_tz":420,"elapsed":11,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"4c5775f3-5c7f-4bd5-c553-15794bbac500"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello,', ' ', 'world.', ' ', '', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"]}]},{"cell_type":"markdown","source":["  - Modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.])\n","  - Capitalization helps LLMs distinguish between proper nouns and common nouns, so we will refrain from making all text lowercase."],"metadata":{"id":"ZOmG-ymmamEP"}},{"cell_type":"code","source":["result = re.split(f'([,.]|\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSmfgx0_ai56","executionInfo":{"status":"ok","timestamp":1754452941284,"user_tz":420,"elapsed":35,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"2e34f13b-eeb3-4891-f0f4-148e9b383b73"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"]}]},{"cell_type":"markdown","source":["  - Remove all the whitespace characters"],"metadata":{"id":"65Yg8SYbbSrC"}},{"cell_type":"code","source":["result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iM0DbembMT3","executionInfo":{"status":"ok","timestamp":1754452941290,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"cbbcf304-7110-4f1c-b23a-7a30e12fd16a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"]}]},{"cell_type":"markdown","source":["  - Need to modify the exmple further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double dashes we have seen earlier in the first 100 characters"],"metadata":{"id":"TknIjlo8bnIE"}},{"cell_type":"code","source":["text = \"Hello, world.  Is this -- a test?\"\n","result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIc6NAiUbeVS","executionInfo":{"status":"ok","timestamp":1754453128733,"user_tz":420,"elapsed":67,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"1f4b73c3-43bc-4d81-ebcd-44b455c02d28"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"]}]},{"cell_type":"markdown","source":["- Apply the basic tokenizer to the story"],"metadata":{"id":"y_Ga5zYFcL5w"}},{"cell_type":"code","source":["preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(len(preprocessed))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sPs-uHNcGbn","executionInfo":{"status":"ok","timestamp":1754453148977,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"5e972097-c8f7-45e5-e1bc-67bf3a85fa58"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["4690\n"]}]},{"cell_type":"markdown","source":["  - Let's print the first 30 tokens for a quick visual check"],"metadata":{"id":"uez6G3vii6BI"}},{"cell_type":"code","source":["print(preprocessed[:30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7DdEef-i0oh","executionInfo":{"status":"ok","timestamp":1754453150212,"user_tz":420,"elapsed":13,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"c0ff7b41-dd4d-4f68-ef18-c5aaed71634d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"]}]},{"cell_type":"markdown","source":["## 2.3 - Converting tokens into token IDs\n","\n","  - Next let's convert these tokens from a Python string to an integer representation to produce the token IDs."],"metadata":{"id":"o1HvaSd1jqRX"}},{"cell_type":"code","source":["all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","print(f\"Vocab size: {vocab_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffdVi2azjAS-","executionInfo":{"status":"ok","timestamp":1754453274885,"user_tz":420,"elapsed":5,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"da6cde65-db6c-46c9-a3c2-c2269768bce6"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 1130\n"]}]},{"cell_type":"markdown","source":["  - Create a vocabulary and print the first 51 entries."],"metadata":{"id":"xnmiz2I7kIU5"}},{"cell_type":"code","source":["vocab = {token:integer for integer, token in enumerate(all_words)}\n","for i, item in enumerate(vocab.items()):\n","  print(item)\n","  if i >= 50:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoduyISskD6V","executionInfo":{"status":"ok","timestamp":1754453367220,"user_tz":420,"elapsed":40,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"c48b2d80-600a-421b-94b3-79a987835585"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"markdown","source":["  - Our next goal is to apply this vocabulary to convert new text into token IDs.\n","  - Let's implete a tokenizer class in Python with an encode method.\n","  - Also create a decode method, so we can reverse this process."],"metadata":{"id":"RpaWpINskgyz"}},{"cell_type":"code","source":["class SimpleTokenizerV1:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()}\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"aQvyGQ5akdja","executionInfo":{"status":"ok","timestamp":1754453847996,"user_tz":420,"elapsed":43,"user":{"displayName":"culver stone","userId":"14998972164611572123"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["  - Using the SimpleTokenizerV1, we can intantiate new tokenizer objects via an existing vocabulary"],"metadata":{"id":"rZYk0-Yslsx5"}},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV1(vocab)\n","text = \"\"\"\"It's the last he painted, you know,\"\n","        Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS9zO47Tlrlt","executionInfo":{"status":"ok","timestamp":1754453849166,"user_tz":420,"elapsed":9,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"1d3ef472-716e-4ca0-c20c-25954cc7ae11"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"]}]},{"cell_type":"markdown","source":["  - Next ensure we can turn the token IDs back into text"],"metadata":{"id":"N4jwqYCGmHRB"}},{"cell_type":"code","source":["print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUUuZDGhmEL7","executionInfo":{"status":"ok","timestamp":1754453850135,"user_tz":420,"elapsed":11,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"ee90a4c0-fa11-4023-f4e5-2c7a57422f83"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"]}]},{"cell_type":"markdown","source":["  - Let's now apply our tokenizer to a new text sample."],"metadata":{"id":"k6xX9-x-mZlP"}},{"cell_type":"code","source":["text = \"Hello, do you like tea?\"\n","print(tokenizer.encode(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"Ktn-kmJVmNEv","executionInfo":{"status":"error","timestamp":1754453917085,"user_tz":420,"elapsed":31,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"92376e5b-84dd-4e6f-c32d-bd08be37b277"},"execution_count":24,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'Hello'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1763555282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1021348907.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1021348907.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Hello'"]}]},{"cell_type":"markdown","source":["  - Throws and error since \"Hello\" was not in our short story, hence it's not in our vocabulary."],"metadata":{"id":"bViZH38Jmkmx"}},{"cell_type":"code","source":[],"metadata":{"id":"7s8k4uvMmjzR"},"execution_count":null,"outputs":[]}]}