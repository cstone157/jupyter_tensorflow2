{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### -----\n",
        "#### Build a Large Language Model\n",
        "#### Sebastian Raschka\n",
        "#### -----\n",
        "\n",
        "# Working with text data\n",
        "\n",
        "  - During the pretraining stage, LLMs process text, one word at a time.\n",
        "\n",
        "## 2.1 - Understanding word embeddings\n",
        "\n",
        "  - Converting data into a vector format is referred to as embedding.  An embedding is a mapping from discrete objects, such as words, images, or even entire documents, to point in a continuous vector space - the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.\n",
        "  - In addition to word embeddings, there are also embedding for sentences, paragraphs, or whole documents.\n",
        "  - Retrieval-augmented generation combines generation (like producting text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.\n",
        "\n",
        "## 2.2 - Tokenizing text\n",
        "\n",
        "  - We will tokenize \"The Verdict,\" from https://en.wikisource.org/wiki/The_Verdict\n"
      ],
      "metadata": {
        "id": "KYlZjvsSZr88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhIOhqVnZrW8",
        "outputId": "bb8018b8-bf79-46b5-c052-2f52ed76bf68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# Download our text\n",
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Now load our text file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "print(\"Total number of characters:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training."
      ],
      "metadata": {
        "id": "noJXRYhfaFM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple example text using re.split command with the following syntax\n",
        "import re\n",
        "text = \"Hello, world.  This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeWk1J7vKtcW",
        "outputId": "a8afbcb2-6228-4e7f-c6fc-184698f83d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', '', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.])\n",
        "  - Capitalization helps LLMs distinguish between proper nouns and common nouns, so we will refrain from making all text lowercase."
      ],
      "metadata": {
        "id": "ZOmG-ymmamEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(f'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSmfgx0_ai56",
        "outputId": "02da8e8e-b052-43b7-b51f-06d52fd69f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Remove all the whitespace characters"
      ],
      "metadata": {
        "id": "65Yg8SYbbSrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iM0DbembMT3",
        "outputId": "38cc049c-f49d-4dda-9622-cdfbd644a4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Need to modify the exmple further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double dashes we have seen earlier in the first 100 characters"
      ],
      "metadata": {
        "id": "TknIjlo8bnIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world.  Is this -- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIc6NAiUbeVS",
        "outputId": "2723ef29-d3b0-4b84-ea3f-dc75b98fbe90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Apply the basic tokenizer to the story"
      ],
      "metadata": {
        "id": "y_Ga5zYFcL5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sPs-uHNcGbn",
        "outputId": "0c9950cf-c597-441e-a01d-f1b60070d6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Let's print the first 30 tokens for a quick visual check"
      ],
      "metadata": {
        "id": "uez6G3vii6BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7DdEef-i0oh",
        "outputId": "b9a25cbe-e306-4a87-de61-2d0a3194fdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 - Converting tokens into token IDs\n",
        "\n",
        "  - Next let's convert these tokens from a Python string to an integer representation to produce the token IDs."
      ],
      "metadata": {
        "id": "o1HvaSd1jqRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffdVi2azjAS-",
        "outputId": "17d99245-cf81-46e8-bc40-3a065e766f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Create a vocabulary and print the first 51 entries."
      ],
      "metadata": {
        "id": "xnmiz2I7kIU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i >= 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoduyISskD6V",
        "outputId": "bb999bd6-2a5e-4abe-cba7-8b8d1b03002f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Our next goal is to apply this vocabulary to convert new text into token IDs.\n",
        "  - Let's implete a tokenizer class in Python with an encode method.\n",
        "  - Also create a decode method, so we can reverse this process."
      ],
      "metadata": {
        "id": "RpaWpINskgyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "aQvyGQ5akdja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Using the SimpleTokenizerV1, we can intantiate new tokenizer objects via an existing vocabulary"
      ],
      "metadata": {
        "id": "rZYk0-Yslsx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS9zO47Tlrlt",
        "outputId": "e7d23051-4b02-4200-e71f-7b849a21affb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Next ensure we can turn the token IDs back into text"
      ],
      "metadata": {
        "id": "N4jwqYCGmHRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUUuZDGhmEL7",
        "outputId": "4458e948-6bad-45ab-99c3-ce77c97737cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Let's now apply our tokenizer to a new text sample."
      ],
      "metadata": {
        "id": "k6xX9-x-mZlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "\n",
        "try:\n",
        "  print(tokenizer.encode(text))\n",
        "except Exception as e:\n",
        "  print(f\"unknown token {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktn-kmJVmNEv",
        "outputId": "4477504a-e704-4348-a46b-b571161d192d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unknown token 'Hello'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Throws and error since \"Hello\" was not in our short story, hence it's not in our vocabulary.\n",
        "\n",
        "## 2.4 - Adding special context tokens\n",
        "\n",
        "  - We need to modify the tokenizer to handle unknown words.\n",
        "  - Modify the tokenizer to handle two special tokesn, <unk> and <|endoftext|>"
      ],
      "metadata": {
        "id": "bViZH38Jmkmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s8k4uvMmjzR",
        "outputId": "3a8c7523-6727-40db-9223-cb07ca459452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The new vocabulary size is 1132 (the previous vocabulary size was 1130)"
      ],
      "metadata": {
        "id": "nE8CSu7euzXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zecndQ9RuzJz",
        "outputId": "25607cea-6970-4f85-9841-09987eecdbea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Base on the ouput, we can confirm that the two new special tokens were indeed successfully incorporated into the vocab."
      ],
      "metadata": {
        "id": "Dy9C1laWvbc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items() }\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [\n",
        "        item.strip() for item in preprocessed if item.strip()\n",
        "    ]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "                    else \"<|unk|>\" for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "U1cGNVYgvcur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Compared to v1, v2 replaces unknown words with <|unk|> tokens."
      ],
      "metadata": {
        "id": "1zz_1YG0wIwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlsYev7YwH5y",
        "outputId": "0c6038dc-79f6-4f21-fd2b-40d0d4d23038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMLJ3WOdwdH9",
        "outputId": "2b439585-516b-49af-9a45-da422559a1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(tokenizer.encode(text))\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BslPebcZwidy",
        "outputId": "7b50dc19-a900-4a13-a1c0-33be821e812a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - Depending on the LLM, some researchers also consider additional special tokens such as:\n",
        "    - [BOS] (beginning of sequence) - marks the start of text\n",
        "    - [EOS] (end of sequence) - positionaed at the end of a text is especially useful when concatenating multiple unreleated text similar to <|endoftext|>.\n",
        "    - [PAD] (padding) - when training LLMs the batch sizes can vary, to ensure all texts have the same length, the shorter texts are extended or \"padded\".\n",
        "\n",
        "## 2.5 Byte pair encoding\n",
        "\n",
        "  - Since BPE (byte pair encoding) is complicated we will use an existing python library called toktoken.  The code is based on tiktoken 0.7.0, check the version you currently have installed."
      ],
      "metadata": {
        "id": "vlZeVKfPxGHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken==0.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPwjLDRww0Mu",
        "outputId": "f4fcffa0-dcf5-491a-b968-e959171632ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B95EH_eyyb7H",
        "outputId": "017dc44f-0e2b-4c7b-ab2a-c6b33ef66a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the BPE tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Demonstrated usage of tokenizer\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces \"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5wqkrhuypJg",
        "outputId": "504480e3-0c82-473a-a952-7fcd423817e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the token IDs back into text using the decode method\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwwpgvZozTvg",
        "outputId": "a734b7cc-77d5-4d8a-f704-b1c7d388ac4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The <|endoftext|> is assigned a relatively large token ID.  The total vocabulary is 50257\n",
        "  - The BPE tokenizer encodes and decodes unknown words, such as someunknownPlace corrrectly.\n",
        "  - BPE breaks down words that aren't in its predifined vocab into smaller subword units or even individual characters.\n",
        "  - The BPE builds its vocab by iteratively merging frequent characters into sub-words and frequent subwords into words.\n",
        "\n",
        "## 2.6 - Data sampling with a sliding window\n",
        "\n",
        "  - The next step in creating the embeddings for LLM is to generate a input-target pairs required for training an LLM."
      ],
      "metadata": {
        "id": "TrAsZFkUyfuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "  text = file.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3ezgmEcyTIb",
        "outputId": "decd81ee-afd7-42d2-9b6c-a314f5456292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Remove the first 50 tokens from the dataset for demonstration purposes, as it\n",
        "# results in a slightly more interesting text passage\n",
        "enc_sample = enc_text[50:]\n",
        "\n",
        "#   Create two variable x and y, where x contains the input tokens and y contains\n",
        "# the targets\n",
        "context_size = 4\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-RTVQA9zqzW",
        "outputId": "23cfc033-cd2e-43aa-b958-13a77c596a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   By processing the inputs along with targets, we can create the next-word\n",
        "# prediction tasks\n",
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCgLSRxd0Iyf",
        "outputId": "484e9b84-43d6-4aa1-81c0-44003824ac73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   Everything left of the arrow refers to the input, and the token id on the right\n",
        "# side represents the target token ID that the LLM is supposed to predict.\n",
        "\n",
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxMKpVyi49dI",
        "outputId": "06ba59f7-19c0-4332-9ff0-d52597f42589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - The last task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors."
      ],
      "metadata": {
        "id": "7e0TElAR51Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "Wy2mqw4Q5spN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - GPTDatasetV1 is based on PyTorch Dataset class and defines how individual rows are fetched."
      ],
      "metadata": {
        "id": "WfAJCJ9ENB4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader\n",
        "\n",
        "# Test dataloader with batch size of 1 for an LLM\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNshQQD76NNk",
        "outputId": "26c30518-1931-4df5-f144-c0becbcb6f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   The first_batch contains two tensors: the first tensor stores the input token\n",
        "# IDs, and the second tensor stores the target toke IDs.\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpPywcHTOz1l",
        "outputId": "bdda16c0-796b-446e-85cc-df96b0f656e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the data loader to sample with a batch size greater than 1\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"Targets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOp9kVvfPFIH",
        "outputId": "607f9d67-b5ed-4cb7-fcff-b429740b4f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 - Creating token embeddings\n",
        "\n",
        "  - The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors.\n",
        "  - We must initialize these embedding weights with random values.\n",
        "  - Initialization serves as the starting point for LLM's learning process."
      ],
      "metadata": {
        "id": "PgGfqzx5PoRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Let's see how the token ID to embedding vector conversion works with a\n",
        "# hands-on example\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "#   We want to create embeddings of size 3 (we have a small vocabular of only\n",
        "# 6 words)\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "#   We can instantiate an embedding layer in PyTorch, setting the random seed\n",
        "# 123 for reproducibility purposes:\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaMcm3ZfPj7I",
        "outputId": "a73a9596-de40-47ac-91c5-d81b6e2daa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions"
      ],
      "metadata": {
        "id": "erKDXpciFFTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfuxg7tMFCe5",
        "outputId": "64b89583-b696-4c5e-b143-90aaab93ff36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the fourth row.\n",
        "    - The embeddinbg layer approach descirbed here is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication."
      ],
      "metadata": {
        "id": "xiLAJc8DGKTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's apply to all four inputs\n",
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sblVT_XdFdwr",
        "outputId": "d469f80c-b33d-44fd-c2b8-55d022a2a9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 - Encoding word positions\n",
        "\n",
        "  - A shortcoming of LLMs is that their self-attention mechanism doesn't have a notion of position or order for the tokens within a sequence.\n",
        "  - Since self-attention mechanism of LLM itself is also position-agnostic, it is helpful to inject additional positional information into the LLM.\n",
        "  - Two categories to achieve this:\n",
        "    - Absolute positional embeddings are directly associated with specific positions in a sequence.  For each, position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location.\n",
        "    - Relative positional embeddings is on the relative position or distnace between tokens.  "
      ],
      "metadata": {
        "id": "F5UcqmJ_G3qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Let's consider more realistic and useful embedding sizes and encode the\n",
        "# input tokens into a 256-dimentsional vector.\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "#   Using the previous token_embedding layer, if we sample data from the data\n",
        "# loader we embed each token in each batch into a 256-dimensional vector.\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wYx8jslGyNt",
        "outputId": "36565646-22d3-40e6-b47e-3a0e70881318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   The token ID tensor is 8 x 4 dimensional, meaning that the data batch\n",
        "# consists of eight text samples with four tokens each.\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7nLuKhgNOXk",
        "outputId": "16b41dcf-e305-46b3-b6d5-6129378578e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   The 8 x 4 x 256 dimensional tensor ouput shows that each token ID is now\n",
        "# embedded as a 256-dimensional vector.\n",
        "#   For a GPT model's absolute embedding approach, we just need to create\n",
        "# another embedding layer that has the same embedding dimesion as the\n",
        "# token_embedding_layer\n",
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSPGhyHjb45S",
        "outputId": "bb5476a2-5f1c-485b-8b8c-9f7f5d5a63f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   The positional embedding tensor consits of four 256-dimensional vectors.\n",
        "# We can now add these directly to the token embeddings, where PyTorch will\n",
        "# add the 4 x 256-dimensional pos_embeddings tensor to each 4 x 256-dimensional token embedding tensor\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nBaxqVKdOYV",
        "outputId": "120af44e-7262-4e8c-9917-beef8d5059e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqsYfS2Wd073"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}