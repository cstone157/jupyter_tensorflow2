{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTjPLOPuYET9R8xC41giVx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["-----\n","Build a Large Language Model\n","Sebastian Raschka\n","-----\n","\n","# Working with text data\n","\n","  - During the pretraining stage, LLMs process text, one word at a time.\n","\n","## 2.1 - Understanding word embeddings\n","\n","  - Converting data into a vector format is referred to as embedding.  An embedding is a mapping from discrete objects, such as words, images, or even entire documents, to point in a continuous vector space - the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.\n","  - In addition to word embeddings, there are also embedding for sentences, paragraphs, or whole documents.\n","  - Retrieval-augmented generation combines generation (like producting text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.\n","\n","## 2.2 - Tokenizing text\n","\n","  - We will tokenize \"The Verdict,\" from https://en.wikisource.org/wiki/The_Verdict\n"],"metadata":{"id":"KYlZjvsSZr88"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhIOhqVnZrW8","executionInfo":{"status":"ok","timestamp":1754262067317,"user_tz":420,"elapsed":192,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"ee51c972-680d-4971-a368-50b956124ccc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}],"source":["# Download our text\n","import urllib.request\n","url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n","file_path = \"the-verdict.txt\"\n","urllib.request.urlretrieve(url, file_path)\n","\n","# Now load our text file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    raw_text = file.read()\n","print(\"Total number of characters:\", len(raw_text))\n","print(raw_text[:99])"]},{"cell_type":"markdown","source":["  - Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training."],"metadata":{"id":"noJXRYhfaFM2"}},{"cell_type":"code","source":["# Simple example text using re.split command with the following syntax\n","import re\n","text = \"Hello, world.  This, is a test.\"\n","result = re.split(r'(\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeWk1J7vKtcW","executionInfo":{"status":"ok","timestamp":1754282995507,"user_tz":420,"elapsed":14,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"b2813a4c-adf3-4d08-ff60-273b15907d50"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello,', ' ', 'world.', ' ', '', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"]}]},{"cell_type":"markdown","source":["  - Modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.])\n","  - Capitalization helps LLMs distinguish between proper nouns and common nouns, so we will refrain from making all text lowercase."],"metadata":{"id":"ZOmG-ymmamEP"}},{"cell_type":"code","source":["result = re.split(f'([,.]|\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSmfgx0_ai56","executionInfo":{"status":"ok","timestamp":1754283180265,"user_tz":420,"elapsed":7,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"44378428-0595-4e0c-8c7c-97d1fa31a469"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"]}]},{"cell_type":"markdown","source":["  - Remove all the whitespace characters"],"metadata":{"id":"65Yg8SYbbSrC"}},{"cell_type":"code","source":["result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iM0DbembMT3","executionInfo":{"status":"ok","timestamp":1754283238933,"user_tz":420,"elapsed":24,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"cc2af4aa-d8da-4766-f9ba-494ae03105db"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"]}]},{"cell_type":"markdown","source":["  - Need to modify the exmple further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double dashes we have seen earlier in the first 100 characters"],"metadata":{"id":"TknIjlo8bnIE"}},{"cell_type":"code","source":["text = \"Hello, world.  Is this -- a test?\"\n","result = re.split(f'([,.:;?_!\"()\\']|--|\\s\")', text)\n","result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIc6NAiUbeVS","executionInfo":{"status":"ok","timestamp":1754283403230,"user_tz":420,"elapsed":69,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"5c5a0a44-3a76-400c-ca98-c8cfd3625093"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', ' world', '.', '  Is this ', '--', ' a test', '?']\n"]}]},{"cell_type":"markdown","source":["- Apply the basic tokenizer to the story"],"metadata":{"id":"y_Ga5zYFcL5w"}},{"cell_type":"code","source":[],"metadata":{"id":"3sPs-uHNcGbn"},"execution_count":null,"outputs":[]}]}