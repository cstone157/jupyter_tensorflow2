{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXx3whIZcNmzoKoMBLnNUT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["-----\n","Build a Large Language Model\n","Sebastian Raschka\n","-----\n","\n","# Working with text data\n","\n","  - During the pretraining stage, LLMs process text, one word at a time.\n","\n","## 2.1 - Understanding word embeddings\n","\n","  - Converting data into a vector format is referred to as embedding.  An embedding is a mapping from discrete objects, such as words, images, or even entire documents, to point in a continuous vector space - the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.\n","  - In addition to word embeddings, there are also embedding for sentences, paragraphs, or whole documents.\n","  - Retrieval-augmented generation combines generation (like producting text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text.\n","\n","## 2.2 - Tokenizing text\n","\n","  - We will tokenize \"The Verdict,\" from https://en.wikisource.org/wiki/The_Verdict\n"],"metadata":{"id":"KYlZjvsSZr88"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhIOhqVnZrW8","executionInfo":{"status":"ok","timestamp":1754541006965,"user_tz":420,"elapsed":196,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"bc2f9fee-3a00-49c2-ba6a-48dfdc93851e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of characters: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}],"source":["# Download our text\n","import urllib.request\n","url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n","file_path = \"the-verdict.txt\"\n","urllib.request.urlretrieve(url, file_path)\n","\n","# Now load our text file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    raw_text = file.read()\n","print(\"Total number of characters:\", len(raw_text))\n","print(raw_text[:99])"]},{"cell_type":"markdown","source":["  - Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training."],"metadata":{"id":"noJXRYhfaFM2"}},{"cell_type":"code","source":["# Simple example text using re.split command with the following syntax\n","import re\n","text = \"Hello, world.  This, is a test.\"\n","result = re.split(r'(\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oeWk1J7vKtcW","executionInfo":{"status":"ok","timestamp":1754541006966,"user_tz":420,"elapsed":54,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"7065bcd4-c91c-4042-e1e1-5f54b1ba3c32"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello,', ' ', 'world.', ' ', '', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"]}]},{"cell_type":"markdown","source":["  - Modify the regular expression splits on whitespaces (\\s), commas, and periods ([,.])\n","  - Capitalization helps LLMs distinguish between proper nouns and common nouns, so we will refrain from making all text lowercase."],"metadata":{"id":"ZOmG-ymmamEP"}},{"cell_type":"code","source":["result = re.split(f'([,.]|\\s)', text)\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JSmfgx0_ai56","executionInfo":{"status":"ok","timestamp":1754541006967,"user_tz":420,"elapsed":49,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"5f27e653-b15c-4fe3-b011-36a3496e354b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', '', ' ', 'world', '.', '', ' ', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"]}]},{"cell_type":"markdown","source":["  - Remove all the whitespace characters"],"metadata":{"id":"65Yg8SYbbSrC"}},{"cell_type":"code","source":["result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iM0DbembMT3","executionInfo":{"status":"ok","timestamp":1754541006968,"user_tz":420,"elapsed":8,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"f7dbe95e-aee0-4cb3-9fe0-f8cc20cec224"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"]}]},{"cell_type":"markdown","source":["  - Need to modify the exmple further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double dashes we have seen earlier in the first 100 characters"],"metadata":{"id":"TknIjlo8bnIE"}},{"cell_type":"code","source":["text = \"Hello, world.  Is this -- a test?\"\n","result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","result = [item for item in result if item.strip()]\n","print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIc6NAiUbeVS","executionInfo":{"status":"ok","timestamp":1754541007056,"user_tz":420,"elapsed":87,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"8a1fa0ff-48d0-4d92-ad00-334a214fded6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"]}]},{"cell_type":"markdown","source":["- Apply the basic tokenizer to the story"],"metadata":{"id":"y_Ga5zYFcL5w"}},{"cell_type":"code","source":["preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item.strip() for item in preprocessed if item.strip()]\n","print(len(preprocessed))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3sPs-uHNcGbn","executionInfo":{"status":"ok","timestamp":1754541007152,"user_tz":420,"elapsed":87,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"e7d39217-0741-4d7f-d2fd-15f5dd55e671"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["4690\n"]}]},{"cell_type":"markdown","source":["  - Let's print the first 30 tokens for a quick visual check"],"metadata":{"id":"uez6G3vii6BI"}},{"cell_type":"code","source":["print(preprocessed[:30])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7DdEef-i0oh","executionInfo":{"status":"ok","timestamp":1754541007222,"user_tz":420,"elapsed":69,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"c542b78e-ec26-4c56-87dc-8129504e121f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"]}]},{"cell_type":"markdown","source":["## 2.3 - Converting tokens into token IDs\n","\n","  - Next let's convert these tokens from a Python string to an integer representation to produce the token IDs."],"metadata":{"id":"o1HvaSd1jqRX"}},{"cell_type":"code","source":["all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","print(f\"Vocab size: {vocab_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffdVi2azjAS-","executionInfo":{"status":"ok","timestamp":1754541007225,"user_tz":420,"elapsed":66,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"94a7980d-53e1-4765-d998-8896193c95c0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 1130\n"]}]},{"cell_type":"markdown","source":["  - Create a vocabulary and print the first 51 entries."],"metadata":{"id":"xnmiz2I7kIU5"}},{"cell_type":"code","source":["vocab = {token:integer for integer, token in enumerate(all_words)}\n","for i, item in enumerate(vocab.items()):\n","  print(item)\n","  if i >= 50:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NoduyISskD6V","executionInfo":{"status":"ok","timestamp":1754541007225,"user_tz":420,"elapsed":61,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"42a81603-238b-4846-e54c-eccd876b6830"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["('!', 0)\n","('\"', 1)\n","(\"'\", 2)\n","('(', 3)\n","(')', 4)\n","(',', 5)\n","('--', 6)\n","('.', 7)\n","(':', 8)\n","(';', 9)\n","('?', 10)\n","('A', 11)\n","('Ah', 12)\n","('Among', 13)\n","('And', 14)\n","('Are', 15)\n","('Arrt', 16)\n","('As', 17)\n","('At', 18)\n","('Be', 19)\n","('Begin', 20)\n","('Burlington', 21)\n","('But', 22)\n","('By', 23)\n","('Carlo', 24)\n","('Chicago', 25)\n","('Claude', 26)\n","('Come', 27)\n","('Croft', 28)\n","('Destroyed', 29)\n","('Devonshire', 30)\n","('Don', 31)\n","('Dubarry', 32)\n","('Emperors', 33)\n","('Florence', 34)\n","('For', 35)\n","('Gallery', 36)\n","('Gideon', 37)\n","('Gisburn', 38)\n","('Gisburns', 39)\n","('Grafton', 40)\n","('Greek', 41)\n","('Grindle', 42)\n","('Grindles', 43)\n","('HAD', 44)\n","('Had', 45)\n","('Hang', 46)\n","('Has', 47)\n","('He', 48)\n","('Her', 49)\n","('Hermia', 50)\n"]}]},{"cell_type":"markdown","source":["  - Our next goal is to apply this vocabulary to convert new text into token IDs.\n","  - Let's implete a tokenizer class in Python with an encode method.\n","  - Also create a decode method, so we can reverse this process."],"metadata":{"id":"RpaWpINskgyz"}},{"cell_type":"code","source":["class SimpleTokenizerV1:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = {i:s for s, i in vocab.items()}\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"aQvyGQ5akdja","executionInfo":{"status":"ok","timestamp":1754541007226,"user_tz":420,"elapsed":56,"user":{"displayName":"culver stone","userId":"14998972164611572123"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["  - Using the SimpleTokenizerV1, we can intantiate new tokenizer objects via an existing vocabulary"],"metadata":{"id":"rZYk0-Yslsx5"}},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV1(vocab)\n","text = \"\"\"\"It's the last he painted, you know,\"\n","        Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS9zO47Tlrlt","executionInfo":{"status":"ok","timestamp":1754541007247,"user_tz":420,"elapsed":26,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"acd0aaeb-a449-43dc-b83f-c2f09e8ba1ef"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"]}]},{"cell_type":"markdown","source":["  - Next ensure we can turn the token IDs back into text"],"metadata":{"id":"N4jwqYCGmHRB"}},{"cell_type":"code","source":["print(tokenizer.decode(ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUUuZDGhmEL7","executionInfo":{"status":"ok","timestamp":1754541007248,"user_tz":420,"elapsed":16,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"23a0c398-9498-4086-8d0d-5204d76d6341"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"]}]},{"cell_type":"markdown","source":["  - Let's now apply our tokenizer to a new text sample."],"metadata":{"id":"k6xX9-x-mZlP"}},{"cell_type":"code","source":["text = \"Hello, do you like tea?\"\n","\n","try:\n","  print(tokenizer.encode(text))\n","except Exception as e:\n","  print(f\"unknown token {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ktn-kmJVmNEv","executionInfo":{"status":"ok","timestamp":1754541007248,"user_tz":420,"elapsed":11,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"ddf4b7ce-5045-4143-b634-f9a487f5e737"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["unknown token 'Hello'\n"]}]},{"cell_type":"markdown","source":["  - Throws and error since \"Hello\" was not in our short story, hence it's not in our vocabulary.\n","\n","## 2.4 - Adding special context tokens\n","\n","  - We need to modify the tokenizer to handle unknown words.\n","  - Modify the tokenizer to handle two special tokesn, <unk> and <|endoftext|>"],"metadata":{"id":"bViZH38Jmkmx"}},{"cell_type":"code","source":["all_tokens = sorted(list(set(preprocessed)))\n","all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n","vocab = {token:integer for integer, token in enumerate(all_tokens)}\n","\n","print(len(vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7s8k4uvMmjzR","executionInfo":{"status":"ok","timestamp":1754541007249,"user_tz":420,"elapsed":7,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"75f3a815-6911-4731-9b74-612982d17fff"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["1132\n"]}]},{"cell_type":"markdown","source":["  - The new vocabulary size is 1132 (the previous vocabulary size was 1130)"],"metadata":{"id":"nE8CSu7euzXu"}},{"cell_type":"code","source":["for i, item in enumerate(list(vocab.items())[-5:]):\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zecndQ9RuzJz","executionInfo":{"status":"ok","timestamp":1754541007292,"user_tz":420,"elapsed":43,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"3d83e370-c6d3-4ab2-dd2e-8ad445beb655"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["('younger', 1127)\n","('your', 1128)\n","('yourself', 1129)\n","('<|endoftext|>', 1130)\n","('<|unk|>', 1131)\n"]}]},{"cell_type":"markdown","source":["  - Base on the ouput, we can confirm that the two new special tokens were indeed successfully incorporated into the vocab."],"metadata":{"id":"Dy9C1laWvbc0"}},{"cell_type":"code","source":["class SimpleTokenizerV2:\n","  def __init__(self, vocab):\n","    self.str_to_int = vocab\n","    self.int_to_str = { i:s for s,i in vocab.items() }\n","\n","  def encode(self, text):\n","    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n","    preprocessed = [\n","        item.strip() for item in preprocessed if item.strip()\n","    ]\n","    preprocessed = [item if item in self.str_to_int\n","                    else \"<|unk|>\" for item in preprocessed]\n","\n","    ids = [self.str_to_int[s] for s in preprocessed]\n","    return ids\n","\n","  def decode(self, ids):\n","    text = \" \".join([self.int_to_str[i] for i in ids])\n","\n","    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","    return text"],"metadata":{"id":"U1cGNVYgvcur","executionInfo":{"status":"ok","timestamp":1754541007295,"user_tz":420,"elapsed":1,"user":{"displayName":"culver stone","userId":"14998972164611572123"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["  - Compared to v1, v2 replaces unknown words with <|unk|> tokens."],"metadata":{"id":"1zz_1YG0wIwi"}},{"cell_type":"code","source":["text1 = \"Hello, do you like tea?\"\n","text2 = \"In the sunlit terraces of the palace.\"\n","text = \" <|endoftext|> \".join((text1, text2))\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mlsYev7YwH5y","executionInfo":{"status":"ok","timestamp":1754541007301,"user_tz":420,"elapsed":4,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"662b57c3-db56-452c-fb9d-28687912a1ab"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"]}]},{"cell_type":"code","source":["tokenizer = SimpleTokenizerV2(vocab)\n","print(tokenizer.encode(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMLJ3WOdwdH9","executionInfo":{"status":"ok","timestamp":1754541007328,"user_tz":420,"elapsed":25,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"3980063d-2edc-4198-d927-9b3503a992c1"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"]}]},{"cell_type":"code","source":["strings = tokenizer.decode(tokenizer.encode(text))\n","print(strings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BslPebcZwidy","executionInfo":{"status":"ok","timestamp":1754541007328,"user_tz":420,"elapsed":8,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"2f51489c-54ec-40b8-fbe4-405e022ebbcf"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"]}]},{"cell_type":"markdown","source":["  - Depending on the LLM, some researchers also consider additional special tokens such as:\n","    - [BOS] (beginning of sequence) - marks the start of text\n","    - [EOS] (end of sequence) - positionaed at the end of a text is especially useful when concatenating multiple unreleated text similar to <|endoftext|>.\n","    - [PAD] (padding) - when training LLMs the batch sizes can vary, to ensure all texts have the same length, the shorter texts are extended or \"padded\".\n","\n","## 2.5 Byte pair encoding\n","\n","  - Since BPE (byte pair encoding) is complicated we will use an existing python library called toktoken.  The code is based on tiktoken 0.7.0, check the version you currently have installed."],"metadata":{"id":"vlZeVKfPxGHA"}},{"cell_type":"code","source":["pip install tiktoken==0.7.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPwjLDRww0Mu","executionInfo":{"status":"ok","timestamp":1754541014094,"user_tz":420,"elapsed":6769,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"fbfd6c79-eafb-44aa-f04f-f1e20efc00e1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tiktoken==0.7.0 in /usr/local/lib/python3.11/dist-packages (0.7.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken==0.7.0) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken==0.7.0) (2025.7.14)\n"]}]},{"cell_type":"code","source":["from importlib.metadata import version\n","import tiktoken\n","print(\"tiktoken version:\", version(\"tiktoken\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B95EH_eyyb7H","executionInfo":{"status":"ok","timestamp":1754541014100,"user_tz":420,"elapsed":11,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"c1b2452e-27b2-463b-c6f2-737a810285d3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tiktoken version: 0.7.0\n"]}]},{"cell_type":"code","source":["# Instantiate the BPE tokenizer\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","# Demonstrated usage of tokenizer\n","text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","    \"of someunknownPlace.\"\n",")\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","print(integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5wqkrhuypJg","executionInfo":{"status":"ok","timestamp":1754541146480,"user_tz":420,"elapsed":1233,"user":{"displayName":"culver stone","userId":"14998972164611572123"}},"outputId":"a6db84e8-62d7-43d0-93ae-cd01ea24458e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hwwpgvZozTvg"},"execution_count":null,"outputs":[]}]}