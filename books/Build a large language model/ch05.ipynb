{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TPQpy_PLJiw",
        "outputId": "83e3de65-959f-4c3a-cc82-0618cb10292f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "print(\"torch version:\", version(\"torch\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzQx4mxrLTNY",
        "outputId": "718b5ac0-bf3d-424a-8c3d-2e000555102a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D2SgL7aNMLg7"
      },
      "outputs": [],
      "source": [
        "## Import code from previous chapters\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(\n",
        "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "        (x + 0.44715 * torch.pow(x, 3))\n",
        "    ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out,\n",
        "               context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \\\n",
        "        \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "                   diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(\n",
        "        b, num_tokens, self.num_heads, self.head_dim\n",
        "    )\n",
        "\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim = -1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "    context_vec = context_vec.contiguous().view(\n",
        "        b, num_tokens, self.d_out\n",
        "    )\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec\n",
        "\n",
        "#   The transformer block component of GPT\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in = cfg[\"emb_dim\"],\n",
        "        d_out = cfg[\"emb_dim\"],\n",
        "        context_length = cfg[\"context_length\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        dropout = cfg[\"drop_rate\"],\n",
        "        qkv_bias = cfg[\"qkv_bias\"])\n",
        "\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "    return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "\n",
        "    pos_embeds = self.pos_emb(\n",
        "      torch.arange(seq_len, device=in_idx.device)\n",
        "    )\n",
        "\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhZKiRRFLWzV"
      },
      "source": [
        "# Pretraining on unlabeled data\n",
        "\n",
        "## 5.1 - Evaluating generative text models\n",
        "\n",
        "### 5.1.1 - Using GPT to generate text\n",
        "\n",
        "  - Let's set up the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZsJqYy7LYF7",
        "outputId": "a4ec457f-15ff-4977-e3f1-c88846d59119"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256,  # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_ow4GowNXJ8",
        "outputId": "6552a03d-b02f-4650-9088-09cff5509b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text: \n",
            " Every effort moves you rentingetic wasnم refres RexAngel infieldcigans\n"
          ]
        }
      ],
      "source": [
        "#   Utility functions for text to token ID conversion\n",
        "import tiktoken\n",
        "\n",
        "def generate_text_simple(model, idx,\n",
        "        max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    logits = logits[:, -1, :]\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "  return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  flat = token_ids.squeeze(0)\n",
        "  return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens = 10,\n",
        "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hXZDvTCPvm9"
      },
      "source": [
        "  - The model isn't yet producing coherent text.\n",
        "  - Next, we will calculate a loss metric for the generated outputs.\n",
        "\n",
        "### 5.1.2 - calculating the text generation loss\n",
        "\n",
        "  - Techniques for numerically assessing text quality generated during training by calculating a text generation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdCB3aGHPrjn",
        "outputId": "f3fa789b-3d98-4309-de9b-0725ef2c6404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ],
      "source": [
        "#   Consider two input examples, which have already been mapped\n",
        "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])  # \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345],     # [\" effort moves you\",\n",
        "                        [1107, 588,  11311]])  # \"really like chocolate\"]\n",
        "\n",
        "#   Now feed the inputs into model to calculate logits vectors for thewo input\n",
        "# examples, each of three tokens.\n",
        "with torch.no_grad():\n",
        "  logits = model(inputs)\n",
        "probas = torch.softmax(logits, dim = -1)\n",
        "print(probas.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhT34tQUO4iQ",
        "outputId": "943c2f10-ac84-4be7-eaea-69530cea2c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[16657],\n",
            "         [  339],\n",
            "         [42826]],\n",
            "\n",
            "        [[49906],\n",
            "         [29669],\n",
            "         [41751]]])\n"
          ]
        }
      ],
      "source": [
        "#   The first number, 2, corresponds to the two examples (rows)\n",
        "#\n",
        "#   The second number, 3, corresponds to the number of tokens in each input (row)\n",
        "#\n",
        "#   The last number corresponds to the embedding dimensionality, which is determined\n",
        "# by the vocabulary size.\n",
        "\n",
        "#   Following the conversion from logits to probabilities via the softmax function,\n",
        "# the generate_text_simple function then converts the resulting probability scores\n",
        "# back into text\n",
        "token_ids = torch.argmax(probas, dim = -1, keepdim = True)\n",
        "print(\"Token IDs:\\n\", token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY_L8W4BPOar",
        "outputId": "1b3b57de-4f1a-488a-e2b3-ddc35f790197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 2:  Armed heNetflix\n"
          ]
        }
      ],
      "source": [
        "#   Finally convert the token IDs back into text:\n",
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 2: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A721cjYAXHFV"
      },
      "source": [
        "  - Now we need to evaluate the performance of the model's generated text numerically via a loss.  \n",
        "  - This is useful for measuring the quality of the generated text, but also as a building block for implementing training function.\n",
        "  - Part of text evaluation process that we implement, is to measure \"how far\" the generated tokens are from correct predictions (targets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZbeyZhGXExr",
        "outputId": "f68660ae-c627-4a4e-f72e-1b599110d406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([7.2671e-05, 3.1046e-05, 1.1696e-05])\n",
            "Text 2: tensor([1.0426e-05, 5.4604e-05, 4.7716e-06])\n"
          ]
        }
      ],
      "source": [
        "#   For each of the two input texts, we can print the initial softmax\n",
        "# probability scores corresponding to the target torkens using the following code:\n",
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 2:\", target_probas_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS5Al7ERY5Xt"
      },
      "source": [
        "  - The goal of training an LLM is to maximize the likelihood of the correct token, which involves increasing its probability relative to other tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC8UGhweY1IE",
        "outputId": "2c00181a-f96f-4766-ecd3-f0eb0133113e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ -9.5296, -10.3800, -11.3563, -11.4712,  -9.8154, -12.2528])\n"
          ]
        }
      ],
      "source": [
        "#   Next, we calculate the loss for the probability scores of the two example batches.\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQndsbkxZXDD",
        "outputId": "faf54c3d-7e50-4deb-b605-a8e8bc005e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.8009)\n"
          ]
        }
      ],
      "source": [
        "#   Next we combine these log probabilities into a single score\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVDDBgiraM3p",
        "outputId": "2ad26d86-2c8e-433b-e8dc-93a63ba35680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.8009)\n"
          ]
        }
      ],
      "source": [
        "#   The goal is to get the average log probability as close to 0 as possible by\n",
        "# updating the model's weights as part of the training process.\n",
        "#\n",
        "#   The goal is to bring the negative average log probability down to 0.\n",
        "#\n",
        "#   The term for turning the negative falu is known as cross entropy loss.\n",
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAAYP9PdastS",
        "outputId": "270943fb-2cfc-47e0-b2ab-8fe9fdcea717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])\n",
            "Targets shape: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "#   Review the shape of the logits and target tensors:\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Targets shape:\", targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvE34lfNbJYY",
        "outputId": "23d92e01-727d-4a32-b6ea-4bdd67a45a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened logits:  torch.Size([6, 50257])\n",
            "Flattened targets:  torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "#   For the cross_entropy loss function in PyTorch, we want to flatten these\n",
        "# tensors by combining them over the batch dimension:\n",
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "print(\"Flattened logits: \", logits_flat.shape)\n",
        "print(\"Flattened targets: \", targets_flat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFYf_DiGbheC",
        "outputId": "f5fcc93f-1a57-4c68-8b57-87c5f0adfc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.8009)\n"
          ]
        }
      ],
      "source": [
        "#   Previously, we applied the softmax function, selected the probaility scores\n",
        "# corresponding to the target IDs, and computed the negative average log\n",
        "# probabilities.  PyTorch's cross_entropy function will take care of all the\n",
        "# steps for us:\n",
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-D2DqoqcFMM"
      },
      "source": [
        "### 5.1.3 - Calculating the training and validation set losses\n",
        "\n",
        "  - We must first prepare the training and validation datasets that we will use to train the LLM.\n",
        "  - To compute the loss on the training and validation datasets, we use a very small text dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_Yhb052KcBAK"
      },
      "outputs": [],
      "source": [
        "#   The following code loads \"The Verdict\"\n",
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "  text_data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMVFNVCIfozQ",
        "outputId": "243851d9-5dfa-4204-d445-937e2ba6c91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ],
      "source": [
        "#   Check the number of charaters and tokens in data set\n",
        "total_caracters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(\"Characters:\", total_caracters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W9Hq-fHafz17"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Twwmd51qg6eX"
      },
      "outputs": [],
      "source": [
        "#   Next divide the dataset into a training and validation set and use the data\n",
        "# loaders from chapter 2\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "#   Usie the train_data and val_data subsets, create respective data loaders\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "  train_data,\n",
        "  batch_size=2,\n",
        "  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "  stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "  shuffle=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        "  val_data,\n",
        "  batch_size=2,\n",
        "  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "  stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "  shuffle=True,\n",
        "  drop_last=True,\n",
        "  num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBa2KfqlhitW",
        "outputId": "86aa9ff5-caea-4533-ffc1-198998b73155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ],
      "source": [
        "#   We used a relatively small batch size to reduce the computational resource\n",
        "# demand because we were working with a very small dataset.\n",
        "#  Iterate through the data loaders to ensure that they were create correctly:\n",
        "\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "  print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "  print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6yYNjW9BiFy4"
      },
      "outputs": [],
      "source": [
        "#   Implement a utility function to calculate the cross entropy loss of a given\n",
        "# batch returned via the training and validation loader:\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  loss = torch.nn.functional.cross_entropy(\n",
        "      logits.flatten(0, 1), target_batch.flatten()\n",
        "  )\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lAut4NV6jUh4"
      },
      "outputs": [],
      "source": [
        "#   Function to compute the training and validation loss\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(\n",
        "          input_batch, target_batch, model, device\n",
        "      )\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hr1rapGkEMy",
        "outputId": "9b3569ef-71e0-41e5-e919-97b1f8ef4e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:  10.987016677856445\n",
            "Validation loss:  10.980591773986816\n"
          ]
        }
      ],
      "source": [
        "#   Let's see calc_loss_loader function in action\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss: \", train_loss)\n",
        "print(\"Validation loss: \", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gopjF2k7yS"
      },
      "source": [
        "  - The loss values are relatively high because the model hasn't been trained.\n",
        "  \n",
        "## 5.2 - Training an LLM\n",
        "\n",
        "  - Time to implement the code for pretraining the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ABBPJcp1k5P9"
      },
      "outputs": [],
      "source": [
        "#   Themain function for pretraining LLMs\n",
        "def train_model_simple(model, train_loader, val_loader,\n",
        "                       optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(\n",
        "          input_batch, target_batch, model, device\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(\n",
        "          model, train_loader, val_loader, device, eval_iter\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d})\"\n",
        "              f\"Train loss {train_loss:.3f}, \"\n",
        "              f\"Val loss {val_loss:.3f}\"\n",
        "        )\n",
        "\n",
        "    generate_and_print_sample(\n",
        "      model, tokenizer, device, start_context\n",
        "    )\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "#   The evaluate_model function calculates the loss over the training and\n",
        "# validation set wwhile ensuring the model is in evaluation mode with gradient\n",
        "# tracking and dropout disabled when calculating the loss over the training\n",
        "# and validation sets:\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(\n",
        "      train_loader, model, device, num_batches=eval_iter\n",
        "    )\n",
        "    val_loss = calc_loss_loader(\n",
        "      val_loader, model, device, num_batches=eval_iter\n",
        "    )\n",
        "  model.train()\n",
        "  return train_loss, val_loss\n",
        "\n",
        "#   Function used to track whether the model impproves during training.\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(\n",
        "      model=model, idx=encoded,\n",
        "      max_new_tokens=50, context_size=context_size\n",
        "    )\n",
        "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\", \" \"))\n",
        "  model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc2K0N3czsbq",
        "outputId": "ad6fcd6b-7872-40cc-c16b-208dd7c483fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000)Train loss 9.764, Val loss 9.903\n",
            "Ep 1 (Step 000005)Train loss 8.077, Val loss 8.339\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010)Train loss 6.760, Val loss 7.044\n",
            "Ep 2 (Step 000015)Train loss 6.115, Val loss 6.608\n",
            "Every effort moves you, the,,,,,,,,,,.                                     \n",
            "Ep 3 (Step 000020)Train loss 5.769, Val loss 6.525\n",
            "Ep 3 (Step 000025)Train loss 5.398, Val loss 6.403\n",
            "Every effort moves you.                                                 \n",
            "Ep 4 (Step 000030)Train loss 4.753, Val loss 6.312\n",
            "Ep 4 (Step 000035)Train loss 4.126, Val loss 6.228\n"
          ]
        }
      ],
      "source": [
        "#   Let's see this all in action by training a GPTModel instance for 10 epochs\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "  model.parameters(),\n",
        "  lr = 0.0004, weight_decay = 0.1\n",
        ")\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_m87EZK0h12"
      },
      "outputs": [],
      "source": [
        "# Simple plot that shows the training and validation set losses side by side:\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "  ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "  ax1.plot(\n",
        "      epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
        "  )\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "  ax2.set_xlabel(\"Tokens seen\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Lwf_on9BDn"
      },
      "source": [
        "- Both the training and validation losses start to improve for the first epoch.  The losses start to diverge past the second epoch.  This divergence and the fact that the validation loss is much larger than the training loss indicate that the model is overfitting to the training data.\n",
        "- We can confirm that the model memorizes the training data verbatim by searching for the generated text snippets\n",
        "- The memorization is expected since we are working with a very, small training dataset and training the mode for multiple epochs.\n",
        "\n",
        "## 5.3 - Decoding strategies to control randomness\n",
        "\n",
        "- Let's look at text generation strategies.\n",
        "- We will begin by transferring the model back from GPU to CPU since inference with a relatively small model does not require a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3RkDnUx9_UX"
      },
      "outputs": [],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3bpeGYU63Xs"
      },
      "outputs": [],
      "source": [
        "#   We plug the GPTModel instance (model) into the generate_text_simple function\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate_text_simple(\n",
        "  model=model,\n",
        "  idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "  max_new_tokens=25,\n",
        "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KrN0OAH-nNH"
      },
      "source": [
        "### 5.3.1 - Temperature scaling\n",
        "\n",
        "- Inside the generate_text_simple function, we always sampled the token with the highest probability as the next token, using torch.argmax, also known as greeding decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXBhxO_4-qJk"
      },
      "outputs": [],
      "source": [
        "#   To illustrate probabilistic sampling with a concrete example, lets briefly\n",
        "# discuss the next-token generation process using a very small vocabulary\n",
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "#   Lets assume the LLM is given the start context \"every effor moves yoo\"\n",
        "# and generates the following next-token logits:\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "\n",
        "#   Convert the logits into probabilities via the softmax function and obtain the token ID\n",
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "print(inverse_vocab[next_token_id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYPfQ1MtEcwo"
      },
      "outputs": [],
      "source": [
        "#   The output is \"forward\" just like before.  the multinomial function\n",
        "# samples the next token proportional to its probability score.\n",
        "\n",
        "# Implement a function that repeats the sampling 1000 times:\n",
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123)\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
        "                for i in range(1000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "print_sampled_tokens(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t05ilV82Ecwo"
      },
      "source": [
        "- We can further control the distribution and selection process via a concept call temperature scaling.\n",
        "- Temperature scaling is just a fancy description for dividing the logits by a number greater than 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDi48wUsEcwp"
      },
      "outputs": [],
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "#   Temperatures greater than 1 result in more uniformly distributed token\n",
        "# probabilities, and temperatures small than 1 will result in more confident\n",
        "# (sharper or more peaky) distributions.\n",
        "temperatures = [1, 0.1, 5]\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
        "                 for T in temperatures]\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i, T in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
        "                   bar_width, label=f\"Temerature = {T}\")\n",
        "\n",
        "ax.set_ylabel(\"Probability\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_2xtBl_Ecwp"
      },
      "source": [
        "### 5.3.2 Top-k sampling\n",
        "\n",
        "- Top-k sampling, when combined with probabilistic sampling and temperature scaling can improvve the text generation resluts.  In top-k sampling, we can restric the sampled tokens to the top-k most likely tokens and exclude all other tokens from the selection process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "print(\"Top logits:\", top_logits)\n",
        "print(\"Top positions:\", top_pos)"
      ],
      "metadata": {
        "id": "DNiwDbDp6X9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   Apply PyTorch where function to set the logit values of tokens that are\n",
        "# below the lowest logit value within our top-three selection to negative\n",
        "# infinity\n",
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    input=torch.tensor(float(\"-inf\")),\n",
        "    other=next_token_logits\n",
        ")\n",
        "print(new_logits)"
      ],
      "metadata": {
        "id": "7---HnqOLAcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   Let's apply the softmax function to turn these into next-token probabilities:\n",
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "print(topk_probas)"
      ],
      "metadata": {
        "id": "bv7rbCjfLfXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.3 Modifying the text generation function"
      ],
      "metadata": {
        "id": "wM5Qu6KRLr0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Combine temperature sampling and top-k sampling to modify the\n",
        "# generate_text_simple function\n",
        "def generate(model, idx, max_new_tokens, context_size,\n",
        "             temperature=0.0, top_k=None, eos_id=None):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "    logits = logits[:, -1, :]\n",
        "    if top_k is not None:\n",
        "      top_logits, _ = torch.topk(logits, top_k)\n",
        "      min_val = top_logits[:, -1]\n",
        "      logits = torch.where(\n",
        "          logits < min_val,\n",
        "          torch.tensor(float(\"-inf\")).to(logits.device),\n",
        "          logits\n",
        "      )\n",
        "    if temperature > 0.0:\n",
        "      logits = logits / temperature\n",
        "      probas = torch.softmax(logits, dim=1)\n",
        "      idx_next = torch.multinomial(probas, num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    if idx_next == eos_id:\n",
        "      break\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "xnfcavyALoKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   View the results of new generate function in action\n",
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(\"Every effor moves you\", tokenizer),\n",
        "    max_new_tokens = 15,\n",
        "    context_size = GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k = 25,\n",
        "    temperature = 1.4\n",
        ")\n",
        "\n",
        "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "6QkxoDnnM36-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 - Loading and saving model weights in PyTorch"
      ],
      "metadata": {
        "id": "h2aHvey-NTii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   The recommended way to save a model with torch is the torch.save\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "M9EfzmESNR9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   To load the dict\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\"), map_location=device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yw7Ijy-4HFXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout helps prevent the model from overfitting to the training data by randomly \"dropping out\" of a layer's neurons during training.\n",
        "- If we plan to continue pre-training a model layter - for example, using the train_model_simple function defined earlier in this chapter"
      ],
      "metadata": {
        "id": "k0ueJJ_GHZEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict()\n",
        "  },\n",
        "  \"model_and_optimizer.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "7zBV5yheHWWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the model and optimizer states\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train()"
      ],
      "metadata": {
        "id": "GSDKV1WEH_GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 - Loading pretrained weights from OpenAI\n",
        "\n",
        "- OpenAI openly shared the weights of their GPT-2 models, thes eliminating the need to invest tens to hundreds of thousands of dollars in retraining the model on a large corpus ourselves.\n",
        "- Let's load these weights into our GPTModel class and use the model for text generation."
      ],
      "metadata": {
        "id": "GO338mrXIc3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ],
      "metadata": {
        "id": "EOkvwlGfIbmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   Dowanload code gpt_download.py from chapter's online repository:\n",
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "00p1uyA0I8eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   After downloading this file to the local directory\n",
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=\"124M\", modes_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "id": "Z6l3kSR0Ji-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4vywz8qfJzvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}