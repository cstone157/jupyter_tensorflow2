{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TPQpy_PLJiw",
    "outputId": "7d45dd83-4ced-4d49-8d58-548c477f683a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzQx4mxrLTNY",
    "outputId": "8c922743-e67b-435a-f937-63263bfb8e2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D2SgL7aNMLg7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "## Import code from previous chapters\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "    return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return 0.5 * x * (1 + torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "        (x + 0.44715 * torch.pow(x, 3))\n",
    "    ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out,\n",
    "               context_length, dropout, num_heads, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_out // num_heads\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.out_proj = nn.Linear(d_out, d_out)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        \"mask\",\n",
    "        torch.triu(torch.ones(context_length, context_length),\n",
    "                   diagonal=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, num_tokens, d_in = x.shape\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    queries = queries.view(\n",
    "        b, num_tokens, self.num_heads, self.head_dim\n",
    "    )\n",
    "\n",
    "    keys = keys.transpose(1, 2)\n",
    "    queries = queries.transpose(1, 2)\n",
    "    values = values.transpose(1, 2)\n",
    "\n",
    "    attn_scores = queries @ keys.transpose(2, 3)\n",
    "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "    attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim = -1\n",
    "    )\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "    context_vec = context_vec.contiguous().view(\n",
    "        b, num_tokens, self.d_out\n",
    "    )\n",
    "    context_vec = self.out_proj(context_vec)\n",
    "    return context_vec\n",
    "\n",
    "#   The transformer block component of GPT\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.att = MultiHeadAttention(\n",
    "        d_in = cfg[\"emb_dim\"],\n",
    "        d_out = cfg[\"emb_dim\"],\n",
    "        context_length = cfg[\"context_length\"],\n",
    "        num_heads = cfg[\"n_heads\"],\n",
    "        dropout = cfg[\"drop_rate\"],\n",
    "        qkv_bias = cfg[\"qkv_bias\"])\n",
    "\n",
    "    self.ff = FeedForward(cfg)\n",
    "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "  def forward(self, x):\n",
    "    shortcut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.att(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "    return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    self.trf_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "    )\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "    pos_embeds = self.pos_emb(\n",
    "      torch.arange(seq_len, device=in_idx.device)\n",
    "    )\n",
    "\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhZKiRRFLWzV"
   },
   "source": [
    "# Pretraining on unlabeled data\n",
    "\n",
    "## 5.1 - Evaluating generative text models\n",
    "\n",
    "### 5.1.1 - Using GPT to generate text\n",
    "\n",
    "  - Let's set up the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZsJqYy7LYF7",
    "outputId": "70a3c2c7-fc5e-4231-b0ba-24eca6bd2dcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_ow4GowNXJ8",
    "outputId": "80c9f5f4-774c-4012-ffb2-87cc1167232e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#   Utility functions for text to token ID conversion\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text_simple\u001b[39m(model, idx,\n\u001b[1;32m      5\u001b[0m         max_new_tokens, context_size):\n\u001b[1;32m      6\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "#   Utility functions for text to token ID conversion\n",
    "import tiktoken\n",
    "\n",
    "def generate_text_simple(model, idx,\n",
    "        max_new_tokens, context_size):\n",
    "  for _ in range(max_new_tokens):\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "    with torch.no_grad():\n",
    "      logits = model(idx_cond)\n",
    "\n",
    "    logits = logits[:, -1, :]\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "  return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "  return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "  flat = token_ids.squeeze(0)\n",
    "  return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hXZDvTCPvm9"
   },
   "source": [
    "  - The model isn't yet producing coherent text.\n",
    "  - Next, we will calculate a loss metric for the generated outputs.\n",
    "\n",
    "### 5.1.2 - calculating the text generation loss\n",
    "\n",
    "  - Techniques for numerically assessing text quality generated during training by calculating a text generation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdCB3aGHPrjn",
    "outputId": "8a989f36-7a49-44a9-93e1-776c1ed2d08b"
   },
   "outputs": [],
   "source": [
    "#   Consider two input examples, which have already been mapped\n",
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])  # \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],     # [\" effort moves you\",\n",
    "                        [1107, 588,  11311]])  # \"really like chocolate\"]\n",
    "\n",
    "#   Now feed the inputs into model to calculate logits vectors for thewo input\n",
    "# examples, each of three tokens.\n",
    "with torch.no_grad():\n",
    "  logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim = -1)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HhT34tQUO4iQ",
    "outputId": "e833be62-d268-4809-9b84-4b84861e5e81"
   },
   "outputs": [],
   "source": [
    "#   The first number, 2, corresponds to the two examples (rows)\n",
    "#\n",
    "#   The second number, 3, corresponds to the number of tokens in each input (row)\n",
    "#\n",
    "#   The last number corresponds to the embedding dimensionality, which is determined\n",
    "# by the vocabulary size.\n",
    "\n",
    "#   Following the conversion from logits to probabilities via the softmax function,\n",
    "# the generate_text_simple function then converts the resulting probability scores\n",
    "# back into text\n",
    "token_ids = torch.argmax(probas, dim = -1, keepdim = True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AY_L8W4BPOar",
    "outputId": "fdc44328-6a85-43c9-b493-8860422d3591"
   },
   "outputs": [],
   "source": [
    "#   Finally convert the token IDs back into text:\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 2: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A721cjYAXHFV"
   },
   "source": [
    "  - Now we need to evaluate the performance of the model's generated text numerically via a loss.  \n",
    "  - This is useful for measuring the quality of the generated text, but also as a building block for implementing training function.\n",
    "  - Part of text evaluation process that we implement, is to measure \"how far\" the generated tokens are from correct predictions (targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZbeyZhGXExr",
    "outputId": "45422a89-b74a-44d4-87db-0a81f724cf93"
   },
   "outputs": [],
   "source": [
    "#   For each of the two input texts, we can print the initial softmax\n",
    "# probability scores corresponding to the target torkens using the following code:\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS5Al7ERY5Xt"
   },
   "source": [
    "  - The goal of training an LLM is to maximize the likelihood of the correct token, which involves increasing its probability relative to other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cC8UGhweY1IE",
    "outputId": "87a34a2b-89c7-4fe1-cce1-414cb3c9f0b7"
   },
   "outputs": [],
   "source": [
    "#   Next, we calculate the loss for the probability scores of the two example batches.\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQndsbkxZXDD",
    "outputId": "17cce2c9-97ae-43a6-98e6-0dc5250cdfa5"
   },
   "outputs": [],
   "source": [
    "#   Next we combine these log probabilities into a single score\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVDDBgiraM3p",
    "outputId": "5c5797cc-8207-42cd-dbc9-82925c86c4c9"
   },
   "outputs": [],
   "source": [
    "#   The goal is to get the average log probability as close to 0 as possible by\n",
    "# updating the model's weights as part of the training process.\n",
    "#\n",
    "#   The goal is to bring the negative average log probability down to 0.\n",
    "#\n",
    "#   The term for turning the negative falu is known as cross entropy loss.\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAAYP9PdastS",
    "outputId": "345dd6e3-3bf7-466f-b990-72008ffa3398"
   },
   "outputs": [],
   "source": [
    "#   Review the shape of the logits and target tensors:\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvE34lfNbJYY",
    "outputId": "909d51ea-4090-4a5d-ef2c-d6d7bec3530a"
   },
   "outputs": [],
   "source": [
    "#   For the cross_entropy loss function in PyTorch, we want to flatten these\n",
    "# tensors by combining them over the batch dimension:\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits: \", logits_flat.shape)\n",
    "print(\"Flattened targets: \", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFYf_DiGbheC",
    "outputId": "8c705781-8f3d-4e9b-913c-799bbc7a13d8"
   },
   "outputs": [],
   "source": [
    "#   Previously, we applied the softmax function, selected the probaility scores\n",
    "# corresponding to the target IDs, and computed the negative average log\n",
    "# probabilities.  PyTorch's cross_entropy function will take care of all the\n",
    "# steps for us:\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-D2DqoqcFMM"
   },
   "source": [
    "### 5.1.3 - Calculating the training and validation set losses\n",
    "\n",
    "  - We must first prepare the training and validation datasets that we will use to train the LLM.\n",
    "  - To compute the loss on the training and validation datasets, we use a very small text dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Yhb052KcBAK"
   },
   "outputs": [],
   "source": [
    "#   The following code loads \"The Verdict\"\n",
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "  text_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TMVFNVCIfozQ",
    "outputId": "84780765-e83e-45a1-89f0-bf7cba9bbed1"
   },
   "outputs": [],
   "source": [
    "#   Check the number of charaters and tokens in data set\n",
    "total_caracters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_caracters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Hq-fHafz17"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "  dataloader = DataLoader(\n",
    "      dataset,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=shuffle,\n",
    "      drop_last=drop_last,\n",
    "      num_workers=num_workers\n",
    "  )\n",
    "\n",
    "  return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Twwmd51qg6eX"
   },
   "outputs": [],
   "source": [
    "#   Next divide the dataset into a training and validation set and use the data\n",
    "# loaders from chapter 2\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "#   Usie the train_data and val_data subsets, create respective data loaders\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "  train_data,\n",
    "  batch_size=2,\n",
    "  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "  stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "  shuffle=True,\n",
    "  drop_last=True,\n",
    "  num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "  val_data,\n",
    "  batch_size=2,\n",
    "  max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "  stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "  shuffle=True,\n",
    "  drop_last=True,\n",
    "  num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBa2KfqlhitW",
    "outputId": "0e456980-2e53-4350-b8cb-c2e0449f6eea"
   },
   "outputs": [],
   "source": [
    "#   We used a relatively small batch size to reduce the computational resource\n",
    "# demand because we were working with a very small dataset.\n",
    "#  Iterate through the data loaders to ensure that they were create correctly:\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "  print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "  print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yYNjW9BiFy4"
   },
   "outputs": [],
   "source": [
    "#   Implement a utility function to calculate the cross entropy loss of a given\n",
    "# batch returned via the training and validation loader:\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "  input_batch = input_batch.to(device)\n",
    "  target_batch = target_batch.to(device)\n",
    "  logits = model(input_batch)\n",
    "  loss = torch.nn.functional.cross_entropy(\n",
    "      logits.flatten(0, 1), target_batch.flatten()\n",
    "  )\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAut4NV6jUh4"
   },
   "outputs": [],
   "source": [
    "#   Function to compute the training and validation loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "  total_loss = 0\n",
    "  if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "  elif num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "  else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "    if i < num_batches:\n",
    "      loss = calc_loss_batch(\n",
    "          input_batch, target_batch, model, device\n",
    "      )\n",
    "      total_loss += loss.item()\n",
    "    else:\n",
    "      break\n",
    "\n",
    "  return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hr1rapGkEMy",
    "outputId": "689d9a05-8fe8-4de8-81b0-90d08699f687"
   },
   "outputs": [],
   "source": [
    "#   Let's see calc_loss_loader function in action\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  train_loss = calc_loss_loader(train_loader, model, device)\n",
    "  val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss: \", train_loss)\n",
    "print(\"Validation loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7gopjF2k7yS"
   },
   "source": [
    "  - The loss values are relatively high because the model hasn't been trained.\n",
    "  \n",
    "## 5.2 - Training an LLM\n",
    "\n",
    "  - Time to implement the code for pretraining the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABBPJcp1k5P9"
   },
   "outputs": [],
   "source": [
    "#   Themain function for pretraining LLMs\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "  tokens_seen, global_step = 0, -1\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_batch, target_batch in train_loader:\n",
    "      optimizer.zero_grad()\n",
    "      loss = calc_loss_batch(\n",
    "          input_batch, target_batch, model, device\n",
    "      )\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      tokens_seen += input_batch.numel()\n",
    "      global_step += 1\n",
    "\n",
    "      if global_step % eval_freq == 0:\n",
    "        train_loss, val_loss = evaluate_model(\n",
    "          model, train_loader, val_loader, device, eval_iter\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        track_tokens_seen.append(tokens_seen)\n",
    "        print(f\"Ep {epoch+1} (Step {global_step:06d})\"\n",
    "              f\"Train loss {train_loss:.3f}, \"\n",
    "              f\"Val loss {val_loss:.3f}\"\n",
    "        )\n",
    "\n",
    "    generate_and_print_sample(\n",
    "      model, tokenizer, device, start_context\n",
    "    )\n",
    "  return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "#   The evaluate_model function calculates the loss over the training and\n",
    "# validation set wwhile ensuring the model is in evaluation mode with gradient\n",
    "# tracking and dropout disabled when calculating the loss over the training\n",
    "# and validation sets:\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "      train_loader, model, device, num_batches=eval_iter\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "      val_loader, model, device, num_batches=eval_iter\n",
    "    )\n",
    "  model.train()\n",
    "  return train_loss, val_loss\n",
    "\n",
    "#   Function used to track whether the model impproves during training.\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "  model.eval()\n",
    "  context_size = model.pos_emb.weight.shape[0]\n",
    "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "  with torch.no_grad():\n",
    "    token_ids = generate_text_simple(\n",
    "      model=model, idx=encoded,\n",
    "      max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "  print(decoded_text.replace(\"\\n\", \" \"))\n",
    "  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fc2K0N3czsbq",
    "outputId": "ce0480a5-b1ea-4832-961a-76fc7fcfaa1d"
   },
   "outputs": [],
   "source": [
    "#   Let's see this all in action by training a GPTModel instance for 10 epochs\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "  model.parameters(),\n",
    "  lr = 0.0004, weight_decay = 0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_m87EZK0h12"
   },
   "outputs": [],
   "source": [
    "# Simple plot that shows the training and validation set losses side by side:\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "  ax1.plot(epochs_seen, train_losses, lobel=\"Training loss\")\n",
    "  ax1.plot(\n",
    "      epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "  )\n",
    "  axl.set_xlabel(\"Epochs\")\n",
    "  axl.set_ylabel(\"Loss\")\n",
    "  axl.legend(loc=\"upper right\")\n",
    "  axl.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "  ax2 = ax1.twiny()\n",
    "  ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "  ax2.set_xlabel(\"Tokens seen\")\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1Lwf_on9BDn"
   },
   "source": [
    "- Both the training and validation losses start to improve for the first epoch.  The losses start to diverge past the second epoch.  This divergence and the fact that the validation loss is much larger than the training loss indicate that the model is overfitting to the training data.\n",
    "- We can confirm that the model memorizes the training data verbatim by searching for the generated text snippets\n",
    "- The memorization is expected since we are working with a very, small training dataset and training the mode for multiple epochs.\n",
    "\n",
    "## 5.3 - Decoding strategies to control randomness\n",
    "\n",
    "- Let's look at text generation strategies.\n",
    "- We will begin by transferring the model back from GPU to CPU since inference with a relatively small model does not require a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3RkDnUx9_UX"
   },
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3bpeGYU63Xs"
   },
   "outputs": [],
   "source": [
    "#   We plug the GPTModel instance (model) into the generate_text_simple function\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "  model=model,\n",
    "  idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "  max_new_tokens=25,\n",
    "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text: \\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KrN0OAH-nNH"
   },
   "source": [
    "### 5.3.1 - Temperature scaling\n",
    "\n",
    "- Inside the generate_text_simple function, we always sampled the token with the highest probability as the next token, using torch.argmax, also known as greeding decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXBhxO_4-qJk"
   },
   "outputs": [],
   "source": [
    "#   To illustrate probabilistic sampling with a concrete example, lets briefly \n",
    "# discuss the next-token generation process using a very small vocabulary\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "inverse_vocab = [v: k for k, v in vocab.items()}\n",
    "\n",
    "#   Lets assume the LLM is given the start context \"every effor moves yoo\"\n",
    "# and generates the following next-token logits:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "#   Convert the logits into probabilities via the softmax function and obtain the token ID\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
