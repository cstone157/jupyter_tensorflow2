{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bb6c1f-14a5-4d39-acf5-dc18c8b1da9f",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "## Word embedding - origins and fundamentals\n",
    "-    collective name for a set of language modeling and feature learning techings in natural language processing (NLP)\n",
    "-    one-hot encoding is early example, but doesn't work\n",
    "-    other examples borrow from Information Retrieval (IR): Term Frequency-Inverse Document Frequency (TF-IDF), Latent Semantic Analysis (LSA), and topic modeling\n",
    "\n",
    "### Distributed representations\n",
    "-    Attempt to capture the meaning of word by considering its relations with other words in its context.\n",
    "\n",
    "### Static embeddings\n",
    "-    Embeddings are generated against a large corpus, but the nuumber of words, though large, is finite.\n",
    "-    Think of static embedding as a dictionary\n",
    "#### Word2Vec\n",
    "-    Self-supervised\n",
    "-    Continuous Bag of Words (CBOW) and Skip-gram\n",
    "-    Skip-Gram with Negative Sampling (SGNS) model\n",
    "-    GloVe - Global vectors for word representation\n",
    "#### Creating your own embeddings using Gesim\n",
    "-    Gesim is an open-source python library designed to extract semantic meaning from text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7611c997-ee87-4060-b7fe-f8469126c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "dataset = api.load(\"text8\")\n",
    "model = Word2Vec(dataset)\n",
    "model.save(\"data/text8-word2vec.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb6c3d-9210-406d-980c-cbbea85ee745",
   "metadata": {},
   "source": [
    "### Exploring the embedding space with Gensim\n",
    "-    Reload the model we just built and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56cb749-5981-4703-8e6d-40649e54fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load(\"data/text8-word2vec.bin\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01986db9-eadf-4986-b05f-8488cc761906",
   "metadata": {},
   "source": [
    "-    Look at the first few words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c002a93e-57d2-49c7-85cc-4d1f9990b3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words = word_vectors.vocab.keys()\n",
    "\n",
    "#print([x for i, x in enumerate(words) if i < 10])\n",
    "#assert(\"king\" in words)\n",
    "\n",
    "my_dict = dict({})\n",
    "i = 0\n",
    "for idx, key in enumerate(model.wv.key_to_index):\n",
    "    my_dict[key] = model.wv[key]\n",
    "    i += 1\n",
    "    if i >= 10:\n",
    "        break\n",
    "\n",
    "my_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919fc3-40cd-4668-b0aa-8d0f954f4b6c",
   "metadata": {},
   "source": [
    "Look for similar words to a given word \"king\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7333eba3-191d-4658-a242-e4e9314e8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729 prince\n",
      "0.699 emperor\n",
      "0.697 vii\n",
      "0.695 constantine\n",
      "0.690 throne\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar(word_conf_pairs, k):\n",
    "    for i, (word, conf) in enumerate(word_conf_pairs):\n",
    "        print(\"{:.3f} {:s}\".format(conf, word))\n",
    "        if i >= k-1:\n",
    "            break\n",
    "    if k < len(word_conf_pairs):\n",
    "        print(\"...\")\n",
    "\n",
    "print_most_similar(word_vectors.most_similar(\"king\"), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726cb68-753c-42f0-ae59-2b7ea6d19851",
   "metadata": {},
   "source": [
    "You can also do vector arithmetic similar to the country-capital example we described earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9090fa-884e-4b74-8039-5c423940c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779 germany\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print_most_similar(word_vectors.most_similar(\n",
    "    positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d125030-6e42-4f40-8f65-a915a2bc7cf5",
   "metadata": {},
   "source": [
    "The preceding similaring value is reported cosine.  Alternatively copyte the distance with lag scale, amplifying the difference between sorter distance and reducing the difference between longer ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434ecfc7-0700-46d5-939f-9d3eb1d0bd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.940 germany\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print_most_similar(word_vectors.most_similar_cosmul(\n",
    "    positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3e276-712d-4156-8c92-498b427e4d1d",
   "metadata": {},
   "source": [
    "Gensim also provides a doesnt_match function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36d22d6-2140-4164-a82d-4d6648afea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singapore\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.doesnt_match([\"hindus\", \"parsis\", \"singapore\", \"christians\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f7f4d-7697-412d-acf5-7fbcef0ff319",
   "metadata": {},
   "source": [
    "We can also calculate the similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "388dca6c-3aad-4abe-9b4b-40e14a2d0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(man, woman) = 0.724\n",
      "similarity(man, dog) = 0.413\n",
      "similarity(man, whale) = 0.245\n",
      "similarity(man, tree) = 0.286\n"
     ]
    }
   ],
   "source": [
    "for word in [\"woman\", \"dog\", \"whale\", \"tree\"]:\n",
    "    print(\"similarity({:s}, {:s}) = {:.3f}\".format(\n",
    "        \"man\", word, word_vectors.similarity(\"man\", word)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aec03d-3073-4184-998b-4cc893994e7b",
   "metadata": {},
   "source": [
    "similar_by_word() function is simlar to similar(), except the latter normalizes the vector between comparing by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2652a15e-9956-43be-9579-05ac0c5400ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877 malaysia\n",
      "0.847 indonesia\n",
      "0.825 uganda\n",
      "0.815 nepal\n",
      "0.810 thailand\n",
      "...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print_most_similar(\n",
    "    word_vectors.similar_by_word(\"singapore\"), 5)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ffaa85-5781-4adc-a347-ebb8e3876677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance(singapore, malaysia) = 0.123\n"
     ]
    }
   ],
   "source": [
    "print(\"distance(singapore, malaysia) = {:.3f}\".format(\n",
    "    word_vectors.distance(\"singapore\", \"malaysia\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7b8fa-e39f-4172-af08-8084e6608bb9",
   "metadata": {},
   "source": [
    "Lookup vectors for a vocabulary word either directly from the word_vectors object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28beee74-35ed-474e-ac85-2c6b5715ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_song = word_vectors[\"song\"]\n",
    "#vec_song_2 = word_vectors.word_vec(\"song\", use_norm=True) ## Throws an error \"KeyedVectors.get_vector() got an unexpected keyword argument 'use_norm'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76243a9-a778-4760-917a-ae2918763a67",
   "metadata": {},
   "source": [
    "### Using word embeddings for spam detection\n",
    "\n",
    "Embeddings provide dense fixed dimension vector for each token.  Each token is replaced with its vector, and this converts the sequence of text into a matrix of examples, each of which has a fixed number of features corresponding to the dimensionality of the embedding.\n",
    "\n",
    "    - Convolutional Neural Network (CNN)\n",
    "    - Short Message Service (SMS)\n",
    "\n",
    "We will see how the program learns an embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15ba6e1-a8be-4dfd-88d7-4c16e75019c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 03:00:01.531097: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 03:00:01.531206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 03:00:01.614039: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-17 03:00:01.777360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn\n",
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbef93f-b850-4a2a-9520-1846320227eb",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d74858-13dd-47cb-a116-3233385298ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url,\n",
    "        extract=True, cache_dir=\".\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == 'spam' else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c1ac1-c678-4813-80d0-43b116f6ff17",
   "metadata": {},
   "source": [
    "### Making the data ready for use\n",
    "\n",
    "    - The next step is to process the data so it can be consumed.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0b87db-91e6-4959-bde6-e0f08c85263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189\n"
     ]
    }
   ],
   "source": [
    "# tokenize and pad text\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "num_records = len(text_sequences)\n",
    "max_seqlen = len(text_sequences[0])\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b23a74-13ce-4299-954e-803eb1216ceb",
   "metadata": {},
   "source": [
    "Convert our labels to categorical or one-hot encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7873dc8f-d7f9-43e4-9ed3-acc6ee40990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "NUM_CLASSES = 2\n",
    "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a720c6d-51fe-46b9-978d-3d9f6efd4bee",
   "metadata": {},
   "source": [
    "Tokenizer allows access to the vocabulary created through the word_index attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c70a6c-c539-4e27-ac8e-84f2af550d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9010\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222bdc2-c4cc-4080-9ca3-9b4c4b07a2dc",
   "metadata": {},
   "source": [
    "Create the dataset object that our network will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9bc28f-9fdc-46f6-9c20-9d700146c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num_records // 4\n",
    "val_size = (num_records - text_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "BATCH_SIZE = 128\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de81f6d-c9e5-412e-87a4-5d207451d1d3",
   "metadata": {},
   "source": [
    "### Building the embedding matrix\n",
    "\n",
    "- Gensim toolkit provides access to various trained embedding models.  You can see them by running\n",
    "<code>import gensim.downloader as api\n",
    "api.info(\"models\").keys()</code>\n",
    "\n",
    "- A cople of the trained word embeddings:\n",
    "    - Word2Vec\n",
    "    - GloVe\n",
    "    - fastText\n",
    "    - ConceptNet Numberbatch\n",
    "\n",
    "<hr/>\n",
    "For our example, we will chose the 300d CloVe embeddings trained on the Gigaword corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fe739a-dfb2-4131-918f-1c98954221f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 14.9% 55.9/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "Embedding matrix: (9010, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_394/3660993592.py:10: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  E[idx] = word_vectors.word_vec(word)\n"
     ]
    }
   ],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        E = np.load(embedding_file)\n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        E = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                E[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError: # word not im embedding\n",
    "                pass\n",
    "        np.save(embedding_file, E)\n",
    "    return E\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "DATA_DIR = \"data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "E = build_embedding_matrix(text_sequences, word2idx,\n",
    "                           EMBEDDING_DIM, EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix:\", E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af765d-6a36-4d20-94da-ad4d4fe4fda9",
   "metadata": {},
   "source": [
    "## Defining the spam classifier\n",
    "\n",
    "- We will use a one-dimensional Convolutional Neural Network or ConvNet (1D CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e801122a-fe14-462e-8115-42f995ffb862",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_mode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m conv_num_filters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[1;32m     39\u001b[0m conv_kernel_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m SpamClassifierModel(\n\u001b[1;32m     41\u001b[0m     vocab_size, EMBEDDING_DIM, max_seqlen,\n\u001b[1;32m     42\u001b[0m     conv_num_filters, conv_kernel_size, NUM_CLASSES,\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mrun_mode\u001b[49m, E)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_mode' is not defined"
     ]
    }
   ],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embeded_sz, input_length,\n",
    "                 num_filters, kernel_sz, output_sz,\n",
    "                 run_mode, embedding_weights,\n",
    "                 **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "                embeded_sz, input_length=input_length,\n",
    "                trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "                embeded_sz, input_length=input_length,\n",
    "                trainable=False)\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "                embeded_sz, input_length=input_length,\n",
    "                weights=[embedding_weights],\n",
    "                trainable=True)\n",
    "\n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "            kernel_size=kernel_sz, activation=\"relu\")\n",
    "\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz,\n",
    "            activation=\"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "# Model definition\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(\n",
    "    vocab_size, EMBEDDING_DIM, max_seqlen,\n",
    "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "    run_mode, E)\n",
    "model.build(input_shape=(None, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6186726-b082-4769-9063-3cd7dc6d3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a8fb0-eea6-45f6-b625-50dce9802af2",
   "metadata": {},
   "source": [
    "### The individuals that wrote the book couldn't be bothered to write complete code the above example doesn't work.  Check the other folders in chapter 04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19469b-4f15-48fd-afef-9ab5b12043bc",
   "metadata": {},
   "source": [
    "## Training and evaluating the model\n",
    "\n",
    "The dataset is imbalanced, with 747 instaces of spam to 4827 instances of ham.  We set class weights to indicat that an error on a spam SMS is eight times as expensive as an error on a ham SMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1d65e-c43c-46d1-8aa2-fbd28db7eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "# data distrinution is 4827 ham and 747 spam (total 5574), which\n",
    "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
    "# and this works out to being each spam (1) item as being \n",
    "# approximately 8 times as important as each ham (0) message.\n",
    "CLASS_WEIGHTS = { 0: 1, 1: 8 }\n",
    "# train model\n",
    "model.fit(train_dataset, epochs=NUM_EPOCHS,\n",
    "          validation_data=val_dataset,\n",
    "          class_weight=CLASS_WEIGHTS)\n",
    "# evaluate against test set\n",
    "labels, predictions = [], []\n",
    "for Xtest, Ytest in test_dataset:\n",
    "    Ytest_ = model.predict_on_batch(Xtest)\n",
    "    ytest = np.argmax(Ttest, axis=1)\n",
    "    labels.extend(ytest.tolist())\n",
    "    predictions.extend(ytest.tolist())\n",
    "\n",
    "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6248fc-5eb3-4511-b925-d2f4eef5b144",
   "metadata": {},
   "source": [
    "## Running the spam detector\n",
    "\n",
    "The 3 scenarios we want to look at are:\n",
    "- let the network lean the embedding for the task\n",
    "- start with fixed external third-party embedding where the embedding matrix is treated like a vectorizer to transorm the sequence of integers into a sequence of vectors\n",
    "- Starting with external thrid party embedding which is further fine tuned to the task during the training.\n",
    "\n",
    "## Neural embedding - not just for words\n",
    "\n",
    "### Item2Vec\n",
    "### node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefa3b36-0ca6-40e7-b14a-292d5d7c1abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 04:21:49.432116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-05 04:21:49.432343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-05 04:21:49.526978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-05 04:21:49.719163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "logging.basicConfig(format='%asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391b049-42b3-40f3-bfba-6d9303c7d6ea",
   "metadata": {},
   "source": [
    "Next downloa dthe data from UCI repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63bae11b-ec52-42ee-8f57-a3fa2362a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "UCI_DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\"\n",
    "\n",
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url, cache_dir=\".\")\n",
    "    row_ids, col_ids, data = [], [], []\n",
    "    rid = 0\n",
    "    f = open(p, \"r\")\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"\\\"\\\",\"):\n",
    "            # header\n",
    "            continue\n",
    "\n",
    "        # compute non-zero elements for current row\n",
    "        counts = np.array([int(x) for x in line.split(',')[1:]])\n",
    "        nz_col_ids = np.nonzero(counts)[0]\n",
    "        nz_data = counts[nz_col_ids]\n",
    "        nz_row_ids = np.repeat(rid, len(nz_col_ids))\n",
    "        rid += 1\n",
    "\n",
    "        # add data to big lists\n",
    "        row_ids.extend(nz_row_ids.tolist())\n",
    "        col_ids.extend(nz_col_ids.tolist())\n",
    "        data.extend(nz_data.tolist())\n",
    "    f.close()\n",
    "    TD = csr_matrix((\n",
    "        np.array(data), (\n",
    "            np.array(row_ids), np.array(col_ids)\n",
    "        )),\n",
    "        shape=(rid, counts.shape[0]))\n",
    "\n",
    "    return TD\n",
    "\n",
    "# read data and convert to Term-Document matrix\n",
    "TD = download_and_read(UCI_DATA_URL)\n",
    "\n",
    "# compute undirected, unweighted edge matrix\n",
    "E = TD.T * TD\n",
    "\n",
    "#binarize\n",
    "E[E > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2480869-3fb8-41e9-8693-d4dc1a05c68c",
   "metadata": {},
   "source": [
    "Note this is a very slow process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1046dda-f535-4931-9904-ad2f907e5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WALKS_PER_VERTEX = 32\n",
    "MAX_PATH_LENGTH = 40\n",
    "RESTART_PROB = 0.15\n",
    "RANDOM_WALKS_FILE = os.path.join(DATA_DIR, \"random-walks.txt\")\n",
    "def construct_random_walks(E, n, alpha, l, ofile):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
