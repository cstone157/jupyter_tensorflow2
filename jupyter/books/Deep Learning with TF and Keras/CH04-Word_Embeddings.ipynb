{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bb6c1f-14a5-4d39-acf5-dc18c8b1da9f",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "## Word embedding - origins and fundamentals\n",
    "-    collective name for a set of language modeling and feature learning techings in natural language processing (NLP)\n",
    "-    one-hot encoding is early example, but doesn't work\n",
    "-    other examples borrow from Information Retrieval (IR): Term Frequency-Inverse Document Frequency (TF-IDF), Latent Semantic Analysis (LSA), and topic modeling\n",
    "\n",
    "### Distributed representations\n",
    "-    Attempt to capture the meaning of word by considering its relations with other words in its context.\n",
    "\n",
    "### Static embeddings\n",
    "-    Embeddings are generated against a large corpus, but the nuumber of words, though large, is finite.\n",
    "-    Think of static embedding as a dictionary\n",
    "#### Word2Vec\n",
    "-    Self-supervised\n",
    "-    Continuous Bag of Words (CBOW) and Skip-gram\n",
    "-    Skip-Gram with Negative Sampling (SGNS) model\n",
    "-    GloVe - Global vectors for word representation\n",
    "#### Creating your own embeddings using Gesim\n",
    "-    Gesim is an open-source python library designed to extract semantic meaning from text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7611c997-ee87-4060-b7fe-f8469126c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "dataset = api.load(\"text8\")\n",
    "model = Word2Vec(dataset)\n",
    "model.save(\"data/text8-word2vec.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb6c3d-9210-406d-980c-cbbea85ee745",
   "metadata": {},
   "source": [
    "### Exploring the embedding space with Gensim\n",
    "-    Reload the model we just built and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56cb749-5981-4703-8e6d-40649e54fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load(\"data/text8-word2vec.bin\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01986db9-eadf-4986-b05f-8488cc761906",
   "metadata": {},
   "source": [
    "-    Look at the first few words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c002a93e-57d2-49c7-85cc-4d1f9990b3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words = word_vectors.vocab.keys()\n",
    "\n",
    "#print([x for i, x in enumerate(words) if i < 10])\n",
    "#assert(\"king\" in words)\n",
    "\n",
    "my_dict = dict({})\n",
    "i = 0\n",
    "for idx, key in enumerate(model.wv.key_to_index):\n",
    "    my_dict[key] = model.wv[key]\n",
    "    i += 1\n",
    "    if i >= 10:\n",
    "        break\n",
    "\n",
    "my_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919fc3-40cd-4668-b0aa-8d0f954f4b6c",
   "metadata": {},
   "source": [
    "Look for similar words to a given word \"king\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7333eba3-191d-4658-a242-e4e9314e8789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737 prince\n",
      "0.733 queen\n",
      "0.716 emperor\n",
      "0.709 vii\n",
      "0.691 throne\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def print_most_similar(word_conf_pairs, k):\n",
    "    for i, (word, conf) in enumerate(word_conf_pairs):\n",
    "        print(\"{:.3f} {:s}\".format(conf, word))\n",
    "        if i >= k-1:\n",
    "            break\n",
    "    if k < len(word_conf_pairs):\n",
    "        print(\"...\")\n",
    "\n",
    "print_most_similar(word_vectors.most_similar(\"king\"), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f726cb68-753c-42f0-ae59-2b7ea6d19851",
   "metadata": {},
   "source": [
    "You can also do vector arithmetic similar to the country-capital example we described earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9090fa-884e-4b74-8039-5c423940c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 germany\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print_most_similar(word_vectors.most_similar(\n",
    "    positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d125030-6e42-4f40-8f65-a915a2bc7cf5",
   "metadata": {},
   "source": [
    "The preceding similaring value is reported cosine.  Alternatively copyte the distance with lag scale, amplifying the difference between sorter distance and reducing the difference between longer ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434ecfc7-0700-46d5-939f-9d3eb1d0bd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.971 germany\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print_most_similar(word_vectors.most_similar_cosmul(\n",
    "    positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3e276-712d-4156-8c92-498b427e4d1d",
   "metadata": {},
   "source": [
    "Gensim also provides a doesnt_match function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b36d22d6-2140-4164-a82d-4d6648afea4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singapore\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.doesnt_match([\"hindus\", \"parsis\", \"singapore\", \"christians\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f7f4d-7697-412d-acf5-7fbcef0ff319",
   "metadata": {},
   "source": [
    "We can also calculate the similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388dca6c-3aad-4abe-9b4b-40e14a2d0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(man, woman) = 0.729\n",
      "similarity(man, dog) = 0.463\n",
      "similarity(man, whale) = 0.294\n",
      "similarity(man, tree) = 0.322\n"
     ]
    }
   ],
   "source": [
    "for word in [\"woman\", \"dog\", \"whale\", \"tree\"]:\n",
    "    print(\"similarity({:s}, {:s}) = {:.3f}\".format(\n",
    "        \"man\", word, word_vectors.similarity(\"man\", word)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aec03d-3073-4184-998b-4cc893994e7b",
   "metadata": {},
   "source": [
    "similar_by_word() function is simlar to similar(), except the latter normalizes the vector between comparing by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2652a15e-9956-43be-9579-05ac0c5400ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869 malaysia\n",
      "0.828 philippines\n",
      "0.824 indonesia\n",
      "0.815 thailand\n",
      "0.803 nigeria\n",
      "...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print_most_similar(\n",
    "    word_vectors.similar_by_word(\"singapore\"), 5)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3ffaa85-5781-4adc-a347-ebb8e3876677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance(singapore, malaysia) = 0.131\n"
     ]
    }
   ],
   "source": [
    "print(\"distance(singapore, malaysia) = {:.3f}\".format(\n",
    "    word_vectors.distance(\"singapore\", \"malaysia\")\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d7b8fa-e39f-4172-af08-8084e6608bb9",
   "metadata": {},
   "source": [
    "Lookup vectors for a vocabulary word either directly from the word_vectors object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28beee74-35ed-474e-ac85-2c6b5715ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_song = word_vectors[\"song\"]\n",
    "#vec_song_2 = word_vectors.word_vec(\"song\", use_norm=True) ## Throws an error \"KeyedVectors.get_vector() got an unexpected keyword argument 'use_norm'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76243a9-a778-4760-917a-ae2918763a67",
   "metadata": {},
   "source": [
    "### Using word embeddings for spam detection\n",
    "\n",
    "Embeddings provide dense fixed dimension vector for each token.  Each token is replaced with its vector, and this converts the sequence of text into a matrix of examples, each of which has a fixed number of features corresponding to the dimensionality of the embedding.\n",
    "\n",
    "    - Convolutional Neural Network (CNN)\n",
    "    - Short Message Service (SMS)\n",
    "\n",
    "We will see how the program learns an embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c15ba6e1-a8be-4dfd-88d7-4c16e75019c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbef93f-b850-4a2a-9520-1846320227eb",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d74858-13dd-47cb-a116-3233385298ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "   8192/Unknown - 0s 0us/step"
     ]
    }
   ],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    p = tf.keras.utils.get_file(local_file, url,\n",
    "        extract=True, cache_dir=\".\")\n",
    "    labels, texts = [], []\n",
    "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "    with open(local_file, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            label, text = line.strip().split('\\t')\n",
    "            labels.append(1 if label == 'spam' else 0)\n",
    "            texts.append(text)\n",
    "    return texts, labels\n",
    "\n",
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c1ac1-c678-4813-80d0-43b116f6ff17",
   "metadata": {},
   "source": [
    "### Making the data ready for use\n",
    "\n",
    "    - The next step is to process the data so it can be consumed.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
