{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a832742-9e34-4f3d-9c1e-a34ff62bde4a",
   "metadata": {},
   "source": [
    "# V1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a013b87-5143-45e7-bc25-a795ff8eb3e6",
   "metadata": {},
   "source": [
    "## TensorFlow - Convolutional Neural Networks\n",
    "\n",
    "#### Two types of deep neural networks:\n",
    "<ul><li>\n",
    "    Convolutional Neural Networks\n",
    "</li><li>\n",
    "    Recurrent Neural Networks\n",
    "</li></ul>\n",
    "\n",
    "Convolution Neural Network (CNN) are designed to process data through multiple layers of arrays.  Convolutional neural network uses three basic ideas −\n",
    "<ul>\n",
    "    <li>Local respective fields</li>\n",
    "    <li>Convolution</li>\n",
    "    <li>Pooling</li>\n",
    "</ul>\n",
    "\n",
    "## TensorFlow Implementation of CNN\n",
    "\n",
    "### Step 1 - \n",
    "Import the necissary modules of TensorFlow and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614bf6ea-6b59-4dfe-a042-577ed930e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data ## Old code\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e914ffe-1d7c-454e-bf1d-0f5627217e18",
   "metadata": {},
   "source": [
    "### Step 2 - \n",
    "Declare a function called run_cnn(), which includes various parameters and optimization variables with declaration of data placeholders. These optimization variables will declare the training pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f8cb0ff-5ba0-40a5-baa5-7b39e6e4c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "#dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n",
    "\n",
    "def run_cnn():\n",
    "    #mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "    mnist = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 10\n",
    "    batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8fda5f-2d09-4e3d-ae13-00da7845a5e4",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Declare the training data placeholders of 28px by 28px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02fcd50-fec1-4b37-9446-0718b05c074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the t1 compat to make this V1 code still work\n",
    "tf.compat.v1.disable_eager_execution() # Disable V2's eager execution\n",
    "\n",
    "#x = tf.placeholder(tf.float32, [None, 784])\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 784])\n",
    "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "#y = tf.placeholder(tf.float32, [None, 10])\n",
    "y = tf.compat.v1.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb17d3-89b4-4f82-b405-9f9422bbce57",
   "metadata": {},
   "source": [
    "### Step 4 − \n",
    "\n",
    "Now it is important to create some convolutional layers −"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0b9728-ad81-4326-89cc-833d187441bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_conv_layer(\n",
    "    input_data, num_input_channels, num_filters,filter_shape, pool_shape, name):\n",
    "    \n",
    "    conv_filt_shape = [\n",
    "        filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "    \n",
    "    #weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev = 0.03), name = name+'_W')\n",
    "    weights = tf.Variable(tf.random.truncated_normal(conv_filt_shape, stddev = 0.03), name = name+'_W')\n",
    "    #bias = tf.Variable(tf.truncated_normal([num_filters]), name = name+'_b')\n",
    "    bias = tf.Variable(tf.random.truncated_normal([num_filters]), name = name+'_b')\n",
    "    \n",
    "    #Out layer defines the output\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding = 'SAME')\n",
    "    \n",
    "    out_layer += bias\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(\n",
    "        out_layer, ksize = ksize, strides = strides, padding = 'SAME')\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name = 'layer1')\n",
    "layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name = 'layer2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562ace3-37f1-4cb8-97ed-e18e05102d65",
   "metadata": {},
   "source": [
    "### Step 5 − \n",
    "\n",
    "Let us flatten the output ready for the fully connected output stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff56c646-2674-46dd-a10e-750ff5da67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
    "\n",
    "#wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev = 0.03), name = 'wd1')\n",
    "wd1 = tf.Variable(tf.random.truncated_normal([7 * 7 * 64, 1000], stddev = 0.03), name = 'wd1')\n",
    "#bd1 = tf.Variable(tf.truncated_normal([1000], stddev = 0.01), name = 'bd1')\n",
    "bd1 = tf.Variable(tf.random.truncated_normal([1000], stddev = 0.01), name = 'bd1')\n",
    "\n",
    "dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "dense_layer1 = tf.nn.relu(dense_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad7f57-3b8e-438a-b8bd-824d58a8b320",
   "metadata": {},
   "source": [
    "### Step 6 - \n",
    "\n",
    "Add another layer with specific softmax activations with the required optimizer defines the accuracy assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "769e2835-8d0c-4cba-8e2c-5bdd635f8e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm not sure why they defined these variables in a function, so we couldn't use them later.\n",
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "#wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev = 0.03), name = 'wd2')\n",
    "wd2 = tf.Variable(tf.random.truncated_normal([1000, 10], stddev = 0.03), name = 'wd2')\n",
    "#bd2 = tf.Variable(tf.truncated_normal([10], stddev = 0.01), name = 'bd2')\n",
    "bd2 = tf.Variable(tf.random.truncated_normal([10], stddev = 0.01), name = 'bd2')\n",
    "\n",
    "dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "y_ = tf.nn.softmax(dense_layer2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "   tf.nn.softmax_cross_entropy_with_logits(logits = dense_layer2, labels = y))\n",
    "\n",
    "optimiser = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init_op = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46bd899-aca4-45f5-bd9d-f549f37df600",
   "metadata": {},
   "source": [
    "### Step 7 -\n",
    "\n",
    "Run our bad code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a13c426e-858d-42d4-b633-368c6b7b5ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 03:24:27.153022: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-06-20 03:24:27.153061: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: 23d480978687\n",
      "2024-06-20 03:24:27.153069: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: 23d480978687\n",
      "2024-06-20 03:24:27.153144: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 545.23.6\n",
      "2024-06-20 03:24:27.153169: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 470.239.6\n",
      "2024-06-20 03:24:27.153175: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:244] kernel version 470.239.6 does not match DSO version 545.23.6 -- cannot find working devices in this configuration\n",
      "2024-06-20 03:24:27.161866: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[1;32m      6\u001b[0m     sess\u001b[38;5;241m.\u001b[39mrun(init_op)\n\u001b[0;32m----> 7\u001b[0m     total_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mmnist\u001b[49m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlabels) \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     10\u001b[0m         avg_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "tf.summary.scalar('accuracy', accuracy)\n",
    "merged = tf.compat.v1.summary.merge_all()\n",
    "writer = tf.compat.v1.summary.FileWriter('E:\\TensorFlowProject')\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    total_batch = int(len(mnist.train.labels) / batch_size)\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size = batch_size)\n",
    "        _, c = sess.run([optimiser, cross_entropy], feed_dict = {x:batch_x, y: batch_y})\n",
    "        avg_cost += c / total_batch\n",
    "    test_acc = sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels})\n",
    "    summary = sess.run(merged, feed_dict = {x: mnist.test.images, y:mnist.test.labels})\n",
    "    writer.add_summary(summary, epoch)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "writer.add_graph(sess.graph)\n",
    "print(sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "def create_new_conv_layer(\n",
    "    input_data, num_input_channels, num_filters,filter_shape, pool_shape, name):\n",
    "    \n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels, num_filters]\n",
    "\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev = 0.03), name = name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name = name+'_b')\n",
    "\n",
    "    #Out layer defines the output\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "    out_layer += bias\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize = ksize, strides = strides, padding = 'SAME')\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a43b9-dc43-43a7-9e72-ff45175a6f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
