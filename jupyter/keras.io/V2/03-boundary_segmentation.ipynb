{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430af532-3e09-4d4b-a7a8-c36d51ecc7d7",
   "metadata": {},
   "source": [
    "https://keras.io/examples/vision/basnet_segmentation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bb508-357f-4aa8-9f04-0ca593bd8d38",
   "metadata": {},
   "source": [
    "## Highly accurate boundaries segmentation using BASNet\n",
    "\n",
    "<b>Author:</b> <a href=\"https://github.com/hamidriasat\">Hamid Ali</a> <br />\n",
    "<b>Date created:</b> 2023/05/30 <br />\n",
    "<b>Last modified:</b> 2023/07/13 <br />\n",
    "<b>Description:</b> Boundaries aware segmentation model trained on the DUTS dataset. <br />\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Deep semantic segmentation algorithms have improved a lot recently, but still fails to correctly predict pixels around object boundaries. In this example we implement <b>Boundary-Aware Segmentation Network (BASNet)</b>, using two stage predict and refine architecture, and a hybrid loss it can predict highly accurate boundaries and fine structures for image segmentation.\n",
    "References:\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"https://arxiv.org/abs/2101.04704\">Boundary-Aware Segmentation Network for Mobile and Web Applications</a></li>\n",
    "    <li><a href=\"https://github.com/hamidriasat/BASNet/tree/basnet_keras\">BASNet Keras Implementation</a></li>\n",
    "    <li><a href=\"https://openaccess.thecvf.com/content_cvpr_2017/html/Wang_Learning_to_Detect_CVPR_2017_paper.html\">Learning to Detect Salient Objects with Image-level Supervision</a></li>\n",
    "</ul>\n",
    "\n",
    "### Download the Data\n",
    "\n",
    "We will use the <a href=\"http://saliencydetection.net/duts/\">DUTS-TE</a> dataset for training. It has 5,019 images but we will use 140 for training and validation to save notebook running time. DUTS is relatively large salient object segmentation dataset. which contain diversified textures and structures common to real-world images in both foreground and background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e744d-18d0-4945-bd77-2d3b1f190b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-11 02:33:42--  http://saliencydetection.net/duts/download/DUTS-TE.zip\n",
      "Resolving saliencydetection.net (saliencydetection.net)... 36.55.239.177\n",
      "Connecting to saliencydetection.net (saliencydetection.net)|36.55.239.177|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 139799089 (133M) [application/zip]\n",
      "Saving to: ‘DUTS-TE.zip.1’\n",
      "\n",
      "DUTS-TE.zip.1       100%[===================>] 133.32M  1.23MB/s    in 2m 37s  \n",
      "\n",
      "2024-06-11 02:36:24 (872 KB/s) - ‘DUTS-TE.zip.1’ saved [139799089/139799089]\n",
      "\n",
      "replace DUTS-TE/DUTS-TE-Image/ILSVRC2012_test_00040628.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!wget http://saliencydetection.net/duts/download/DUTS-TE.zip\n",
    "!unzip -q DUTS-TE.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c559686-9368-4b9e-8643-c0e2187b1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras_cv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7181c2-1858-40d2-81bd-0b39101162f6",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526cb5b-6cda-438e-ba35-5bbb7b251525",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 288\n",
    "BATCH_SIZE = 4\n",
    "OUT_CLASSES = 1\n",
    "TRAIN_SPLIT_RATIO = 0.90\n",
    "DATA_DIR = \"./DUTS-TE/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491689bd-47fe-4836-87f2-7a4429100f06",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "Create TensorFlow Dataset\n",
    "\n",
    "We will use <font color=\"red\">load_paths()</font> to load and split 140 paths into train and validation set, and <font color=\"red\">load_dataset()</font> to convert paths into <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset\">tf.data.Dataset object</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfb985-2e84-4f49-a039-ffe221490153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paths(path, split_ratio):\n",
    "    images = sorted(glob(os.path.join(path, \"DUTS-TE-Image/*\")))[:140]\n",
    "    masks = sorted(glob(os.path.join(path, \"DUTS-TE-Mask/*\")))[:140]\n",
    "    len_ = int(len(images) * split_ratio)\n",
    "    return (images[:len_], masks[:len_]), (images[len_:], masks[len_:])\n",
    "\n",
    "\n",
    "def read_image(path, size, mode):\n",
    "    x = keras.utils.load_img(path, target_size=size, color_mode=mode)\n",
    "    x = keras.utils.img_to_array(x)\n",
    "    x = (x / 255.0).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess(x_batch, y_batch, img_size, out_classes):\n",
    "    def f(_x, _y):\n",
    "        _x, _y = _x.decode(), _y.decode()\n",
    "        _x = read_image(_x, (img_size, img_size), mode=\"rgb\")  # image\n",
    "        _y = read_image(_y, (img_size, img_size), mode=\"grayscale\")  # mask\n",
    "        return _x, _y\n",
    "\n",
    "    images, masks = tf.numpy_function(f, [x_batch, y_batch], [tf.float32, tf.float32])\n",
    "    images.set_shape([img_size, img_size, 3])\n",
    "    masks.set_shape([img_size, img_size, out_classes])\n",
    "    return images, masks\n",
    "\n",
    "\n",
    "def load_dataset(image_paths, mask_paths, img_size, out_classes, batch, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "    if shuffle:\n",
    "        dataset = dataset.cache().shuffle(buffer_size=1000)\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: preprocess(x, y, img_size, out_classes),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train_paths, val_paths = load_paths(DATA_DIR, TRAIN_SPLIT_RATIO)\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    train_paths[0], train_paths[1], IMAGE_SIZE, OUT_CLASSES, BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_dataset = load_dataset(\n",
    "    val_paths[0], val_paths[1], IMAGE_SIZE, OUT_CLASSES, BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train Dataset: {train_dataset}\")\n",
    "print(f\"Validation Dataset: {val_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1b9a1-91ab-44ca-812b-d08b6a7489d1",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399cd7d0-53cb-4687-9d1b-69b37b9620dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(keras.utils.array_to_img(display_list[i]), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for image, mask in val_dataset.take(1):\n",
    "    display([image[0], mask[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe070d33-5c7d-497b-8f24-000f3ce26296",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Analyze Mask\n",
    "\n",
    "Lets print unique values of above displayed mask. You can see despite belonging to one class, it's intensity is changing between low(0) to high(255). This variation in intensity makes it hard for network to generate good segmentation map for <b>salient or camouflaged object segmentation</b>. Because of its Residual Refined Module (RMs), BASNet is good in generating highly accurate boundaries and fine structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183e42c-f356-40cb-95fb-0138883f7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique values count: {len(np.unique((mask[0] * 255)))}\")\n",
    "print(\"Unique values:\")\n",
    "print(np.unique((mask[0] * 255)).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51e768-907b-4f18-b17d-21f69d8dfb4d",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Building the BASNet Model\n",
    "\n",
    "BASNet comprises of a predict-refine architecture and a hybrid loss. The predict-refine architecture consists of a densely supervised encoder-decoder network and a residual refinement module, which are respectively used to predict and refine a segmentation probability map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142cf9cd-d3b1-4e15-a7e5-661795b2a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_block(x_input, filters, stride=1, down_sample=None, activation=None):\n",
    "    \"\"\"Creates a residual(identity) block with two 3*3 convolutions.\"\"\"\n",
    "    residual = x_input\n",
    "\n",
    "    x = layers.Conv2D(filters, (3, 3), strides=stride, padding=\"same\", use_bias=False)(\n",
    "        x_input\n",
    "    )\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, (3, 3), strides=(1, 1), padding=\"same\", use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if down_sample is not None:\n",
    "        residual = down_sample\n",
    "\n",
    "    x = layers.Add()([x, residual])\n",
    "\n",
    "    if activation is not None:\n",
    "        x = layers.Activation(activation)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def convolution_block(x_input, filters, dilation=1):\n",
    "    \"\"\"Apply convolution + batch normalization + relu layer.\"\"\"\n",
    "    x = layers.Conv2D(filters, (3, 3), padding=\"same\", dilation_rate=dilation)(x_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def segmentation_head(x_input, out_classes, final_size):\n",
    "    \"\"\"Map each decoder stage output to model output classes.\"\"\"\n",
    "    x = layers.Conv2D(out_classes, kernel_size=(3, 3), padding=\"same\")(x_input)\n",
    "\n",
    "    if final_size is not None:\n",
    "        x = layers.Resizing(final_size[0], final_size[1])(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_resnet_block(_resnet, block_num):\n",
    "    \"\"\"Extract and return ResNet-34 block.\"\"\"\n",
    "    resnet_layers = [3, 4, 6, 3]  # ResNet-34 layer sizes at different block.\n",
    "    return keras.models.Model(\n",
    "        inputs=_resnet.get_layer(f\"v2_stack_{block_num}_block1_1_conv\").input,\n",
    "        outputs=_resnet.get_layer(\n",
    "            f\"v2_stack_{block_num}_block{resnet_layers[block_num]}_add\"\n",
    "        ).output,\n",
    "        name=f\"resnet34_block{block_num + 1}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60d079-eaa9-45a9-8cfe-80dabeb76a69",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prediction Module\n",
    "\n",
    "Prediction module is a heavy encoder decoder structure like U-Net. The encoder includes an input convolutional layer and six stages. First four are adopted from ResNet-34 and rest are basic res-blocks. Since first convolution and pooling layer of ResNet-34 is skipped so we will use <font color=\"pink\">get_resnet_block()</font> to extract first four blocks. Both bridge and decoder uses three convolutional layers with side outputs. The module produces seven segmentation probability maps during training, with the last one considered the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dc862-5382-4a99-99fa-da6d42fa2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basnet_predict(input_shape, out_classes):\n",
    "    \"\"\"BASNet Prediction Module, it outputs coarse label map.\"\"\"\n",
    "    filters = 64\n",
    "    num_stages = 6\n",
    "\n",
    "    x_input = layers.Input(input_shape)\n",
    "\n",
    "    # -------------Encoder--------------\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(x_input)\n",
    "\n",
    "    resnet = keras_cv.models.ResNet34Backbone(\n",
    "        include_rescaling=False,\n",
    "    )\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for i in range(num_stages):\n",
    "        if i < 4:  # First four stages are adopted from ResNet-34 blocks.\n",
    "            x = get_resnet_block(resnet, i)(x)\n",
    "            encoder_blocks.append(x)\n",
    "            x = layers.Activation(\"relu\")(x)\n",
    "        else:  # Last 2 stages consist of three basic resnet blocks.\n",
    "            x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "            x = basic_block(x, filters=filters * 8, activation=\"relu\")\n",
    "            x = basic_block(x, filters=filters * 8, activation=\"relu\")\n",
    "            x = basic_block(x, filters=filters * 8, activation=\"relu\")\n",
    "            encoder_blocks.append(x)\n",
    "\n",
    "    # -------------Bridge-------------\n",
    "    x = convolution_block(x, filters=filters * 8, dilation=2)\n",
    "    x = convolution_block(x, filters=filters * 8, dilation=2)\n",
    "    x = convolution_block(x, filters=filters * 8, dilation=2)\n",
    "    encoder_blocks.append(x)\n",
    "\n",
    "    # -------------Decoder-------------\n",
    "    decoder_blocks = []\n",
    "    for i in reversed(range(num_stages)):\n",
    "        if i != (num_stages - 1):  # Except first, scale other decoder stages.\n",
    "            shape = keras.backend.int_shape(x)\n",
    "            x = layers.Resizing(shape[1] * 2, shape[2] * 2)(x)\n",
    "\n",
    "        x = layers.concatenate([encoder_blocks[i], x], axis=-1)\n",
    "        x = convolution_block(x, filters=filters * 8)\n",
    "        x = convolution_block(x, filters=filters * 8)\n",
    "        x = convolution_block(x, filters=filters * 8)\n",
    "        decoder_blocks.append(x)\n",
    "\n",
    "    decoder_blocks.reverse()  # Change order from last to first decoder stage.\n",
    "    decoder_blocks.append(encoder_blocks[-1])  # Copy bridge to decoder.\n",
    "\n",
    "    # -------------Side Outputs--------------\n",
    "    decoder_blocks = [\n",
    "        segmentation_head(decoder_block, out_classes, input_shape[:2])\n",
    "        for decoder_block in decoder_blocks\n",
    "    ]\n",
    "\n",
    "    return keras.models.Model(inputs=[x_input], outputs=decoder_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4e2211-8ac8-4156-ad92-d71ef0660cae",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Residual Refinement Module\n",
    "\n",
    "Refinement Modules (RMs), designed as a residual block aim to refines the coarse(blurry and noisy boundaries) segmentation maps generated by prediction module. Similar to prediction module it's also an encode decoder structure but with light weight 4 stages, each containing one <font color=\"red\">convolutional block()</font> init. At the end it adds both coarse and residual output to generate refined output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c08e2-5848-438d-8cbd-fae12c6ea812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basnet_rrm(base_model, out_classes):\n",
    "    \"\"\"BASNet Residual Refinement Module(RRM) module, output fine label map.\"\"\"\n",
    "    num_stages = 4\n",
    "    filters = 64\n",
    "\n",
    "    x_input = base_model.output[0]\n",
    "\n",
    "    # -------------Encoder--------------\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding=\"same\")(x_input)\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for _ in range(num_stages):\n",
    "        x = convolution_block(x, filters=filters)\n",
    "        encoder_blocks.append(x)\n",
    "        x = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # -------------Bridge--------------\n",
    "    x = convolution_block(x, filters=filters)\n",
    "\n",
    "    # -------------Decoder--------------\n",
    "    for i in reversed(range(num_stages)):\n",
    "        shape = keras.backend.int_shape(x)\n",
    "        x = layers.Resizing(shape[1] * 2, shape[2] * 2)(x)\n",
    "        x = layers.concatenate([encoder_blocks[i], x], axis=-1)\n",
    "        x = convolution_block(x, filters=filters)\n",
    "\n",
    "    x = segmentation_head(x, out_classes, None)  # Segmentation head.\n",
    "\n",
    "    # ------------- refined = coarse + residual\n",
    "    x = layers.Add()([x_input, x])  # Add prediction + refinement output\n",
    "\n",
    "    return keras.models.Model(inputs=[base_model.input], outputs=[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80160a5e-1f2b-4296-81c5-2f556dc92564",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Combine Predict and Refinement Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c82b98-5ea8-4618-8806-0a55f3127c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basnet(input_shape, out_classes):\n",
    "    \"\"\"BASNet, it's a combination of two modules\n",
    "    Prediction Module and Residual Refinement Module(RRM).\"\"\"\n",
    "\n",
    "    # Prediction model.\n",
    "    predict_model = basnet_predict(input_shape, out_classes)\n",
    "    # Refinement model.\n",
    "    refine_model = basnet_rrm(predict_model, out_classes)\n",
    "\n",
    "    output = [refine_model.output]  # Combine outputs.\n",
    "    output.extend(predict_model.output)\n",
    "\n",
    "    output = [layers.Activation(\"sigmoid\")(_) for _ in output]  # Activations.\n",
    "\n",
    "    return keras.models.Model(inputs=[predict_model.input], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d3334-70f7-4b24-b2ec-45b8e899a202",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Hybrid Loss\n",
    "\n",
    "Another important feature of BASNet is its hybrid loss function, which is a combination of binary cross entropy, structural similarity and intersection-over-union losses, which guide the network to learn three-level (i.e., pixel, patch and map level) hierarchy representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb53722-831e-4840-8187-74bb0b186ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasnetLoss(keras.losses.Loss):\n",
    "    \"\"\"BASNet hybrid loss.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(name=\"basnet_loss\", **kwargs)\n",
    "        self.smooth = 1.0e-9\n",
    "\n",
    "        # Binary Cross Entropy loss.\n",
    "        self.cross_entropy_loss = keras.losses.BinaryCrossentropy()\n",
    "        # Structural Similarity Index value.\n",
    "        self.ssim_value = tf.image.ssim\n",
    "        #  Jaccard / IoU loss.\n",
    "        self.iou_value = self.calculate_iou\n",
    "\n",
    "    def calculate_iou(\n",
    "        self,\n",
    "        y_true,\n",
    "        y_pred,\n",
    "    ):\n",
    "        \"\"\"Calculate intersection over union (IoU) between images.\"\"\"\n",
    "        intersection = backend.sum(backend.abs(y_true * y_pred), axis=[1, 2, 3])\n",
    "        union = backend.sum(y_true, [1, 2, 3]) + backend.sum(y_pred, [1, 2, 3])\n",
    "        union = union - intersection\n",
    "        return backend.mean(\n",
    "            (intersection + self.smooth) / (union + self.smooth), axis=0\n",
    "        )\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        cross_entropy_loss = self.cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "        ssim_value = self.ssim_value(y_true, y_pred, max_val=1)\n",
    "        ssim_loss = backend.mean(1 - ssim_value + self.smooth, axis=0)\n",
    "\n",
    "        iou_value = self.iou_value(y_true, y_pred)\n",
    "        iou_loss = 1 - iou_value\n",
    "\n",
    "        # Add all three losses.\n",
    "        return cross_entropy_loss + ssim_loss + iou_loss\n",
    "\n",
    "\n",
    "basnet_model = basnet(\n",
    "    input_shape=[IMAGE_SIZE, IMAGE_SIZE, 3], out_classes=OUT_CLASSES\n",
    ")  # Create model.\n",
    "basnet_model.summary()  # Show model summary.\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4, epsilon=1e-8)\n",
    "# Compile model.\n",
    "basnet_model.compile(\n",
    "    loss=BasnetLoss(),\n",
    "    optimizer=optimizer,\n",
    "    metrics=[keras.metrics.MeanAbsoluteError(name=\"mae\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b798a62-6fe9-425e-9779-4893ec5f3a82",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5f957-24c6-47a5-86f2-e97aa80bfceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basnet_model.fit(train_dataset, validation_data=val_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a920e-a6f9-4c17-b6a3-ceb0f10603ec",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "In paper BASNet was trained on DUTS-TR dataset, which has 10553 images. Model was trained for 400k iterations with a batch size of eight and without a validation dataset. After training model was evaluated on DUTS-TE dataset and achieved a mean absolute error of 0.042.\n",
    "\n",
    "Since BASNet is a deep model and cannot be trained in a short amount of time which is a requirement for keras example notebook, so we will load pretrained weights from here to show model prediction. Due to computer power limitation this model was trained for 120k iterations but it still demonstrates its capabilities. For further details about trainings parameters please check given link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5037a01-06f2-4565-b47d-15f11fa0f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!gdown 1OWKouuAQ7XpXZbWA3mmxDPrFGW71Axrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb957e-d98c-4e8e-b554-dd8bd62c7ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_output(prediction):\n",
    "    max_value = np.max(prediction)\n",
    "    min_value = np.min(prediction)\n",
    "    return (prediction - min_value) / (max_value - min_value)\n",
    "\n",
    "\n",
    "# Load weights.\n",
    "basnet_model.load_weights(\"./basnet_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feb17c-6355-48bd-a198-d650eb900809",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648934df-24ff-4d88-981f-c90267c697ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in val_dataset.take(1):\n",
    "    pred_mask = basnet_model.predict(image)\n",
    "    display([image[0], mask[0], normalize_output(pred_mask[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e470c33-a981-42a3-93d1-c26446fd3b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
