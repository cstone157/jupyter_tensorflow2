{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d78db30c-08dd-49a9-a59b-52fac06b7e26",
   "metadata": {},
   "source": [
    "## Object Detection with RetinaNet\n",
    "\n",
    "<b>Author:</b> <a href=\"https://twitter.com/srihari_rh\">Srihari Humbarwadi</a> <br />\n",
    "<b>Date created:</b> 2020/05/17 <br />\n",
    "<b>Last modified:</b> 2023/07/10 <br />\n",
    "<b>Description:</b> Implementing RetinaNet: Focal Loss for Dense Object Detection. <br />\n",
    "\n",
    "<hr />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Object detection a very important problem in computer vision. Here the model is tasked with localizing the objects present in an image, and at the same time, classifying them into different categories. Object detection models can be broadly classified into \"single-stage\" and \"two-stage\" detectors. Two-stage detectors are often more accurate but at the cost of being slower. Here in this example, we will implement RetinaNet, a popular single-stage detector, which is accurate and runs fast. RetinaNet uses a feature pyramid network to efficiently detect objects at multiple scales and introduces a new loss, the Focal loss function, to alleviate the problem of the extreme foreground-background class imbalance.\n",
    "\n",
    "References:\n",
    "<ul>\n",
    "    <li><a href=\"https://arxiv.org/abs/1708.02002\">RetinaNet Paper</a></li>\n",
    "    <li><a href=\"https://arxiv.org/abs/1612.03144\">Feature Pyramid Network Paper</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b9a2a1-236f-45ef-8a0a-c23ae5df4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 02:37:20.035395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c0f76-d050-431c-b3b4-14e1e7c8dbc9",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Downloading the COCO2017 dataset\n",
    "\n",
    "Training on the entire COCO2017 dataset which has around 118k images takes a lot of time, hence we will be using a smaller subset of ~500 images for training in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a824d22f-fe97-4099-9197-1900a5cf75c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\n",
      " 28196864/560525318 [>.............................] - ETA: 2:21"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
    "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
    "keras.utils.get_file(filename, url)\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(\"data.zip\", \"r\") as z_fp:\n",
    "    z_fp.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189c239-52e2-4def-ab4c-12012cb8e778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
