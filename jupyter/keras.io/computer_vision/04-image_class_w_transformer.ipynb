{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8be2db-2300-406f-b61a-c1138c01f1cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348cea43-36c9-453b-9091-1bb890515bd2",
   "metadata": {},
   "source": [
    "## Image classification with Vision Transformer\n",
    "\n",
    "<b>Author</b>: <a href=\"https://www.linkedin.com/in/khalid-salama-24403144/\">Khalid Salama</a> <br/>\n",
    "<b>Date created</b>: 2021/01/18 <br/>\n",
    "<b>Last modified</b>: 2021/01/18 <br/>\n",
    "<b>Description</b>: Implementing the Vision Transformer (ViT) model for image classification.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<hr />\n",
    "\n",
    "This example implements the <a href=\"https://arxiv.org/abs/2010.11929\">Vision Transformer (ViT)</a> model by Alexey Dosovitskiy et al. for image classification, and demonstrates it on the CIFAR-100 dataset. The ViT model applies the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.\n",
    "\n",
    "<hr />\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d016c3-1da4-4df7-a0fc-ddcdbb315862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18769c55-8d8a-4ff4-a73d-bf31268a0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "#from keras import ops ## ops only included in Keras 3+\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104010b-c8d4-40e3-aa17-9b73d2f7d475",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccf3fe5-b340-4ef1-951b-6844e80a68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169001437/169001437 [==============================] - 412s 2us/step\n",
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a8a38-124a-466c-88cc-d1b7bf3b912f",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab36d7a6-e017-49ac-8449-15f30ab27279",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e1a2e-0283-4364-9b27-d7ef72709a68",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c224fcec-04f8-46f6-88b7-0ce968ccdb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 03:55:21.313868: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacef56f-9f66-4b5c-8cc8-76fb88afef9e",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef35542a-8cac-49ad-91a6-8578db32063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fce93-87a4-42cc-9533-f34b9e370fb2",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee3bbb9-ddf9-4672-8e21-73c964bc252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb240edf-ea0c-467b-a2b5-e334a6746e08",
   "metadata": {},
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70f419-6f87-4b0c-b797-91aa9a7a99a9",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the patch encoding layer\n",
    "\n",
    "The <font color=\"red\">PatchEncoder</font> layer will linearly transform a patch by projecting it into a vector of size <font color=\"red\">projection_dim</font>. In addition, it adds a learnable position embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36344d25-52c8-4ff2-91bc-7bfe7a83f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb25df-b81d-489f-abac-64aef8a54449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
