{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8be2db-2300-406f-b61a-c1138c01f1cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "348cea43-36c9-453b-9091-1bb890515bd2",
   "metadata": {},
   "source": [
    "## Image classification with Vision Transformer\n",
    "\n",
    "<b>Author</b>: <a href=\"https://www.linkedin.com/in/khalid-salama-24403144/\">Khalid Salama</a> <br/>\n",
    "<b>Date created</b>: 2021/01/18 <br/>\n",
    "<b>Last modified</b>: 2021/01/18 <br/>\n",
    "<b>Description</b>: Implementing the Vision Transformer (ViT) model for image classification.\n",
    "\n",
    "<div style=\"background-color: blue; color: white; width: '100%'; text-align: center; font-weight: bold; font-size: '150%';\"> This example uses Keras 3</div>\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<hr />\n",
    "\n",
    "This example implements the <a href=\"https://arxiv.org/abs/2010.11929\">Vision Transformer (ViT)</a> model by Alexey Dosovitskiy et al. for image classification, and demonstrates it on the CIFAR-100 dataset. The ViT model applies the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.\n",
    "\n",
    "<hr />\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d016c3-1da4-4df7-a0fc-ddcdbb315862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18769c55-8d8a-4ff4-a73d-bf31268a0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "#from keras import ops ## ops only included in Keras 3+\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104010b-c8d4-40e3-aa17-9b73d2f7d475",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccf3fe5-b340-4ef1-951b-6844e80a68a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169001437/169001437 [==============================] - 412s 2us/step\n",
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a8a38-124a-466c-88cc-d1b7bf3b912f",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab36d7a6-e017-49ac-8449-15f30ab27279",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 10  # For real training, use num_epochs=100. 10 is a test value\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [\n",
    "    2048,\n",
    "    1024,\n",
    "]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0e1a2e-0283-4364-9b27-d7ef72709a68",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Use data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c224fcec-04f8-46f6-88b7-0ce968ccdb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 03:55:21.313868: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacef56f-9f66-4b5c-8cc8-76fb88afef9e",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef35542a-8cac-49ad-91a6-8578db32063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fce93-87a4-42cc-9533-f34b9e370fb2",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee3bbb9-ddf9-4672-8e21-73c964bc252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        input_shape = ops.shape(images)\n",
    "        batch_size = input_shape[0]\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        channels = input_shape[3]\n",
    "        num_patches_h = height // self.patch_size\n",
    "        num_patches_w = width // self.patch_size\n",
    "        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n",
    "        patches = ops.reshape(\n",
    "            patches,\n",
    "            (\n",
    "                batch_size,\n",
    "                num_patches_h * num_patches_w,\n",
    "                self.patch_size * self.patch_size * channels,\n",
    "            ),\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"patch_size\": self.patch_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb240edf-ea0c-467b-a2b5-e334a6746e08",
   "metadata": {},
   "source": [
    "Let's display patches for a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6071e1bc-3585-47ff-984e-b0474550da08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(\n\u001b[1;32m      7\u001b[0m     ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor([image]), size\u001b[38;5;241m=\u001b[39m(image_size, image_size)\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m patches \u001b[38;5;241m=\u001b[39m Patches(patch_size)(resized_image)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m X \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ops' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#resized_image = ops.image.resize(  ## For Keras version 3+\n",
    "#    ops.convert_to_tensor([image]), size=(image_size, image_size)\n",
    "#)\n",
    "resized_image = keras(  ## For Keras version 2.15\n",
    "    ops.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70f419-6f87-4b0c-b797-91aa9a7a99a9",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "### Implement the patch encoding layer\n",
    "\n",
    "The <font color=\"red\">PatchEncoder</font> layer will linearly transform a patch by projecting it into a vector of size <font color=\"red\">projection_dim</font>. In addition, it adds a learnable position embedding to the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36344d25-52c8-4ff2-91bc-7bfe7a83f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb25df-b81d-489f-abac-64aef8a54449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
